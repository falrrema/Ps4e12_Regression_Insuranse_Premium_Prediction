{"cells":[{"cell_type":"markdown","metadata":{"id":"gtCHSHc_b3Wb"},"source":["# **Autogluon 12hr nonlog local**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ZxwwboJucI6K"},"outputs":[],"source":["%%capture\n","%pip install setuptools wheel autogluon.tabular[all,skex] dask[dataframe]\n","%pip install -U -q ipywidgets\n","%pip install -U scikit-learn"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1868,"status":"ok","timestamp":1735449619293,"user":{"displayName":"Fabián Alberto Reyes Madrid","userId":"11772448517145602220"},"user_tz":180},"id":"ksay2atjbwbZ"},"outputs":[],"source":["# Import basic libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","import warnings\n","import cloudpickle\n","import plotly.io as pio\n","import plotly.graph_objects as go\n","from autogluon.core.metrics import make_scorer\n","import sklearn\n","from plotly.subplots import make_subplots\n","pd.options.plotting.backend = \"plotly\"\n","pio.templates.default = \"simple_white\"\n","warnings.filterwarnings('ignore')\n","\n","# Import specific libraries\n","from autogluon.tabular import TabularDataset, TabularPredictor"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1950,"status":"ok","timestamp":1735449621240,"user":{"displayName":"Fabián Alberto Reyes Madrid","userId":"11772448517145602220"},"user_tz":180},"id":"fbg7pX6ucU1N","outputId":"a088a9b9-3997-4dd2-98be-a9e7545c3c6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6952,"status":"ok","timestamp":1735449628189,"user":{"displayName":"Fabián Alberto Reyes Madrid","userId":"11772448517145602220"},"user_tz":180},"id":"FW9nGCRsb2fg"},"outputs":[],"source":["base_path = os.getenv('DATA_FOLDER_PATH', 'Data/')\n","#base_path = os.getenv('DATA_FOLDER_PATH', '/content/drive/MyDrive/DS_Projects/Playground_Series/Ps4e12_Regression_Insuranse_Premium_Prediction/Data/')\n","\n","train = pd.read_csv(os.path.join(base_path, 'train.csv'))\n","test = pd.read_csv(os.path.join(base_path, 'test.csv'))\n","submission = pd.read_csv(os.path.join(base_path, 'sample_submission.csv'))\n","original = pd.read_csv(os.path.join(base_path, 'Insurance Premium Prediction Dataset.csv'))"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1735449628190,"user":{"displayName":"Fabián Alberto Reyes Madrid","userId":"11772448517145602220"},"user_tz":180},"id":"YFW3slsih7bb"},"outputs":[],"source":["train.set_index('id', inplace=True)\n","test.set_index('id', inplace=True)\n","\n","# Renaming columns for consistency\n","train.columns = train.columns.str.lower()\n","test.columns = test.columns.str.lower()\n","original.columns = original.columns.str.lower()\n","train.columns = [col.replace(\" \", \"_\") for col in train.columns]\n","test.columns = [col.replace(\" \", \"_\") for col in test.columns]\n","original.columns = [col.replace(\" \", \"_\") for col in original.columns]\n","original  = original[train.columns]\n","original = original.dropna(subset=['premium_amount'])"]},{"cell_type":"markdown","metadata":{"id":"S2AvPmfteZw8"},"source":["# **Feature Engineering**"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1735449628190,"user":{"displayName":"Fabián Alberto Reyes Madrid","userId":"11772448517145602220"},"user_tz":180},"id":"c30jWIHAj1-E"},"outputs":[],"source":["def create_date_features(df):\n","    # Basic date features\n","    df['policy_start'] = pd.to_datetime(df['policy_start_date'])\n","    df['year'] = df['policy_start'].dt.year\n","    df['month'] = df['policy_start'].dt.month\n","    df['day'] = df['policy_start'].dt.day\n","    df['week_of_year'] = df['policy_start'].dt.isocalendar().week.astype('int')\n","    df['day_of_week'] = df['policy_start'].dt.day_name()\n","    df['month_name'] = df['policy_start'].dt.month_name()\n","    df['quarter'] = df['policy_start'].dt.quarter\n","\n","    # Cyclical encoding\n","    for col, max_val in [('year', 1), ('month', 12), ('day', 31)]:\n","        df[f'{col}_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n","        df[f'{col}_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n","\n","    # Binary flags\n","    df['is_weekend'] = df['policy_start'].dt.dayofweek.isin([5,6]).astype(int)\n","    df['is_month_end'] = df['policy_start'].dt.is_month_end.astype(int)\n","    df['is_month_start'] = df['policy_start'].dt.is_month_start.astype(int)\n","    df['is_quarter_end'] = df['policy_start'].dt.is_quarter_end.astype(int)\n","    df['is_quarter_start'] = df['policy_start'].dt.is_quarter_start.astype(int)\n","\n","    # Time-based calculations\n","    df['policy_age_days'] = (df['policy_start'].max() - df['policy_start']).dt.days\n","    df['week_of_month'] = df['day'].apply(lambda x: (x-1)//7 + 1)\n","    df['days_in_month'] = df['policy_start'].dt.days_in_month\n","    df['days_remaining_in_month'] = df['days_in_month'] - df['day']\n","\n","    # Seasonal mapping\n","    season_map = {12:'winter', 1:'winter', 2:'winter',\n","                  3:'spring', 4:'spring', 5:'spring',\n","                  6:'summer', 7:'summer', 8:'summer',\n","                  9:'fall', 10:'fall', 11:'fall'}\n","    df['season'] = df['month'].map(season_map)\n","\n","    return df"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1735449628190,"user":{"displayName":"Fabián Alberto Reyes Madrid","userId":"11772448517145602220"},"user_tz":180},"id":"bnMYEbHAec0g"},"outputs":[],"source":["def create_advanced_features(df, is_training=True):\n","    \"\"\"\n","    Create advanced features for insurance premium prediction with proper scaling\n","    \"\"\"\n","    df = df.copy()\n","\n","    # Store scaling factors during training\n","    if is_training:\n","        global scale_params\n","        scale_params = {\n","            'health_score_mean': df['health_score'].mean(),\n","            'health_score_std': df['health_score'].std(),\n","            'credit_score_mean': df['credit_score'].mean(),\n","            'credit_score_std': df['credit_score'].std(),\n","            'customer_feedback_map': {\n","                'Poor': 0.0,    # Higher risk\n","                'Average': 0.5, # Medium risk\n","                'Good': 1.0     # Lower risk\n","            },\n","            'exercise_frequency_map': {\n","                'Rarely': 0.0,   # Highest risk\n","                'Monthly': 0.33, # High risk\n","                'Weekly': 0.66,  # Low risk\n","                'Daily': 1.0     # Lowest risk\n","            },\n","            'smoking_map': {\n","                'Yes': 1.0,  # Higher risk\n","                'No': 0.0    # Lower risk\n","            },\n","            'marital_risk_map': {\n","                'Single': 1.0,    # Base risk\n","                'Married': 0.8,   # Lower risk (shared responsibility)\n","                'Divorced': 1.2   # Higher risk (potentially more financial stress)\n","            },\n","            'property_risk_map': {\n","                'Apartment': 1.0,  # Base risk\n","                'House': 1.5,     # Higher risk (more value/larger space)\n","                'Condo': 1.2      # Medium risk\n","            }\n","        }\n","\n","    # 1. Date-based features\n","    df = create_date_features(df)\n","\n","    # 2. Income-based features with proper scaling\n","    df['income_per_dependent'] = df['annual_income'] / (df['number_of_dependents'] + 1)\n","    df['income_bracket'] = pd.qcut(df['annual_income'], q=5,\n","                                 labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n","\n","    # 3. Risk Score Combinations with standardization\n","    # Standardize health and credit scores\n","    df['health_score_std'] = (df['health_score'] - scale_params['health_score_mean']) / scale_params['health_score_std']\n","    df['credit_score_std'] = (df['credit_score'] - scale_params['credit_score_mean']) / scale_params['credit_score_std']\n","\n","    # Combined risk score (now both features are on same scale)\n","    df['total_risk_score'] = df['health_score_std'] + df['credit_score_std']\n","\n","    # Claims ratio with insurance duration\n","    df['claims_to_duration_ratio'] = df['previous_claims'] / (df['insurance_duration'] + 1)\n","\n","    # 4. Age-related interactions\n","    df['vehicle_to_driver_age_ratio'] = df['vehicle_age'] / df['age']\n","    df['is_young_driver'] = (df['age'] < 25).astype(int)\n","    df['is_senior_driver'] = (df['age'] > 65).astype(int)\n","\n","    # 5. Lifestyle Score (normalized to 0-1 range)\n","    df['exercise_score'] = df['exercise_frequency'].map(scale_params['exercise_frequency_map'])\n","    df['smoking_risk'] = df['smoking_status'].map(scale_params['smoking_map'])\n","    df['lifestyle_score'] = (\n","        df['exercise_score'] * 0.4 +    # Exercise has significant impact\n","        (1 - df['smoking_risk']) * 0.4 + # Non-smoking is positive\n","        (df['health_score_std'] > 0) * 0.2  # Above average health is positive\n","    )\n","\n","    # 6. Location-based features\n","    if is_training:\n","        scale_params['location_risk_map'] = df.groupby('location')['previous_claims'].mean()\n","        scale_params['location_credit_map'] = df.groupby('location')['credit_score'].mean()\n","\n","    df['location_risk'] = df['location'].map(scale_params['location_risk_map'])\n","    df['location_avg_credit'] = df['location'].map(scale_params['location_credit_map'])\n","\n","    # 7. Complex Interaction Features\n","    df['customer_feedback_score'] = df['customer_feedback'].map(scale_params['customer_feedback_map'])\n","\n","    # Weighted responsibility score (all components now 0-1 scaled)\n","    df['responsibility_score'] = (\n","        df['credit_score_std'].clip(-3, 3) * 0.4 +  # Limit outlier effect\n","        df['customer_feedback_score'] * 0.3 +\n","        (1 - df['claims_to_duration_ratio'].clip(0, 1)) * 0.3  # Lower claims is better\n","    )\n","\n","    # 8. Family and Property Risk\n","    df['marital_risk'] = df['marital_status'].map(scale_params['marital_risk_map'])\n","    df['property_risk'] = df['property_type'].map(scale_params['property_risk_map'])\n","\n","    # Combined risk factors\n","    df['family_risk_factor'] = df['marital_risk'] * (df['number_of_dependents'] + 1)\n","    df['asset_risk'] = (\n","        df['property_risk'] * 0.6 +\n","        (df['vehicle_age'] / df['vehicle_age'].max()) * 0.4  # Normalized vehicle age\n","    )\n","\n","    # 9. Customer Segment Features\n","    df['premium_segment'] = 'Standard'\n","    mask_premium = (\n","        (df['credit_score_std'] > 1) &  # Above 1 std in credit\n","        (df['previous_claims'] == 0) &   # No claims\n","        (df['health_score_std'] > 1)     # Above 1 std in health\n","    )\n","    mask_high_risk = (\n","        (df['credit_score_std'] < -1) |  # Below 1 std in credit\n","        (df['previous_claims'] > 3)       # Multiple claims\n","    )\n","\n","    df.loc[mask_premium, 'premium_segment'] = 'Premium'\n","    df.loc[mask_high_risk, 'premium_segment'] = 'High Risk'\n","\n","    # 10. Additional Ratio Features\n","    df['claims_per_year'] = df['previous_claims'] / (df['insurance_duration'] + 1)\n","    df['dependent_income_ratio'] = df['number_of_dependents'] / df['annual_income']\n","\n","    # Drop intermediate columns\n","    intermediate_cols = ['health_score_std', 'credit_score_std', 'exercise_score',\n","                        'smoking_risk', 'customer_feedback_score', 'marital_risk',\n","                        'property_risk']\n","    df = df.drop(columns=[col for col in intermediate_cols if col in df.columns])\n","\n","    return df"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6419,"status":"ok","timestamp":1735449634604,"user":{"displayName":"Fabián Alberto Reyes Madrid","userId":"11772448517145602220"},"user_tz":180},"id":"EYRxN23qfscL","outputId":"723ddd28-165b-4921-c47c-37e5688250f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training data transformation successful!\n","Test data transformation successful!\n"]}],"source":["try:\n","    # Transform training data\n","    train_transformed = create_advanced_features(train, is_training=True)\n","    print(\"Training data transformation successful!\")\n","\n","    # Transform test data\n","    test_transformed = create_advanced_features(test, is_training=False)\n","    print(\"Test data transformation successful!\")\n","\n","except Exception as e:\n","    print(f\"An error occurred: {str(e)}\")\n","    print(\"Please check your data types and column names.\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["train_transformed.to_parquet(os.path.join(base_path, \"train_transformed.parquet\"))\n","test_transformed.to_parquet(os.path.join(base_path, \"test_transformed.parquet\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"XaRagHNL6IcG"},"source":["# **Autogluon Train 12 hours**"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1735449634604,"user":{"displayName":"Fabián Alberto Reyes Madrid","userId":"11772448517145602220"},"user_tz":180},"id":"PGz-o44GrhYM"},"outputs":[],"source":["# Create the AutoGluon scorer using sklearn's implementation\n","rmsle_scorer = make_scorer(\n","    name='rmsle',\n","    score_func=sklearn.metrics.root_mean_squared_log_error,\n","    optimum=0,\n","    greater_is_better=False,\n","    needs_pred=True\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"HW8tAB-b6Tcu"},"outputs":[{"name":"stderr","output_type":"stream","text":["Verbosity: 2 (Standard Logging)\n","=================== System Info ===================\n","AutoGluon Version:  1.1.1\n","Python Version:     3.10.14\n","Operating System:   Darwin\n","Platform Machine:   arm64\n","Platform Version:   Darwin Kernel Version 23.0.0: Fri Sep 15 14:43:05 PDT 2023; root:xnu-10002.1.13~1/RELEASE_ARM64_T6020\n","CPU Count:          10\n","Memory Avail:       4.12 GB / 16.00 GB (25.7%)\n","Disk Space Avail:   86.75 GB / 460.43 GB (18.8%)\n","===================================================\n","Presets specified: ['best_quality']\n","Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n","Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n","DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n","\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n","\tRunning DyStack for up to 10800s of the 43200s of remaining time (25%).\n","\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n","2024-12-29 08:30:07,629\tINFO worker.py:1743 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n","\t\tContext path: \"Autogluon/202412_ps4s12_12hr_training/ds_sub_fit/sub_fit_ho\"\n","\u001b[36m(_dystack pid=10791)\u001b[0m Running DyStack sub-fit ...\n","\u001b[36m(_dystack pid=10791)\u001b[0m Beginning AutoGluon training ... Time limit = 10796s\n","\u001b[36m(_dystack pid=10791)\u001b[0m AutoGluon will save models to \"Autogluon/202412_ps4s12_12hr_training/ds_sub_fit/sub_fit_ho\"\n","\u001b[36m(_dystack pid=10791)\u001b[0m Train Data Rows:    1066666\n","\u001b[36m(_dystack pid=10791)\u001b[0m Train Data Columns: 59\n","\u001b[36m(_dystack pid=10791)\u001b[0m Label Column:       premium_amount\n","\u001b[36m(_dystack pid=10791)\u001b[0m Problem Type:       regression\n","\u001b[36m(_dystack pid=10791)\u001b[0m Preprocessing data ...\n","\u001b[36m(_dystack pid=10791)\u001b[0m Using Feature Generators to preprocess the data ...\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tAvailable Memory:                    4649.40 MB\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tTrain Data (Original)  Memory Usage: 1302.52 MB (28.0% of available memory)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tWarning: Data size prior to feature transformation consumes 28.0% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tStage 1 Generators:\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\t\tNote: Converting 8 features to boolean dtype as they only contain 2 unique values.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tStage 2 Generators:\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tStage 3 Generators:\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tFitting DatetimeFeatureGenerator...\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tStage 4 Generators:\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tStage 5 Generators:\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tUseless Original Features (Count: 2): ['year_cos', 'is_senior_driver']\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tThese features carry no predictive signal and should be manually investigated.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tThis is typically a feature which has the same value for all rows.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tThese features do not need to be present at inference time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tUnused Original Features (Count: 1): ['claims_per_year']\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tThese features do not need to be present at inference time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\t('float', []) : 1 | ['claims_per_year']\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\t('category', [])                   :  1 | ['income_bracket']\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\t('datetime', [])                   :  1 | ['policy_start']\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\t('float', [])                      : 24 | ['age', 'annual_income', 'number_of_dependents', 'health_score', 'previous_claims', ...]\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\t('int', [])                        : 15 | ['year', 'month', 'day', 'week_of_year', 'quarter', ...]\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\t('object', [])                     : 14 | ['gender', 'marital_status', 'education_level', 'occupation', 'location', ...]\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\t('object', ['datetime_as_object']) :  1 | ['policy_start_date']\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\t('category', [])             : 13 | ['marital_status', 'education_level', 'occupation', 'location', 'policy_type', ...]\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\t('float', [])                : 24 | ['age', 'annual_income', 'number_of_dependents', 'health_score', 'previous_claims', ...]\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\t('int', [])                  :  9 | ['year', 'month', 'day', 'week_of_year', 'quarter', ...]\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\t('int', ['bool'])            :  8 | ['gender', 'smoking_status', 'is_weekend', 'is_month_end', 'is_month_start', ...]\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\t('int', ['datetime_as_int']) :  2 | ['policy_start_date', 'policy_start_date.dayofweek']\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t6.5s = Fit runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t56 features in original data used to generate 56 features in processed data.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tTrain Data (Processed) Memory Usage: 281.78 MB (5.4% of available memory)\n","\u001b[36m(_dystack pid=10791)\u001b[0m Data preprocessing and feature engineering runtime = 7.24s ...\n","\u001b[36m(_dystack pid=10791)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'rmsle'\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n","\u001b[36m(_dystack pid=10791)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n","\u001b[36m(_dystack pid=10791)\u001b[0m User-specified model hyperparameters to be fit:\n","\u001b[36m(_dystack pid=10791)\u001b[0m {\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","\u001b[36m(_dystack pid=10791)\u001b[0m }\n","\u001b[36m(_dystack pid=10791)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting 108 L1 models ...\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 7190.75s of the 10788.81s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tWarning: Not enough memory to safely train model. Estimated to require 0.860 GB out of 3.510 GB available memory (24.509%)... (20.000% of avail memory is the max safe size)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.28 to avoid the error)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tNot enough memory to train KNeighborsUnif_BAG_L1... Skipping this model.\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 7187.64s of the 10785.7s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tWarning: Not enough memory to safely train model. Estimated to require 0.860 GB out of 3.495 GB available memory (24.608%)... (20.000% of avail memory is the max safe size)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.28 to avoid the error)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tNot enough memory to train KNeighborsDist_BAG_L1... Skipping this model.\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 7187.16s of the 10785.23s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 56.50% memory usage per fold, 56.50%/80.00% total).\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=56.50%)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_ray_fit pid=10865)\u001b[0m [1000]\tvalid_set's l2: 699160\tvalid_set's rmsle: -1.13518\n","\u001b[36m(_ray_fit pid=10865)\u001b[0m [2000]\tvalid_set's l2: 698759\tvalid_set's rmsle: -1.13448\n","\u001b[36m(_ray_fit pid=11097)\u001b[0m [1000]\tvalid_set's l2: 700323\tvalid_set's rmsle: -1.13651\n","\u001b[36m(_ray_fit pid=11097)\u001b[0m [2000]\tvalid_set's l2: 700119\tvalid_set's rmsle: -1.13573\n","\u001b[36m(_ray_fit pid=11097)\u001b[0m [3000]\tvalid_set's l2: 700402\tvalid_set's rmsle: -1.13565\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_dystack pid=10791)\u001b[0m \tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training... Skipping this model.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=11097, ip=127.0.0.1)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","\u001b[36m(_dystack pid=10791)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","\u001b[36m(_dystack pid=10791)\u001b[0m     out = self._fit(**kwargs)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 218, in _fit\n","\u001b[36m(_dystack pid=10791)\u001b[0m     self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 128, in train_lgb_model\n","\u001b[36m(_dystack pid=10791)\u001b[0m     return lgb.train(**train_params)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/engine.py\", line 314, in train\n","\u001b[36m(_dystack pid=10791)\u001b[0m     evaluation_result_list.extend(booster.eval_valid(feval))\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 4423, in eval_valid\n","\u001b[36m(_dystack pid=10791)\u001b[0m     return [\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 4426, in <listcomp>\n","\u001b[36m(_dystack pid=10791)\u001b[0m     for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 5187, in __inner_eval\n","\u001b[36m(_dystack pid=10791)\u001b[0m     feval_ret = eval_function(self.__inner_predict(data_idx), cur_data)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 45, in function_template\n","\u001b[36m(_dystack pid=10791)\u001b[0m     return metric.name, metric(y_true, y_hat), is_higher_better\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 96, in __call__\n","\u001b[36m(_dystack pid=10791)\u001b[0m     return self._score(y_true=y_true, y_pred=y_pred, **k)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 134, in _score\n","\u001b[36m(_dystack pid=10791)\u001b[0m     return self._sign * self._score_func(y_true, y_pred, **kwargs)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n","\u001b[36m(_dystack pid=10791)\u001b[0m     return func(*args, **kwargs)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 759, in root_mean_squared_log_error\n","\u001b[36m(_dystack pid=10791)\u001b[0m     raise ValueError(\n","\u001b[36m(_dystack pid=10791)\u001b[0m ValueError: Root Mean Squared Logarithmic Error cannot be used when targets contain negative values.\n","\u001b[36m(_dystack pid=10791)\u001b[0m Detailed Traceback:\n","\u001b[36m(_dystack pid=10791)\u001b[0m Traceback (most recent call last):\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","\u001b[36m(_dystack pid=10791)\u001b[0m     model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","\u001b[36m(_dystack pid=10791)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","\u001b[36m(_dystack pid=10791)\u001b[0m     out = self._fit(**kwargs)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","\u001b[36m(_dystack pid=10791)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","\u001b[36m(_dystack pid=10791)\u001b[0m     self._fit_folds(\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","\u001b[36m(_dystack pid=10791)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 666, in after_all_folds_scheduled\n","\u001b[36m(_dystack pid=10791)\u001b[0m     self._run_pseudo_sequential(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 644, in _run_pseudo_sequential\n","\u001b[36m(_dystack pid=10791)\u001b[0m     self._process_fold_results(finished[0], unfinished, fold_ctx)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","\u001b[36m(_dystack pid=10791)\u001b[0m     raise processed_exception\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","\u001b[36m(_dystack pid=10791)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","\u001b[36m(_dystack pid=10791)\u001b[0m     return fn(*args, **kwargs)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","\u001b[36m(_dystack pid=10791)\u001b[0m     return func(*args, **kwargs)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 2667, in get\n","\u001b[36m(_dystack pid=10791)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 864, in get_objects\n","\u001b[36m(_dystack pid=10791)\u001b[0m     raise value.as_instanceof_cause()\n","\u001b[36m(_dystack pid=10791)\u001b[0m ray.exceptions.RayTaskError(ValueError): \u001b[36mray::_ray_fit()\u001b[39m (pid=11097, ip=127.0.0.1)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","\u001b[36m(_dystack pid=10791)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","\u001b[36m(_dystack pid=10791)\u001b[0m     out = self._fit(**kwargs)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 218, in _fit\n","\u001b[36m(_dystack pid=10791)\u001b[0m     self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 128, in train_lgb_model\n","\u001b[36m(_dystack pid=10791)\u001b[0m     return lgb.train(**train_params)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/engine.py\", line 314, in train\n","\u001b[36m(_dystack pid=10791)\u001b[0m     evaluation_result_list.extend(booster.eval_valid(feval))\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 4423, in eval_valid\n","\u001b[36m(_dystack pid=10791)\u001b[0m     return [\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 4426, in <listcomp>\n","\u001b[36m(_dystack pid=10791)\u001b[0m     for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 5187, in __inner_eval\n","\u001b[36m(_dystack pid=10791)\u001b[0m     feval_ret = eval_function(self.__inner_predict(data_idx), cur_data)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 45, in function_template\n","\u001b[36m(_dystack pid=10791)\u001b[0m     return metric.name, metric(y_true, y_hat), is_higher_better\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 96, in __call__\n","\u001b[36m(_dystack pid=10791)\u001b[0m     return self._score(y_true=y_true, y_pred=y_pred, **k)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 134, in _score\n","\u001b[36m(_dystack pid=10791)\u001b[0m     return self._sign * self._score_func(y_true, y_pred, **kwargs)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n","\u001b[36m(_dystack pid=10791)\u001b[0m     return func(*args, **kwargs)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 759, in root_mean_squared_log_error\n","\u001b[36m(_dystack pid=10791)\u001b[0m     raise ValueError(\n","\u001b[36m(_dystack pid=10791)\u001b[0m ValueError: Root Mean Squared Logarithmic Error cannot be used when targets contain negative values.\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 7057.97s of the 10656.03s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 47.73% memory usage per fold, 47.73%/80.00% total).\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=47.73%)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_ray_fit pid=11403)\u001b[0m [1000]\tvalid_set's l2: 695227\tvalid_set's rmsle: -1.13135\n","\u001b[36m(_ray_fit pid=11763)\u001b[0m [1000]\tvalid_set's l2: 697560\tvalid_set's rmsle: -1.1286\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_dystack pid=10791)\u001b[0m \t-1.1314\t = Validation score   (-rmsle)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t137.12s\t = Training   runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t2.58s\t = Validation runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 6918.52s of the 10516.58s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 269 due to low memory. Expected memory usage reduced from 16.73% -> 15.0% of available memory...\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t-1.1286\t = Validation score   (-rmsle)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t774.81s\t = Training   runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t22.28s\t = Validation runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 6120.63s of the 9718.69s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 52.33% memory usage per fold, 52.33%/80.00% total).\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=52.33%)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t-1.1336\t = Validation score   (-rmsle)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t4908.37s\t = Training   runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t1.44s\t = Validation runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 1210.04s of the 4808.1s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t-1.1306\t = Validation score   (-rmsle)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t261.9s\t = Training   runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t22.79s\t = Validation runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 924.33s of the 4522.4s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tWarning: Potentially not enough memory to safely train model. Estimated to require 3.361 GB out of 4.183 GB available memory (80.345%)... (90.000% of avail memory is the max safe size)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.12 to avoid the warning)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 80.35% memory usage per fold, 80.35%/80.00% total).\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=80.35%)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\u001b[36m(_ray_fit pid=34145)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n","\u001b[36m(_ray_fit pid=34471)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n","\u001b[36m(_ray_fit pid=34844)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n","\u001b[36m(_ray_fit pid=35159)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n","\u001b[36m(_ray_fit pid=35491)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n","\u001b[36m(_ray_fit pid=35771)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n","\u001b[36m(_ray_fit pid=36102)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n","\u001b[36m(_ray_fit pid=36390)\u001b[0m Metric rmsle is not supported by this model - using mean_squared_error instead\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t-1.1365\t = Validation score   (-rmsle)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t650.8s\t = Training   runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t3.49s\t = Validation runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 270.92s of the 3868.99s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 61.47% memory usage per fold, 61.47%/80.00% total).\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=61.47%)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t-1.1505\t = Validation score   (-rmsle)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t235.56s\t = Training   runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t2.69s\t = Validation runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 33.03s of the 3631.1s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 37.21% memory usage per fold, 74.42%/80.00% total).\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=37.21%)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tTime limit exceeded... Skipping NeuralNetTorch_BAG_L1.\n","\u001b[36m(_dystack pid=10791)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=37773, ip=127.0.0.1)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","\u001b[36m(_dystack pid=10791)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","\u001b[36m(_dystack pid=10791)\u001b[0m     out = self._fit(**kwargs)\n","\u001b[36m(_dystack pid=10791)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 205, in _fit\n","\u001b[36m(_dystack pid=10791)\u001b[0m     raise TimeLimitExceeded\n","\u001b[36m(_dystack pid=10791)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 20.34s of the 3618.4s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 36.03% memory usage per fold, 72.07%/80.00% total).\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=36.03%)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t-1.1393\t = Validation score   (-rmsle)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t21.99s\t = Training   runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t0.84s\t = Validation runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 719.07s of the 3594.75s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tEnsemble Weights: {'RandomForestMSE_BAG_L1': 1.0}\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t-1.1286\t = Validation score   (-rmsle)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t2.32s\t = Training   runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t0.01s\t = Validation runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting 106 L2 models ...\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 3592.39s of the 3592.3s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 49.87% memory usage per fold, 49.87%/80.00% total).\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=49.87%)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t-1.1288\t = Validation score   (-rmsle)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t80.29s\t = Training   runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t1.26s\t = Validation runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 3509.82s of the 3509.73s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 47.73% memory usage per fold, 47.73%/80.00% total).\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=47.73%)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t-1.1288\t = Validation score   (-rmsle)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t100.83s\t = Training   runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t1.42s\t = Validation runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 3406.63s of the 3406.53s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 293 due to low memory. Expected memory usage reduced from 15.34% -> 15.0% of available memory...\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t-1.1287\t = Validation score   (-rmsle)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t1646.62s\t = Training   runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t26.93s\t = Validation runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 1731.98s of the 1731.9s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 52.97% memory usage per fold, 52.97%/80.00% total).\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=52.97%)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t-1.1288\t = Validation score   (-rmsle)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t887.75s\t = Training   runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t0.87s\t = Validation runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 841.86s of the 841.76s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t-1.1287\t = Validation score   (-rmsle)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t320.7s\t = Training   runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t23.52s\t = Validation runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 496.62s of the 496.53s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tWarning: Not enough memory to safely train model. Estimated to require 3.690 GB out of 4.009 GB available memory (92.041%)... (90.000% of avail memory is the max safe size)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.07 to avoid the error)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tNot enough memory to train NeuralNetFastAI_BAG_L2... Skipping this model.\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 495.66s of the 495.57s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 69.15% memory usage per fold, 69.15%/80.00% total).\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=69.15%)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t-1.1323\t = Validation score   (-rmsle)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t414.51s\t = Training   runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t2.88s\t = Validation runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 78.53s of the 78.44s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 42.70% memory usage per fold, 42.70%/80.00% total).\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=42.70%)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tTime limit exceeded... Skipping NeuralNetTorch_BAG_L2.\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 65.47s of the 65.38s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 47.85% memory usage per fold, 47.85%/80.00% total).\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=47.85%)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t-1.1297\t = Validation score   (-rmsle)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t66.36s\t = Training   runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t0.94s\t = Validation runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -3.88s of remaining time.\n","\u001b[36m(_dystack pid=10791)\u001b[0m \tEnsemble Weights: {'RandomForestMSE_BAG_L1': 0.5, 'RandomForestMSE_BAG_L2': 0.3, 'ExtraTreesMSE_BAG_L2': 0.2}\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t-1.1285\t = Validation score   (-rmsle)\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t4.68s\t = Training   runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m \t0.01s\t = Validation runtime\n","\u001b[36m(_dystack pid=10791)\u001b[0m AutoGluon training complete, total runtime = 10804.78s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 5800.5 rows/s (133334 batch size)\n","\u001b[36m(_dystack pid=10791)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"Autogluon/202412_ps4s12_12hr_training/ds_sub_fit/sub_fit_ho\")\n","\u001b[36m(_dystack pid=10791)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n","Leaderboard on holdout data (DyStack):\n","                     model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n","0   RandomForestMSE_BAG_L2      -1.130696  -1.128728       rmsle       14.891691      83.046814  8637.181762                 1.039731               26.928152        1646.622008            2       True         11\n","1      WeightedEnsemble_L3      -1.131191  -1.128456       rmsle       15.695893     106.582392  8962.557356                 0.002047                0.014387           4.676606            3       True         16\n","2     ExtraTreesMSE_BAG_L2      -1.131479  -1.128695       rmsle       14.654115      79.639853  7311.258742                 0.802155               23.521191         320.698988            2       True         13\n","3   RandomForestMSE_BAG_L1      -1.131630  -1.128635       rmsle        0.857184      22.284296   774.814787                 0.857184               22.284296         774.814787            1       True          2\n","4      WeightedEnsemble_L2      -1.131630  -1.128635       rmsle        0.859451      22.299242   777.138024                 0.002267                0.014946           2.323237            2       True          8\n","5        LightGBMXT_BAG_L2      -1.132220  -1.128784       rmsle       15.205303      57.376723  7070.851873                 1.353343                1.258061          80.292119            2       True          9\n","6          CatBoost_BAG_L2      -1.132286  -1.128837       rmsle       14.595824      56.991320  7878.305179                 0.743864                0.872658         887.745425            2       True         12\n","7          LightGBM_BAG_L2      -1.132339  -1.128849       rmsle       15.361381      57.539173  7091.387864                 1.509421                1.420511         100.828110            2       True         10\n","8     LightGBMLarge_BAG_L2      -1.133015  -1.129673       rmsle       14.885829      57.056857  7056.919484                 1.033869                0.938195          66.359730            2       True         15\n","9     ExtraTreesMSE_BAG_L1      -1.133537  -1.130649       rmsle        0.851431      22.789888   261.897166                 0.851431               22.789888         261.897166            1       True          4\n","10         LightGBM_BAG_L1      -1.134109  -1.131381       rmsle        2.949043       2.583247   137.122026                 2.949043                2.583247         137.122026            1       True          1\n","11          XGBoost_BAG_L2      -1.135374  -1.132267       rmsle       16.976958      59.001847  7405.072466                 3.124998                2.883184         414.512712            2       True         14\n","12         CatBoost_BAG_L1      -1.136301  -1.133595       rmsle        1.654869       1.443733  4908.372173                 1.654869                1.443733        4908.372173            1       True          3\n","13  NeuralNetFastAI_BAG_L1      -1.138873  -1.136507       rmsle        4.005579       3.485221   650.801972                 4.005579                3.485221         650.801972            1       True          5\n","14    LightGBMLarge_BAG_L1      -1.141835  -1.139276       rmsle        0.679127       0.837790    21.994229                 0.679127                0.837790          21.994229            1       True          7\n","15          XGBoost_BAG_L1      -1.152789  -1.150546       rmsle        2.854727       2.694487   235.557401                 2.854727                2.694487         235.557401            1       True          6\n","\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n","\t10837s\t = DyStack   runtime |\t32363s\t = Remaining runtime\n","Starting main fit with num_stack_levels=1.\n","\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n","Beginning AutoGluon training ... Time limit = 32363s\n","AutoGluon will save models to \"Autogluon/202412_ps4s12_12hr_training\"\n","Train Data Rows:    1200000\n","Train Data Columns: 59\n","Label Column:       premium_amount\n","Problem Type:       regression\n","Preprocessing data ...\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    6596.79 MB\n","\tTrain Data (Original)  Memory Usage: 1465.39 MB (22.2% of available memory)\n","\tWarning: Data size prior to feature transformation consumes 22.2% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 8 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\t\tFitting CategoryFeatureGenerator...\n","\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n","\t\tFitting DatetimeFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tUseless Original Features (Count: 2): ['year_cos', 'is_senior_driver']\n","\t\tThese features carry no predictive signal and should be manually investigated.\n","\t\tThis is typically a feature which has the same value for all rows.\n","\t\tThese features do not need to be present at inference time.\n","\tUnused Original Features (Count: 1): ['claims_per_year']\n","\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n","\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n","\t\tThese features do not need to be present at inference time.\n","\t\t('float', []) : 1 | ['claims_per_year']\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('category', [])                   :  1 | ['income_bracket']\n","\t\t('datetime', [])                   :  1 | ['policy_start']\n","\t\t('float', [])                      : 24 | ['age', 'annual_income', 'number_of_dependents', 'health_score', 'previous_claims', ...]\n","\t\t('int', [])                        : 15 | ['year', 'month', 'day', 'week_of_year', 'quarter', ...]\n","\t\t('object', [])                     : 14 | ['gender', 'marital_status', 'education_level', 'occupation', 'location', ...]\n","\t\t('object', ['datetime_as_object']) :  1 | ['policy_start_date']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('category', [])             : 13 | ['marital_status', 'education_level', 'occupation', 'location', 'policy_type', ...]\n","\t\t('float', [])                : 24 | ['age', 'annual_income', 'number_of_dependents', 'health_score', 'previous_claims', ...]\n","\t\t('int', [])                  :  9 | ['year', 'month', 'day', 'week_of_year', 'quarter', ...]\n","\t\t('int', ['bool'])            :  8 | ['gender', 'smoking_status', 'is_weekend', 'is_month_end', 'is_month_start', ...]\n","\t\t('int', ['datetime_as_int']) :  2 | ['policy_start_date', 'policy_start_date.dayofweek']\n","\t6.3s = Fit runtime\n","\t56 features in original data used to generate 56 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 317.01 MB (4.9% of available memory)\n","Data preprocessing and feature engineering runtime = 7.17s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'rmsle'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n","\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n","\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","AutoGluon will fit 2 stack levels (L1 to L2) ...\n","Fitting 108 L1 models ...\n","Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 21565.42s of the 32356.22s of remaining time.\n","\tWarning: Potentially not enough memory to safely train model. Estimated to require 0.968 GB out of 5.119 GB available memory (18.904%)... (20.000% of avail memory is the max safe size)\n","\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.31 to avoid the warning)\n","\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n","\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n","\t-1.2076\t = Validation score   (-rmsle)\n","\t0.74s\t = Training   runtime\n","\t590.61s\t = Validation runtime\n","Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 20972.51s of the 31763.31s of remaining time.\n","\tWarning: Potentially not enough memory to safely train model. Estimated to require 0.968 GB out of 4.941 GB available memory (19.583%)... (20.000% of avail memory is the max safe size)\n","\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.36 to avoid the warning)\n","\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n","\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n","\t-1.2076\t = Validation score   (-rmsle)\n","\t0.83s\t = Training   runtime\n","\t604.53s\t = Validation runtime\n","Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 20366.45s of the 31157.25s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 45.35% memory usage per fold, 45.35%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=45.35%)\n","\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training... Skipping this model.\n","\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=57008, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 218, in _fit\n","    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 128, in train_lgb_model\n","    return lgb.train(**train_params)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/engine.py\", line 314, in train\n","    evaluation_result_list.extend(booster.eval_valid(feval))\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 4423, in eval_valid\n","    return [\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 4426, in <listcomp>\n","    for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 5187, in __inner_eval\n","    feval_ret = eval_function(self.__inner_predict(data_idx), cur_data)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 45, in function_template\n","    return metric.name, metric(y_true, y_hat), is_higher_better\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 96, in __call__\n","    return self._score(y_true=y_true, y_pred=y_pred, **k)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 134, in _score\n","    return self._sign * self._score_func(y_true, y_pred, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 759, in root_mean_squared_log_error\n","    raise ValueError(\n","ValueError: Root Mean Squared Logarithmic Error cannot be used when targets contain negative values.\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","    self._fit_folds(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","    fold_fitting_strategy.after_all_folds_scheduled()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 666, in after_all_folds_scheduled\n","    self._run_pseudo_sequential(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 644, in _run_pseudo_sequential\n","    self._process_fold_results(finished[0], unfinished, fold_ctx)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","    raise processed_exception\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 2667, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 864, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(ValueError): \u001b[36mray::_ray_fit()\u001b[39m (pid=57008, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 218, in _fit\n","    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 128, in train_lgb_model\n","    return lgb.train(**train_params)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/engine.py\", line 314, in train\n","    evaluation_result_list.extend(booster.eval_valid(feval))\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 4423, in eval_valid\n","    return [\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 4426, in <listcomp>\n","    for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 5187, in __inner_eval\n","    feval_ret = eval_function(self.__inner_predict(data_idx), cur_data)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 45, in function_template\n","    return metric.name, metric(y_true, y_hat), is_higher_better\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 96, in __call__\n","    return self._score(y_true=y_true, y_pred=y_pred, **k)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 134, in _score\n","    return self._sign * self._score_func(y_true, y_pred, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 759, in root_mean_squared_log_error\n","    raise ValueError(\n","ValueError: Root Mean Squared Logarithmic Error cannot be used when targets contain negative values.\n","Fitting model: LightGBM_BAG_L1 ... Training model for up to 20182.94s of the 30973.74s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 44.55% memory usage per fold, 44.55%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=44.55%)\n","\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\tWarning: Exception caused LightGBM_BAG_L1 to fail during training... Skipping this model.\n","\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=57424, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 218, in _fit\n","    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 128, in train_lgb_model\n","    return lgb.train(**train_params)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/engine.py\", line 314, in train\n","    evaluation_result_list.extend(booster.eval_valid(feval))\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 4423, in eval_valid\n","    return [\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 4426, in <listcomp>\n","    for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 5187, in __inner_eval\n","    feval_ret = eval_function(self.__inner_predict(data_idx), cur_data)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 45, in function_template\n","    return metric.name, metric(y_true, y_hat), is_higher_better\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 96, in __call__\n","    return self._score(y_true=y_true, y_pred=y_pred, **k)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 134, in _score\n","    return self._sign * self._score_func(y_true, y_pred, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 759, in root_mean_squared_log_error\n","    raise ValueError(\n","ValueError: Root Mean Squared Logarithmic Error cannot be used when targets contain negative values.\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","    self._fit_folds(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","    fold_fitting_strategy.after_all_folds_scheduled()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 666, in after_all_folds_scheduled\n","    self._run_pseudo_sequential(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 644, in _run_pseudo_sequential\n","    self._process_fold_results(finished[0], unfinished, fold_ctx)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","    raise processed_exception\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 2667, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 864, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(ValueError): \u001b[36mray::_ray_fit()\u001b[39m (pid=57424, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 218, in _fit\n","    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 128, in train_lgb_model\n","    return lgb.train(**train_params)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/engine.py\", line 314, in train\n","    evaluation_result_list.extend(booster.eval_valid(feval))\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 4423, in eval_valid\n","    return [\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 4426, in <listcomp>\n","    for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 5187, in __inner_eval\n","    feval_ret = eval_function(self.__inner_predict(data_idx), cur_data)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 45, in function_template\n","    return metric.name, metric(y_true, y_hat), is_higher_better\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 96, in __call__\n","    return self._score(y_true=y_true, y_pred=y_pred, **k)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 134, in _score\n","    return self._sign * self._score_func(y_true, y_pred, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 759, in root_mean_squared_log_error\n","    raise ValueError(\n","ValueError: Root Mean Squared Logarithmic Error cannot be used when targets contain negative values.\n","Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 20143.63s of the 30934.42s of remaining time.\n","\t-1.1285\t = Validation score   (-rmsle)\n","\t951.53s\t = Training   runtime\n","\t26.53s\t = Validation runtime\n","Fitting model: CatBoost_BAG_L1 ... Training model for up to 19164.8s of the 29955.59s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 52.08% memory usage per fold, 52.08%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=52.08%)\n","\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\t-1.1332\t = Validation score   (-rmsle)\n","\t11243.67s\t = Training   runtime\n","\t2.16s\t = Validation runtime\n","Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 7918.74s of the 18709.54s of remaining time.\n","\t-1.1306\t = Validation score   (-rmsle)\n","\t300.21s\t = Training   runtime\n","\t24.51s\t = Validation runtime\n","Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 7592.79s of the 18383.59s of remaining time.\n","\tWarning: Not enough memory to safely train model. Estimated to require 3.781 GB out of 4.192 GB available memory (90.206%)... (90.000% of avail memory is the max safe size)\n","\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.05 to avoid the error)\n","\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n","\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n","\tNot enough memory to train NeuralNetFastAI_BAG_L1... Skipping this model.\n","Fitting model: XGBoost_BAG_L1 ... Training model for up to 7591.95s of the 18382.75s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 67.16% memory usage per fold, 67.16%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=67.16%)\n","\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n","\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=3529, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 153, in _fit\n","    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1108, in fit\n","    self._Booster = train(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/training.py\", line 182, in train\n","    if cb_container.after_iteration(bst, i, dtrain, evals):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/callback.py\", line 258, in after_iteration\n","    score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 2225, in eval_set\n","    feval_ret = feval(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 151, in inner\n","    return func.__name__, func(y_true, y_score)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_utils.py\", line 54, in custom_metric\n","    return sign * metric(y_true, y_hat)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 96, in __call__\n","    return self._score(y_true=y_true, y_pred=y_pred, **k)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 134, in _score\n","    return self._sign * self._score_func(y_true, y_pred, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 759, in root_mean_squared_log_error\n","    raise ValueError(\n","ValueError: Root Mean Squared Logarithmic Error cannot be used when targets contain negative values.\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","    self._fit_folds(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","    fold_fitting_strategy.after_all_folds_scheduled()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 666, in after_all_folds_scheduled\n","    self._run_pseudo_sequential(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 644, in _run_pseudo_sequential\n","    self._process_fold_results(finished[0], unfinished, fold_ctx)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","    raise processed_exception\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 2667, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 864, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(ValueError): \u001b[36mray::_ray_fit()\u001b[39m (pid=3529, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 153, in _fit\n","    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1108, in fit\n","    self._Booster = train(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/training.py\", line 182, in train\n","    if cb_container.after_iteration(bst, i, dtrain, evals):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/callback.py\", line 258, in after_iteration\n","    score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 2225, in eval_set\n","    feval_ret = feval(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 151, in inner\n","    return func.__name__, func(y_true, y_score)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_utils.py\", line 54, in custom_metric\n","    return sign * metric(y_true, y_hat)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 96, in __call__\n","    return self._score(y_true=y_true, y_pred=y_pred, **k)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 134, in _score\n","    return self._sign * self._score_func(y_true, y_pred, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 759, in root_mean_squared_log_error\n","    raise ValueError(\n","ValueError: Root Mean Squared Logarithmic Error cannot be used when targets contain negative values.\n","Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 6982.19s of the 17772.99s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 35.50% memory usage per fold, 71.00%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=35.50%)\n","\t-1.0754\t = Validation score   (-rmsle)\n","\t606.36s\t = Training   runtime\n","\t5.48s\t = Validation runtime\n","Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 6373.0s of the 17163.79s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 39.77% memory usage per fold, 79.55%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=39.77%)\n","\t-1.1311\t = Validation score   (-rmsle)\n","\t156.94s\t = Training   runtime\n","\t6.78s\t = Validation runtime\n","Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 6213.12s of the 17003.92s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 39.79% memory usage per fold, 79.57%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=39.79%)\n","\t-1.1332\t = Validation score   (-rmsle)\n","\t1142.86s\t = Training   runtime\n","\t1.42s\t = Validation runtime\n","Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 5068.3s of the 15859.1s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 32.23% memory usage per fold, 64.47%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=32.23%)\n","\t-1.082\t = Validation score   (-rmsle)\n","\t1064.17s\t = Training   runtime\n","\t7.26s\t = Validation runtime\n","Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 4000.98s of the 14791.78s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 30.71% memory usage per fold, 61.42%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=30.71%)\n","\tWarning: Exception caused LightGBM_r131_BAG_L1 to fail during training... Skipping this model.\n","\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=6282, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 218, in _fit\n","    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 128, in train_lgb_model\n","    return lgb.train(**train_params)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/engine.py\", line 314, in train\n","    evaluation_result_list.extend(booster.eval_valid(feval))\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 4423, in eval_valid\n","    return [\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 4426, in <listcomp>\n","    for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 5187, in __inner_eval\n","    feval_ret = eval_function(self.__inner_predict(data_idx), cur_data)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 45, in function_template\n","    return metric.name, metric(y_true, y_hat), is_higher_better\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 96, in __call__\n","    return self._score(y_true=y_true, y_pred=y_pred, **k)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 134, in _score\n","    return self._sign * self._score_func(y_true, y_pred, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 759, in root_mean_squared_log_error\n","    raise ValueError(\n","ValueError: Root Mean Squared Logarithmic Error cannot be used when targets contain negative values.\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","    self._fit_folds(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","    fold_fitting_strategy.after_all_folds_scheduled()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n","    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n","    self._process_fold_results(finished, unfinished, fold_ctx)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","    raise processed_exception\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 2667, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 864, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(ValueError): \u001b[36mray::_ray_fit()\u001b[39m (pid=6282, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 218, in _fit\n","    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 128, in train_lgb_model\n","    return lgb.train(**train_params)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/engine.py\", line 314, in train\n","    evaluation_result_list.extend(booster.eval_valid(feval))\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 4423, in eval_valid\n","    return [\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 4426, in <listcomp>\n","    for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/lightgbm/basic.py\", line 5187, in __inner_eval\n","    feval_ret = eval_function(self.__inner_predict(data_idx), cur_data)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 45, in function_template\n","    return metric.name, metric(y_true, y_hat), is_higher_better\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 96, in __call__\n","    return self._score(y_true=y_true, y_pred=y_pred, **k)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 134, in _score\n","    return self._sign * self._score_func(y_true, y_pred, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 759, in root_mean_squared_log_error\n","    raise ValueError(\n","ValueError: Root Mean Squared Logarithmic Error cannot be used when targets contain negative values.\n","Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 3282.15s of the 14072.95s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 56.10% memory usage per fold, 56.10%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=56.10%)\n","\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\t-1.1433\t = Validation score   (-rmsle)\n","\t2544.22s\t = Training   runtime\n","\t8.37s\t = Validation runtime\n","Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 734.96s of the 11525.76s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 32.50% memory usage per fold, 65.01%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=32.50%)\n","\t-1.1349\t = Validation score   (-rmsle)\n","\t591.78s\t = Training   runtime\n","\t2.76s\t = Validation runtime\n","Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 141.13s of the 10931.92s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 27.25% memory usage per fold, 54.49%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=27.25%)\n","\t-1.143\t = Validation score   (-rmsle)\n","\t122.84s\t = Training   runtime\n","\t12.2s\t = Validation runtime\n","Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 14.9s of the 10805.69s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 28.48% memory usage per fold, 56.96%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=28.48%)\n","\tTime limit exceeded... Skipping NeuralNetTorch_r22_BAG_L1.\n","2024-12-29 17:30:10,135\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=6649, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 205, in _fit\n","    raise TimeLimitExceeded\n","autogluon.core.utils.exceptions.TimeLimitExceeded\n","Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 3.37s of the 10794.16s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 40.15% memory usage per fold, 40.15%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=40.15%)\n","\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\t-1.1702\t = Validation score   (-rmsle)\n","\t74.1s\t = Training   runtime\n","\t2.69s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ... Training model for up to 2156.54s of the 10718.12s of remaining time.\n","\tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 0.818, 'NeuralNetTorch_r79_BAG_L1': 0.182}\n","\t-1.0739\t = Validation score   (-rmsle)\n","\t3.74s\t = Training   runtime\n","\t0.02s\t = Validation runtime\n","Fitting 106 L2 models ...\n","Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 10714.33s of the 10714.18s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 37.22% memory usage per fold, 74.45%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=37.22%)\n","\t-1.1287\t = Validation score   (-rmsle)\n","\t67.73s\t = Training   runtime\n","\t2.34s\t = Validation runtime\n","Fitting model: LightGBM_BAG_L2 ... Training model for up to 10644.23s of the 10644.07s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 35.86% memory usage per fold, 71.71%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=35.86%)\n","\t-1.1288\t = Validation score   (-rmsle)\n","\t63.69s\t = Training   runtime\n","\t1.91s\t = Validation runtime\n","Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 10577.86s of the 10577.71s of remaining time.\n","\t-1.1286\t = Validation score   (-rmsle)\n","\t2395.16s\t = Training   runtime\n","\t30.09s\t = Validation runtime\n","Fitting model: CatBoost_BAG_L2 ... Training model for up to 8151.46s of the 8151.31s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 44.54% memory usage per fold, 44.54%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=44.54%)\n","\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\t-1.1287\t = Validation score   (-rmsle)\n","\t1847.13s\t = Training   runtime\n","\t0.95s\t = Validation runtime\n","Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 6301.74s of the 6301.57s of remaining time.\n","\t-1.1286\t = Validation score   (-rmsle)\n","\t392.93s\t = Training   runtime\n","\t25.81s\t = Validation runtime\n","Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 5881.86s of the 5881.72s of remaining time.\n","\tWarning: Potentially not enough memory to safely train model. Estimated to require 4.468 GB out of 5.772 GB available memory (77.401%)... (90.000% of avail memory is the max safe size)\n","\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.08 to avoid the warning)\n","\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n","\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n","\tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 77.40% memory usage per fold, 77.40%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=77.40%)\n","\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\t-1.1293\t = Validation score   (-rmsle)\n","\t2168.37s\t = Training   runtime\n","\t4.0s\t = Validation runtime\n","Fitting model: XGBoost_BAG_L2 ... Training model for up to 3710.4s of the 3710.24s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 51.87% memory usage per fold, 51.87%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=51.87%)\n","\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\t-1.1287\t = Validation score   (-rmsle)\n","\t2553.06s\t = Training   runtime\n","\t3.66s\t = Validation runtime\n","Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 1154.46s of the 1154.3s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 36.96% memory usage per fold, 73.93%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=36.96%)\n","\t-1.1017\t = Validation score   (-rmsle)\n","\t847.57s\t = Training   runtime\n","\t6.47s\t = Validation runtime\n","Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 303.61s of the 303.45s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 39.51% memory usage per fold, 79.02%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=39.51%)\n","\t-1.1289\t = Validation score   (-rmsle)\n","\t87.85s\t = Training   runtime\n","\t2.27s\t = Validation runtime\n","Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 213.05s of the 212.91s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 38.01% memory usage per fold, 76.01%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=38.01%)\n","\t-1.1288\t = Validation score   (-rmsle)\n","\t134.07s\t = Training   runtime\n","\t0.92s\t = Validation runtime\n","Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 76.67s of the 76.51s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 40.14% memory usage per fold, 40.14%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=40.14%)\n","\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\tTime limit exceeded... Skipping NeuralNetTorch_r79_BAG_L2.\n","Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 61.52s of the 61.37s of remaining time.\n","\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 33.36% memory usage per fold, 66.72%/80.00% total).\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=33.36%)\n","\t-1.1318\t = Validation score   (-rmsle)\n","\t56.32s\t = Training   runtime\n","\t2.86s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L3 ... Training model for up to 1071.43s of the 1.36s of remaining time.\n","\tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 0.833, 'NeuralNetTorch_r79_BAG_L1': 0.083, 'NeuralNetTorch_BAG_L2': 0.083}\n","\t-1.073\t = Validation score   (-rmsle)\n","\t6.63s\t = Training   runtime\n","\t0.02s\t = Validation runtime\n","AutoGluon training complete, total runtime = 32368.84s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 709.7 rows/s (150000 batch size)\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"Autogluon/202412_ps4s12_12hr_training\")\n"]},{"data":{"text/plain":["<autogluon.tabular.predictor.predictor.TabularPredictor at 0x296abe5c0>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Setting up\n","label = 'premium_amount'\n","problem_type='regression'\n","hours = 12\n","\n","# Initialize the TabularPredictor\n","predictor = TabularPredictor(label=label,\n","                             problem_type=problem_type,\n","                             eval_metric=rmsle_scorer,\n","                             path = \"Autogluon/202412_ps4s12_12hr_training\")\n","\n","# Fit the model\n","predictor.fit(train_data=train_transformed,\n","              time_limit=3600*hours,\n","              presets=\"best_quality\"\n",")"]},{"cell_type":"markdown","metadata":{"id":"gJ4L9SP17wQa"},"source":["# **Best Submission**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AELB7AvC7xk4"},"outputs":[],"source":["predictor = TabularPredictor.load(os.path.join(base_path, \"Autogluon/202411_ps4s11_8hr_logloss_gpu\"))"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"PsoZUUls72uC"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>score_val</th>\n","      <th>eval_metric</th>\n","      <th>pred_time_val</th>\n","      <th>fit_time</th>\n","      <th>pred_time_val_marginal</th>\n","      <th>fit_time_marginal</th>\n","      <th>stack_level</th>\n","      <th>can_infer</th>\n","      <th>fit_order</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>WeightedEnsemble_L3</td>\n","      <td>-1.073038</td>\n","      <td>rmsle</td>\n","      <td>1301.780608</td>\n","      <td>19654.457292</td>\n","      <td>0.015892</td>\n","      <td>6.631069</td>\n","      <td>3</td>\n","      <td>True</td>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>WeightedEnsemble_L2</td>\n","      <td>-1.073907</td>\n","      <td>rmsle</td>\n","      <td>12.751272</td>\n","      <td>1674.268837</td>\n","      <td>0.017444</td>\n","      <td>3.736816</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>NeuralNetTorch_BAG_L1</td>\n","      <td>-1.075394</td>\n","      <td>rmsle</td>\n","      <td>5.477499</td>\n","      <td>606.360768</td>\n","      <td>5.477499</td>\n","      <td>606.360768</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NeuralNetTorch_r79_BAG_L1</td>\n","      <td>-1.081999</td>\n","      <td>rmsle</td>\n","      <td>7.256329</td>\n","      <td>1064.171253</td>\n","      <td>7.256329</td>\n","      <td>1064.171253</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NeuralNetTorch_BAG_L2</td>\n","      <td>-1.101687</td>\n","      <td>rmsle</td>\n","      <td>1301.764716</td>\n","      <td>19647.826223</td>\n","      <td>6.468448</td>\n","      <td>847.574484</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>RandomForestMSE_BAG_L1</td>\n","      <td>-1.128515</td>\n","      <td>rmsle</td>\n","      <td>26.527960</td>\n","      <td>951.525635</td>\n","      <td>26.527960</td>\n","      <td>951.525635</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>RandomForestMSE_BAG_L2</td>\n","      <td>-1.128615</td>\n","      <td>rmsle</td>\n","      <td>1325.387813</td>\n","      <td>21195.407501</td>\n","      <td>30.091545</td>\n","      <td>2395.155762</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>ExtraTreesMSE_BAG_L2</td>\n","      <td>-1.128645</td>\n","      <td>rmsle</td>\n","      <td>1321.108152</td>\n","      <td>19193.180725</td>\n","      <td>25.811884</td>\n","      <td>392.928986</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>LightGBMXT_BAG_L2</td>\n","      <td>-1.128671</td>\n","      <td>rmsle</td>\n","      <td>1297.640968</td>\n","      <td>18867.985766</td>\n","      <td>2.344700</td>\n","      <td>67.734027</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>XGBoost_BAG_L2</td>\n","      <td>-1.128677</td>\n","      <td>rmsle</td>\n","      <td>1298.952531</td>\n","      <td>21353.308266</td>\n","      <td>3.656263</td>\n","      <td>2553.056527</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>CatBoost_BAG_L2</td>\n","      <td>-1.128739</td>\n","      <td>rmsle</td>\n","      <td>1296.247258</td>\n","      <td>20647.380359</td>\n","      <td>0.950990</td>\n","      <td>1847.128620</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>18</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>CatBoost_r177_BAG_L2</td>\n","      <td>-1.128756</td>\n","      <td>rmsle</td>\n","      <td>1296.215219</td>\n","      <td>18934.325469</td>\n","      <td>0.918951</td>\n","      <td>134.073730</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>LightGBM_BAG_L2</td>\n","      <td>-1.128808</td>\n","      <td>rmsle</td>\n","      <td>1297.208934</td>\n","      <td>18863.938381</td>\n","      <td>1.912666</td>\n","      <td>63.686642</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>LightGBMLarge_BAG_L2</td>\n","      <td>-1.128858</td>\n","      <td>rmsle</td>\n","      <td>1297.570213</td>\n","      <td>18888.106422</td>\n","      <td>2.273944</td>\n","      <td>87.854683</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>NeuralNetFastAI_BAG_L2</td>\n","      <td>-1.129266</td>\n","      <td>rmsle</td>\n","      <td>1299.291634</td>\n","      <td>20968.621025</td>\n","      <td>3.995366</td>\n","      <td>2168.369286</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>ExtraTreesMSE_BAG_L1</td>\n","      <td>-1.130646</td>\n","      <td>rmsle</td>\n","      <td>24.505522</td>\n","      <td>300.209240</td>\n","      <td>24.505522</td>\n","      <td>300.209240</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>LightGBMLarge_BAG_L1</td>\n","      <td>-1.131114</td>\n","      <td>rmsle</td>\n","      <td>6.780478</td>\n","      <td>156.936736</td>\n","      <td>6.780478</td>\n","      <td>156.936736</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>LightGBM_r131_BAG_L2</td>\n","      <td>-1.131785</td>\n","      <td>rmsle</td>\n","      <td>1298.157649</td>\n","      <td>18856.572876</td>\n","      <td>2.861380</td>\n","      <td>56.321137</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>CatBoost_r177_BAG_L1</td>\n","      <td>-1.133230</td>\n","      <td>rmsle</td>\n","      <td>1.417750</td>\n","      <td>1142.861106</td>\n","      <td>1.417750</td>\n","      <td>1142.861106</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>CatBoost_BAG_L1</td>\n","      <td>-1.133231</td>\n","      <td>rmsle</td>\n","      <td>2.163060</td>\n","      <td>11243.665964</td>\n","      <td>2.163060</td>\n","      <td>11243.665964</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>CatBoost_r9_BAG_L1</td>\n","      <td>-1.134863</td>\n","      <td>rmsle</td>\n","      <td>2.759464</td>\n","      <td>591.777950</td>\n","      <td>2.759464</td>\n","      <td>591.777950</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>LightGBM_r96_BAG_L1</td>\n","      <td>-1.143034</td>\n","      <td>rmsle</td>\n","      <td>12.200909</td>\n","      <td>122.843300</td>\n","      <td>12.200909</td>\n","      <td>122.843300</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>NeuralNetFastAI_r191_BAG_L1</td>\n","      <td>-1.143326</td>\n","      <td>rmsle</td>\n","      <td>8.374821</td>\n","      <td>2544.224989</td>\n","      <td>8.374821</td>\n","      <td>2544.224989</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>XGBoost_r33_BAG_L1</td>\n","      <td>-1.170176</td>\n","      <td>rmsle</td>\n","      <td>2.692215</td>\n","      <td>74.098019</td>\n","      <td>2.692215</td>\n","      <td>74.098019</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>KNeighborsUnif_BAG_L1</td>\n","      <td>-1.207583</td>\n","      <td>rmsle</td>\n","      <td>590.607676</td>\n","      <td>0.743732</td>\n","      <td>590.607676</td>\n","      <td>0.743732</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>KNeighborsDist_BAG_L1</td>\n","      <td>-1.207583</td>\n","      <td>rmsle</td>\n","      <td>604.532584</td>\n","      <td>0.833048</td>\n","      <td>604.532584</td>\n","      <td>0.833048</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                          model  score_val eval_metric  pred_time_val  \\\n","0           WeightedEnsemble_L3  -1.073038       rmsle    1301.780608   \n","1           WeightedEnsemble_L2  -1.073907       rmsle      12.751272   \n","2         NeuralNetTorch_BAG_L1  -1.075394       rmsle       5.477499   \n","3     NeuralNetTorch_r79_BAG_L1  -1.081999       rmsle       7.256329   \n","4         NeuralNetTorch_BAG_L2  -1.101687       rmsle    1301.764716   \n","5        RandomForestMSE_BAG_L1  -1.128515       rmsle      26.527960   \n","6        RandomForestMSE_BAG_L2  -1.128615       rmsle    1325.387813   \n","7          ExtraTreesMSE_BAG_L2  -1.128645       rmsle    1321.108152   \n","8             LightGBMXT_BAG_L2  -1.128671       rmsle    1297.640968   \n","9                XGBoost_BAG_L2  -1.128677       rmsle    1298.952531   \n","10              CatBoost_BAG_L2  -1.128739       rmsle    1296.247258   \n","11         CatBoost_r177_BAG_L2  -1.128756       rmsle    1296.215219   \n","12              LightGBM_BAG_L2  -1.128808       rmsle    1297.208934   \n","13         LightGBMLarge_BAG_L2  -1.128858       rmsle    1297.570213   \n","14       NeuralNetFastAI_BAG_L2  -1.129266       rmsle    1299.291634   \n","15         ExtraTreesMSE_BAG_L1  -1.130646       rmsle      24.505522   \n","16         LightGBMLarge_BAG_L1  -1.131114       rmsle       6.780478   \n","17         LightGBM_r131_BAG_L2  -1.131785       rmsle    1298.157649   \n","18         CatBoost_r177_BAG_L1  -1.133230       rmsle       1.417750   \n","19              CatBoost_BAG_L1  -1.133231       rmsle       2.163060   \n","20           CatBoost_r9_BAG_L1  -1.134863       rmsle       2.759464   \n","21          LightGBM_r96_BAG_L1  -1.143034       rmsle      12.200909   \n","22  NeuralNetFastAI_r191_BAG_L1  -1.143326       rmsle       8.374821   \n","23           XGBoost_r33_BAG_L1  -1.170176       rmsle       2.692215   \n","24        KNeighborsUnif_BAG_L1  -1.207583       rmsle     590.607676   \n","25        KNeighborsDist_BAG_L1  -1.207583       rmsle     604.532584   \n","\n","        fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  \\\n","0   19654.457292                0.015892           6.631069            3   \n","1    1674.268837                0.017444           3.736816            2   \n","2     606.360768                5.477499         606.360768            1   \n","3    1064.171253                7.256329        1064.171253            1   \n","4   19647.826223                6.468448         847.574484            2   \n","5     951.525635               26.527960         951.525635            1   \n","6   21195.407501               30.091545        2395.155762            2   \n","7   19193.180725               25.811884         392.928986            2   \n","8   18867.985766                2.344700          67.734027            2   \n","9   21353.308266                3.656263        2553.056527            2   \n","10  20647.380359                0.950990        1847.128620            2   \n","11  18934.325469                0.918951         134.073730            2   \n","12  18863.938381                1.912666          63.686642            2   \n","13  18888.106422                2.273944          87.854683            2   \n","14  20968.621025                3.995366        2168.369286            2   \n","15    300.209240               24.505522         300.209240            1   \n","16    156.936736                6.780478         156.936736            1   \n","17  18856.572876                2.861380          56.321137            2   \n","18   1142.861106                1.417750        1142.861106            1   \n","19  11243.665964                2.163060       11243.665964            1   \n","20    591.777950                2.759464         591.777950            1   \n","21    122.843300               12.200909         122.843300            1   \n","22   2544.224989                8.374821        2544.224989            1   \n","23     74.098019                2.692215          74.098019            1   \n","24      0.743732              590.607676           0.743732            1   \n","25      0.833048              604.532584           0.833048            1   \n","\n","    can_infer  fit_order  \n","0        True         26  \n","1        True         14  \n","2        True          6  \n","3        True          9  \n","4        True         22  \n","5        True          3  \n","6        True         17  \n","7        True         19  \n","8        True         15  \n","9        True         21  \n","10       True         18  \n","11       True         24  \n","12       True         16  \n","13       True         23  \n","14       True         20  \n","15       True          5  \n","16       True          7  \n","17       True         25  \n","18       True          8  \n","19       True          4  \n","20       True         11  \n","21       True         12  \n","22       True         10  \n","23       True         13  \n","24       True          1  \n","25       True          2  "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["leaderboard_test = predictor.leaderboard(silent=True)\n","leaderboard_test"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Best model by autogluon is WeightedEnsemble_L3\n","With a score of RMSLE 1.073038009875013\n"]}],"source":["models = leaderboard_test.head(5)['model'].to_list()\n","best_model = models[0]\n","print(\"Best model by autogluon is\", models[0])\n","print(\"With a score of RMSLE\", np.abs(leaderboard_test[leaderboard_test['model']==best_model]['score_val'][0]))\n","\n","training = \"12hr_nonlog_local\"\n","sub_autogluon = submission.copy()\n","sub_autogluon['Premium Amount'] = predictor.predict(test_transformed, as_pandas=False, model=best_model)\n","sub_autogluon.to_csv(os.path.join(base_path, f\"submission_{best_model}_{training}.csv\"), index=False)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Generating OOF predictions for WeightedEnsemble_L3 - 0/26\n","Generating OOF predictions for WeightedEnsemble_L2 - 1/26\n","Generating OOF predictions for NeuralNetTorch_BAG_L1 - 2/26\n","Generating OOF predictions for NeuralNetTorch_r79_BAG_L1 - 3/26\n","Generating OOF predictions for NeuralNetTorch_BAG_L2 - 4/26\n","Generating OOF predictions for RandomForestMSE_BAG_L1 - 5/26\n","Generating OOF predictions for RandomForestMSE_BAG_L2 - 6/26\n","Generating OOF predictions for ExtraTreesMSE_BAG_L2 - 7/26\n","Generating OOF predictions for LightGBMXT_BAG_L2 - 8/26\n","Generating OOF predictions for XGBoost_BAG_L2 - 9/26\n","Generating OOF predictions for CatBoost_BAG_L2 - 10/26\n","Generating OOF predictions for CatBoost_r177_BAG_L2 - 11/26\n","Generating OOF predictions for LightGBM_BAG_L2 - 12/26\n","Generating OOF predictions for LightGBMLarge_BAG_L2 - 13/26\n","Generating OOF predictions for NeuralNetFastAI_BAG_L2 - 14/26\n","Generating OOF predictions for ExtraTreesMSE_BAG_L1 - 15/26\n","Generating OOF predictions for LightGBMLarge_BAG_L1 - 16/26\n","Generating OOF predictions for LightGBM_r131_BAG_L2 - 17/26\n","Generating OOF predictions for CatBoost_r177_BAG_L1 - 18/26\n","Generating OOF predictions for CatBoost_BAG_L1 - 19/26\n","Generating OOF predictions for CatBoost_r9_BAG_L1 - 20/26\n","Generating OOF predictions for LightGBM_r96_BAG_L1 - 21/26\n","Generating OOF predictions for NeuralNetFastAI_r191_BAG_L1 - 22/26\n","Generating OOF predictions for XGBoost_r33_BAG_L1 - 23/26\n","Generating OOF predictions for KNeighborsUnif_BAG_L1 - 24/26\n","Generating OOF predictions for KNeighborsDist_BAG_L1 - 25/26\n","Saved 26 model predictions for experiment 12nonlog\n"]}],"source":["def save_experiment_oofs(predictor, models, experiment_name, path, islog=True):\n","    \"\"\"\n","    Save OOF predictions as a single DataFrame with experiment identifier in column names\n","    \"\"\"\n","    # Create DataFrame with index from training data\n","    oof_df = pd.DataFrame(index=predictor.predict_oof().index)\n","\n","    # Add OOF predictions for each model with experiment identifier\n","    for i, model in enumerate(models):\n","        print(f\"Generating OOF predictions for {model} - {i}/{len(models)}\")\n","        oof_preds = predictor.predict_oof(model=model)\n","        if islog:\n","            oof_preds = np.power(10, oof_preds)\n","        col_name = f\"{experiment_name}_{model}\"\n","        oof_df[col_name] = oof_preds\n","\n","    # Save DataFrame\n","    filename = f\"oof_preds_{experiment_name}.csv\"\n","    filepath = os.path.join(path, filename)\n","\n","    oof_df.to_csv(filepath)\n","    print(f\"Saved {len(models)} model predictions for experiment {experiment_name}\")\n","    return oof_df\n","\n","# Saving OOFs for later use\n","oofs_path = \"Data/oofs/\"\n","models = leaderboard_test['model'].to_list()\n","experiment_name = \"12nonlog\"\n","oof_df = save_experiment_oofs(predictor, models, experiment_name, oofs_path, islog=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyNVq/bC8GPgxpsBIy3shjYY","gpuType":"V28","machine_shape":"hm","name":"","provenance":[{"file_id":"1EitC7l0z_iX7HTBqvZJyYgN8NpiIH7HH","timestamp":1735442226403}],"toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
