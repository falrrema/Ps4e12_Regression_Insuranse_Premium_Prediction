{"cells":[{"cell_type":"markdown","metadata":{"id":"gtCHSHc_b3Wb"},"source":["# **Autogluon Experiment including original data**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ZxwwboJucI6K"},"outputs":[],"source":["%%capture\n","%pip install setuptools wheel autogluon.tabular[all,skex] dask[dataframe]\n","%pip install -U -q ipywidgets\n","%pip install -U scikit-learn"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1868,"status":"ok","timestamp":1735449619293,"user":{"displayName":"Fabián Alberto Reyes Madrid","userId":"11772448517145602220"},"user_tz":180},"id":"ksay2atjbwbZ"},"outputs":[],"source":["# Import basic libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","import warnings\n","from autogluon.core.metrics import make_scorer\n","import sklearn\n","warnings.filterwarnings('ignore')\n","\n","# Import specific libraries\n","from autogluon.tabular import TabularDataset, TabularPredictor"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1950,"status":"ok","timestamp":1735449621240,"user":{"displayName":"Fabián Alberto Reyes Madrid","userId":"11772448517145602220"},"user_tz":180},"id":"fbg7pX6ucU1N","outputId":"a088a9b9-3997-4dd2-98be-a9e7545c3c6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6952,"status":"ok","timestamp":1735449628189,"user":{"displayName":"Fabián Alberto Reyes Madrid","userId":"11772448517145602220"},"user_tz":180},"id":"FW9nGCRsb2fg"},"outputs":[],"source":["base_path = os.getenv('DATA_FOLDER_PATH', 'Data/')\n","#base_path = os.getenv('DATA_FOLDER_PATH', '/content/drive/MyDrive/DS_Projects/Playground_Series/Ps4e12_Regression_Insuranse_Premium_Prediction/Data/')\n","\n","train = pd.read_csv(os.path.join(base_path, 'train.csv'))\n","test = pd.read_csv(os.path.join(base_path, 'test.csv'))\n","submission = pd.read_csv(os.path.join(base_path, 'sample_submission.csv'))\n","original = pd.read_csv(os.path.join(base_path, 'Insurance Premium Prediction Dataset.csv'))"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>gender</th>\n","      <th>annual_income</th>\n","      <th>marital_status</th>\n","      <th>number_of_dependents</th>\n","      <th>education_level</th>\n","      <th>occupation</th>\n","      <th>health_score</th>\n","      <th>location</th>\n","      <th>policy_type</th>\n","      <th>previous_claims</th>\n","      <th>vehicle_age</th>\n","      <th>credit_score</th>\n","      <th>insurance_duration</th>\n","      <th>policy_start_date</th>\n","      <th>customer_feedback</th>\n","      <th>smoking_status</th>\n","      <th>exercise_frequency</th>\n","      <th>property_type</th>\n","      <th>premium_amount</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>19.0</td>\n","      <td>Female</td>\n","      <td>10049.0</td>\n","      <td>Married</td>\n","      <td>1.0</td>\n","      <td>Bachelor's</td>\n","      <td>Self-Employed</td>\n","      <td>22.598761</td>\n","      <td>Urban</td>\n","      <td>Premium</td>\n","      <td>2.0</td>\n","      <td>17.0</td>\n","      <td>372.0</td>\n","      <td>5.0</td>\n","      <td>2023-12-23 15:21:39.134960</td>\n","      <td>Poor</td>\n","      <td>No</td>\n","      <td>Weekly</td>\n","      <td>House</td>\n","      <td>2869.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>39.0</td>\n","      <td>Female</td>\n","      <td>31678.0</td>\n","      <td>Divorced</td>\n","      <td>3.0</td>\n","      <td>Master's</td>\n","      <td>NaN</td>\n","      <td>15.569731</td>\n","      <td>Rural</td>\n","      <td>Comprehensive</td>\n","      <td>1.0</td>\n","      <td>12.0</td>\n","      <td>694.0</td>\n","      <td>2.0</td>\n","      <td>2023-06-12 15:21:39.111551</td>\n","      <td>Average</td>\n","      <td>Yes</td>\n","      <td>Monthly</td>\n","      <td>House</td>\n","      <td>1483.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>23.0</td>\n","      <td>Male</td>\n","      <td>25602.0</td>\n","      <td>Divorced</td>\n","      <td>3.0</td>\n","      <td>High School</td>\n","      <td>Self-Employed</td>\n","      <td>47.177549</td>\n","      <td>Suburban</td>\n","      <td>Premium</td>\n","      <td>1.0</td>\n","      <td>14.0</td>\n","      <td>NaN</td>\n","      <td>3.0</td>\n","      <td>2023-09-30 15:21:39.221386</td>\n","      <td>Good</td>\n","      <td>Yes</td>\n","      <td>Weekly</td>\n","      <td>House</td>\n","      <td>567.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>21.0</td>\n","      <td>Male</td>\n","      <td>141855.0</td>\n","      <td>Married</td>\n","      <td>2.0</td>\n","      <td>Bachelor's</td>\n","      <td>NaN</td>\n","      <td>10.938144</td>\n","      <td>Rural</td>\n","      <td>Basic</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>367.0</td>\n","      <td>1.0</td>\n","      <td>2024-06-12 15:21:39.226954</td>\n","      <td>Poor</td>\n","      <td>Yes</td>\n","      <td>Daily</td>\n","      <td>Apartment</td>\n","      <td>765.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>21.0</td>\n","      <td>Male</td>\n","      <td>39651.0</td>\n","      <td>Single</td>\n","      <td>1.0</td>\n","      <td>Bachelor's</td>\n","      <td>Self-Employed</td>\n","      <td>20.376094</td>\n","      <td>Rural</td>\n","      <td>Premium</td>\n","      <td>0.0</td>\n","      <td>8.0</td>\n","      <td>598.0</td>\n","      <td>4.0</td>\n","      <td>2021-12-01 15:21:39.252145</td>\n","      <td>Poor</td>\n","      <td>Yes</td>\n","      <td>Weekly</td>\n","      <td>House</td>\n","      <td>2022.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1199995</th>\n","      <td>36.0</td>\n","      <td>Female</td>\n","      <td>27316.0</td>\n","      <td>Married</td>\n","      <td>0.0</td>\n","      <td>Master's</td>\n","      <td>Unemployed</td>\n","      <td>13.772907</td>\n","      <td>Urban</td>\n","      <td>Premium</td>\n","      <td>NaN</td>\n","      <td>5.0</td>\n","      <td>372.0</td>\n","      <td>3.0</td>\n","      <td>2023-05-03 15:21:39.257696</td>\n","      <td>Poor</td>\n","      <td>No</td>\n","      <td>Daily</td>\n","      <td>Apartment</td>\n","      <td>1303.0</td>\n","    </tr>\n","    <tr>\n","      <th>1199996</th>\n","      <td>54.0</td>\n","      <td>Male</td>\n","      <td>35786.0</td>\n","      <td>Divorced</td>\n","      <td>NaN</td>\n","      <td>Master's</td>\n","      <td>Self-Employed</td>\n","      <td>11.483482</td>\n","      <td>Rural</td>\n","      <td>Comprehensive</td>\n","      <td>NaN</td>\n","      <td>10.0</td>\n","      <td>597.0</td>\n","      <td>4.0</td>\n","      <td>2022-09-10 15:21:39.134960</td>\n","      <td>Poor</td>\n","      <td>No</td>\n","      <td>Weekly</td>\n","      <td>Apartment</td>\n","      <td>821.0</td>\n","    </tr>\n","    <tr>\n","      <th>1199997</th>\n","      <td>19.0</td>\n","      <td>Male</td>\n","      <td>51884.0</td>\n","      <td>Divorced</td>\n","      <td>0.0</td>\n","      <td>Master's</td>\n","      <td>NaN</td>\n","      <td>14.724469</td>\n","      <td>Suburban</td>\n","      <td>Basic</td>\n","      <td>0.0</td>\n","      <td>19.0</td>\n","      <td>NaN</td>\n","      <td>6.0</td>\n","      <td>2021-05-25 15:21:39.106582</td>\n","      <td>Good</td>\n","      <td>No</td>\n","      <td>Monthly</td>\n","      <td>Condo</td>\n","      <td>371.0</td>\n","    </tr>\n","    <tr>\n","      <th>1199998</th>\n","      <td>55.0</td>\n","      <td>Male</td>\n","      <td>NaN</td>\n","      <td>Single</td>\n","      <td>1.0</td>\n","      <td>PhD</td>\n","      <td>NaN</td>\n","      <td>18.547381</td>\n","      <td>Suburban</td>\n","      <td>Premium</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","      <td>407.0</td>\n","      <td>4.0</td>\n","      <td>2021-09-19 15:21:39.190215</td>\n","      <td>Poor</td>\n","      <td>No</td>\n","      <td>Daily</td>\n","      <td>Apartment</td>\n","      <td>596.0</td>\n","    </tr>\n","    <tr>\n","      <th>1199999</th>\n","      <td>21.0</td>\n","      <td>Female</td>\n","      <td>NaN</td>\n","      <td>Divorced</td>\n","      <td>0.0</td>\n","      <td>PhD</td>\n","      <td>NaN</td>\n","      <td>10.125323</td>\n","      <td>Rural</td>\n","      <td>Premium</td>\n","      <td>0.0</td>\n","      <td>18.0</td>\n","      <td>502.0</td>\n","      <td>6.0</td>\n","      <td>2020-08-26 15:21:39.155231</td>\n","      <td>Good</td>\n","      <td>Yes</td>\n","      <td>Monthly</td>\n","      <td>House</td>\n","      <td>2480.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1200000 rows × 20 columns</p>\n","</div>"],"text/plain":["          age  gender  annual_income marital_status  number_of_dependents  \\\n","id                                                                          \n","0        19.0  Female        10049.0        Married                   1.0   \n","1        39.0  Female        31678.0       Divorced                   3.0   \n","2        23.0    Male        25602.0       Divorced                   3.0   \n","3        21.0    Male       141855.0        Married                   2.0   \n","4        21.0    Male        39651.0         Single                   1.0   \n","...       ...     ...            ...            ...                   ...   \n","1199995  36.0  Female        27316.0        Married                   0.0   \n","1199996  54.0    Male        35786.0       Divorced                   NaN   \n","1199997  19.0    Male        51884.0       Divorced                   0.0   \n","1199998  55.0    Male            NaN         Single                   1.0   \n","1199999  21.0  Female            NaN       Divorced                   0.0   \n","\n","        education_level     occupation  health_score  location    policy_type  \\\n","id                                                                              \n","0            Bachelor's  Self-Employed     22.598761     Urban        Premium   \n","1              Master's            NaN     15.569731     Rural  Comprehensive   \n","2           High School  Self-Employed     47.177549  Suburban        Premium   \n","3            Bachelor's            NaN     10.938144     Rural          Basic   \n","4            Bachelor's  Self-Employed     20.376094     Rural        Premium   \n","...                 ...            ...           ...       ...            ...   \n","1199995        Master's     Unemployed     13.772907     Urban        Premium   \n","1199996        Master's  Self-Employed     11.483482     Rural  Comprehensive   \n","1199997        Master's            NaN     14.724469  Suburban          Basic   \n","1199998             PhD            NaN     18.547381  Suburban        Premium   \n","1199999             PhD            NaN     10.125323     Rural        Premium   \n","\n","         previous_claims  vehicle_age  credit_score  insurance_duration  \\\n","id                                                                        \n","0                    2.0         17.0         372.0                 5.0   \n","1                    1.0         12.0         694.0                 2.0   \n","2                    1.0         14.0           NaN                 3.0   \n","3                    1.0          0.0         367.0                 1.0   \n","4                    0.0          8.0         598.0                 4.0   \n","...                  ...          ...           ...                 ...   \n","1199995              NaN          5.0         372.0                 3.0   \n","1199996              NaN         10.0         597.0                 4.0   \n","1199997              0.0         19.0           NaN                 6.0   \n","1199998              1.0          7.0         407.0                 4.0   \n","1199999              0.0         18.0         502.0                 6.0   \n","\n","                  policy_start_date customer_feedback smoking_status  \\\n","id                                                                     \n","0        2023-12-23 15:21:39.134960              Poor             No   \n","1        2023-06-12 15:21:39.111551           Average            Yes   \n","2        2023-09-30 15:21:39.221386              Good            Yes   \n","3        2024-06-12 15:21:39.226954              Poor            Yes   \n","4        2021-12-01 15:21:39.252145              Poor            Yes   \n","...                             ...               ...            ...   \n","1199995  2023-05-03 15:21:39.257696              Poor             No   \n","1199996  2022-09-10 15:21:39.134960              Poor             No   \n","1199997  2021-05-25 15:21:39.106582              Good             No   \n","1199998  2021-09-19 15:21:39.190215              Poor             No   \n","1199999  2020-08-26 15:21:39.155231              Good            Yes   \n","\n","        exercise_frequency property_type  premium_amount  \n","id                                                        \n","0                   Weekly         House          2869.0  \n","1                  Monthly         House          1483.0  \n","2                   Weekly         House           567.0  \n","3                    Daily     Apartment           765.0  \n","4                   Weekly         House          2022.0  \n","...                    ...           ...             ...  \n","1199995              Daily     Apartment          1303.0  \n","1199996             Weekly     Apartment           821.0  \n","1199997            Monthly         Condo           371.0  \n","1199998              Daily     Apartment           596.0  \n","1199999            Monthly         House          2480.0  \n","\n","[1200000 rows x 20 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["train.set_index('id', inplace=True)\n","test.set_index('id', inplace=True)\n","\n","# Renaming columns for consistency\n","train.columns = train.columns.str.lower()\n","test.columns = test.columns.str.lower()\n","original.columns = original.columns.str.lower()\n","train.columns = [col.replace(\" \", \"_\") for col in train.columns]\n","test.columns = [col.replace(\" \", \"_\") for col in test.columns]\n","original.columns = [col.replace(\" \", \"_\") for col in original.columns]\n","original  = original[train.columns]\n","original = original[original['premium_amount'] > 0]\n","\n","train"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["<Axes: >"]},"execution_count":4,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Lets look at the original data target distribution\n","original['premium_amount_log'] = np.log1p(original['premium_amount'])\n","original['premium_amount_log'].hist(bins=100)\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["<Axes: >"]},"execution_count":5,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["train['premium_amount_log'] = np.log1p(train['premium_amount'])\n","train['premium_amount_log'].hist(bins=100)\n"]},{"cell_type":"markdown","metadata":{},"source":["# **Fusing original data**"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>gender</th>\n","      <th>annual_income</th>\n","      <th>marital_status</th>\n","      <th>number_of_dependents</th>\n","      <th>education_level</th>\n","      <th>occupation</th>\n","      <th>health_score</th>\n","      <th>location</th>\n","      <th>policy_type</th>\n","      <th>...</th>\n","      <th>vehicle_age</th>\n","      <th>credit_score</th>\n","      <th>insurance_duration</th>\n","      <th>policy_start_date</th>\n","      <th>customer_feedback</th>\n","      <th>smoking_status</th>\n","      <th>exercise_frequency</th>\n","      <th>property_type</th>\n","      <th>premium_amount</th>\n","      <th>premium_amount_log</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>32.0</td>\n","      <td>Male</td>\n","      <td>52915.0</td>\n","      <td>Married</td>\n","      <td>NaN</td>\n","      <td>High School</td>\n","      <td>NaN</td>\n","      <td>33.152657</td>\n","      <td>Urban</td>\n","      <td>Premium</td>\n","      <td>...</td>\n","      <td>16.0</td>\n","      <td>563.0</td>\n","      <td>4.0</td>\n","      <td>2022-09-01 15:21:39.181605</td>\n","      <td>NaN</td>\n","      <td>Yes</td>\n","      <td>Monthly</td>\n","      <td>Apartment</td>\n","      <td>3561.0</td>\n","      <td>8.178077</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>28.0</td>\n","      <td>Male</td>\n","      <td>67419.0</td>\n","      <td>Single</td>\n","      <td>2.0</td>\n","      <td>Master's</td>\n","      <td>Employed</td>\n","      <td>75.174393</td>\n","      <td>Suburban</td>\n","      <td>Basic</td>\n","      <td>...</td>\n","      <td>4.0</td>\n","      <td>312.0</td>\n","      <td>4.0</td>\n","      <td>2020-12-02 15:21:39.091080</td>\n","      <td>Good</td>\n","      <td>Yes</td>\n","      <td>Weekly</td>\n","      <td>Condo</td>\n","      <td>438.0</td>\n","      <td>6.084499</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>62.0</td>\n","      <td>Male</td>\n","      <td>1201.0</td>\n","      <td>Single</td>\n","      <td>1.0</td>\n","      <td>Bachelor's</td>\n","      <td>Employed</td>\n","      <td>26.874350</td>\n","      <td>Urban</td>\n","      <td>Premium</td>\n","      <td>...</td>\n","      <td>12.0</td>\n","      <td>756.0</td>\n","      <td>1.0</td>\n","      <td>2023-12-31 15:21:39.155231</td>\n","      <td>Poor</td>\n","      <td>Yes</td>\n","      <td>Rarely</td>\n","      <td>Condo</td>\n","      <td>119.0</td>\n","      <td>4.787492</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>53.0</td>\n","      <td>Female</td>\n","      <td>2878.0</td>\n","      <td>Divorced</td>\n","      <td>0.0</td>\n","      <td>Master's</td>\n","      <td>Unemployed</td>\n","      <td>19.677060</td>\n","      <td>Suburban</td>\n","      <td>Premium</td>\n","      <td>...</td>\n","      <td>7.0</td>\n","      <td>333.0</td>\n","      <td>8.0</td>\n","      <td>2021-02-12 15:21:39.225916</td>\n","      <td>Poor</td>\n","      <td>Yes</td>\n","      <td>Rarely</td>\n","      <td>House</td>\n","      <td>557.0</td>\n","      <td>6.324359</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>28.0</td>\n","      <td>Male</td>\n","      <td>10451.0</td>\n","      <td>Married</td>\n","      <td>1.0</td>\n","      <td>Master's</td>\n","      <td>Employed</td>\n","      <td>30.232698</td>\n","      <td>Urban</td>\n","      <td>Premium</td>\n","      <td>...</td>\n","      <td>8.0</td>\n","      <td>569.0</td>\n","      <td>4.0</td>\n","      <td>2020-08-16 15:21:39.147735</td>\n","      <td>Poor</td>\n","      <td>No</td>\n","      <td>Rarely</td>\n","      <td>House</td>\n","      <td>990.0</td>\n","      <td>6.898715</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>999995</th>\n","      <td>29.0</td>\n","      <td>Male</td>\n","      <td>5950.0</td>\n","      <td>Single</td>\n","      <td>2.0</td>\n","      <td>PhD</td>\n","      <td>Employed</td>\n","      <td>38.922601</td>\n","      <td>Urban</td>\n","      <td>Premium</td>\n","      <td>...</td>\n","      <td>12.0</td>\n","      <td>728.0</td>\n","      <td>7.0</td>\n","      <td>2023-10-15 15:21:39.134960</td>\n","      <td>Good</td>\n","      <td>No</td>\n","      <td>Monthly</td>\n","      <td>House</td>\n","      <td>1332.0</td>\n","      <td>7.195187</td>\n","    </tr>\n","    <tr>\n","      <th>999996</th>\n","      <td>23.0</td>\n","      <td>Male</td>\n","      <td>62410.0</td>\n","      <td>Divorced</td>\n","      <td>3.0</td>\n","      <td>High School</td>\n","      <td>Unemployed</td>\n","      <td>42.685823</td>\n","      <td>Suburban</td>\n","      <td>Comprehensive</td>\n","      <td>...</td>\n","      <td>12.0</td>\n","      <td>398.0</td>\n","      <td>6.0</td>\n","      <td>2022-05-15 15:21:39.246098</td>\n","      <td>Poor</td>\n","      <td>Yes</td>\n","      <td>Daily</td>\n","      <td>Condo</td>\n","      <td>1155.0</td>\n","      <td>7.052721</td>\n","    </tr>\n","    <tr>\n","      <th>999997</th>\n","      <td>28.0</td>\n","      <td>Male</td>\n","      <td>18227.0</td>\n","      <td>Divorced</td>\n","      <td>2.0</td>\n","      <td>PhD</td>\n","      <td>Employed</td>\n","      <td>45.881225</td>\n","      <td>Suburban</td>\n","      <td>Comprehensive</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>NaN</td>\n","      <td>2.0</td>\n","      <td>2021-06-08 15:21:39.192212</td>\n","      <td>NaN</td>\n","      <td>Yes</td>\n","      <td>Weekly</td>\n","      <td>Apartment</td>\n","      <td>963.0</td>\n","      <td>6.871091</td>\n","    </tr>\n","    <tr>\n","      <th>999998</th>\n","      <td>60.0</td>\n","      <td>Female</td>\n","      <td>19824.0</td>\n","      <td>Divorced</td>\n","      <td>4.0</td>\n","      <td>Bachelor's</td>\n","      <td>Unemployed</td>\n","      <td>59.882901</td>\n","      <td>Rural</td>\n","      <td>Basic</td>\n","      <td>...</td>\n","      <td>13.0</td>\n","      <td>725.0</td>\n","      <td>4.0</td>\n","      <td>2023-06-01 15:21:39.180231</td>\n","      <td>Good</td>\n","      <td>No</td>\n","      <td>Weekly</td>\n","      <td>House</td>\n","      <td>487.0</td>\n","      <td>6.190315</td>\n","    </tr>\n","    <tr>\n","      <th>999999</th>\n","      <td>54.0</td>\n","      <td>Male</td>\n","      <td>2631.0</td>\n","      <td>Divorced</td>\n","      <td>1.0</td>\n","      <td>PhD</td>\n","      <td>Self-Employed</td>\n","      <td>19.848858</td>\n","      <td>Rural</td>\n","      <td>Comprehensive</td>\n","      <td>...</td>\n","      <td>9.0</td>\n","      <td>382.0</td>\n","      <td>5.0</td>\n","      <td>2019-10-14 15:21:39.290098</td>\n","      <td>Good</td>\n","      <td>Yes</td>\n","      <td>Weekly</td>\n","      <td>Condo</td>\n","      <td>654.0</td>\n","      <td>6.484635</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000000 rows × 21 columns</p>\n","</div>"],"text/plain":["         age  gender  annual_income marital_status  number_of_dependents  \\\n","0       32.0    Male        52915.0        Married                   NaN   \n","1       28.0    Male        67419.0         Single                   2.0   \n","2       62.0    Male         1201.0         Single                   1.0   \n","3       53.0  Female         2878.0       Divorced                   0.0   \n","4       28.0    Male        10451.0        Married                   1.0   \n","...      ...     ...            ...            ...                   ...   \n","999995  29.0    Male         5950.0         Single                   2.0   \n","999996  23.0    Male        62410.0       Divorced                   3.0   \n","999997  28.0    Male        18227.0       Divorced                   2.0   \n","999998  60.0  Female        19824.0       Divorced                   4.0   \n","999999  54.0    Male         2631.0       Divorced                   1.0   \n","\n","       education_level     occupation  health_score  location    policy_type  \\\n","0          High School            NaN     33.152657     Urban        Premium   \n","1             Master's       Employed     75.174393  Suburban          Basic   \n","2           Bachelor's       Employed     26.874350     Urban        Premium   \n","3             Master's     Unemployed     19.677060  Suburban        Premium   \n","4             Master's       Employed     30.232698     Urban        Premium   \n","...                ...            ...           ...       ...            ...   \n","999995             PhD       Employed     38.922601     Urban        Premium   \n","999996     High School     Unemployed     42.685823  Suburban  Comprehensive   \n","999997             PhD       Employed     45.881225  Suburban  Comprehensive   \n","999998      Bachelor's     Unemployed     59.882901     Rural          Basic   \n","999999             PhD  Self-Employed     19.848858     Rural  Comprehensive   \n","\n","        ...  vehicle_age  credit_score  insurance_duration  \\\n","0       ...         16.0         563.0                 4.0   \n","1       ...          4.0         312.0                 4.0   \n","2       ...         12.0         756.0                 1.0   \n","3       ...          7.0         333.0                 8.0   \n","4       ...          8.0         569.0                 4.0   \n","...     ...          ...           ...                 ...   \n","999995  ...         12.0         728.0                 7.0   \n","999996  ...         12.0         398.0                 6.0   \n","999997  ...          2.0           NaN                 2.0   \n","999998  ...         13.0         725.0                 4.0   \n","999999  ...          9.0         382.0                 5.0   \n","\n","                 policy_start_date customer_feedback smoking_status  \\\n","0       2022-09-01 15:21:39.181605               NaN            Yes   \n","1       2020-12-02 15:21:39.091080              Good            Yes   \n","2       2023-12-31 15:21:39.155231              Poor            Yes   \n","3       2021-02-12 15:21:39.225916              Poor            Yes   \n","4       2020-08-16 15:21:39.147735              Poor             No   \n","...                            ...               ...            ...   \n","999995  2023-10-15 15:21:39.134960              Good             No   \n","999996  2022-05-15 15:21:39.246098              Poor            Yes   \n","999997  2021-06-08 15:21:39.192212               NaN            Yes   \n","999998  2023-06-01 15:21:39.180231              Good             No   \n","999999  2019-10-14 15:21:39.290098              Good            Yes   \n","\n","       exercise_frequency property_type premium_amount  premium_amount_log  \n","0                 Monthly     Apartment         3561.0            8.178077  \n","1                  Weekly         Condo          438.0            6.084499  \n","2                  Rarely         Condo          119.0            4.787492  \n","3                  Rarely         House          557.0            6.324359  \n","4                  Rarely         House          990.0            6.898715  \n","...                   ...           ...            ...                 ...  \n","999995            Monthly         House         1332.0            7.195187  \n","999996              Daily         Condo         1155.0            7.052721  \n","999997             Weekly     Apartment          963.0            6.871091  \n","999998             Weekly         House          487.0            6.190315  \n","999999             Weekly         Condo          654.0            6.484635  \n","\n","[1000000 rows x 21 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["train_fused = pd.concat([train, original, original, original])\n","train_fused = train_fused.sample(n=1_000_000, random_state=42)\n","train_fused = train_fused.reset_index(drop=True)\n","train_fused"]},{"cell_type":"markdown","metadata":{},"source":["# **Feature Engineering**"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def create_date_features(df):\n","    # Basic date features\n","    df['policy_start'] = pd.to_datetime(df['policy_start_date'])\n","    df['year'] = df['policy_start'].dt.year\n","    df['month'] = df['policy_start'].dt.month\n","    df['day'] = df['policy_start'].dt.day\n","    df['week_of_year'] = df['policy_start'].dt.isocalendar().week.astype('int')\n","    df['day_of_week'] = df['policy_start'].dt.day_name()\n","    df['month_name'] = df['policy_start'].dt.month_name()\n","    df['quarter'] = df['policy_start'].dt.quarter\n","\n","    # Cyclical encoding\n","    for col, max_val in [('year', 1), ('month', 12), ('day', 31)]:\n","        df[f'{col}_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n","        df[f'{col}_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n","\n","    # Binary flags\n","    df['is_weekend'] = df['policy_start'].dt.dayofweek.isin([5,6]).astype(int)\n","    df['is_month_end'] = df['policy_start'].dt.is_month_end.astype(int)\n","    df['is_month_start'] = df['policy_start'].dt.is_month_start.astype(int)\n","    df['is_quarter_end'] = df['policy_start'].dt.is_quarter_end.astype(int)\n","    df['is_quarter_start'] = df['policy_start'].dt.is_quarter_start.astype(int)\n","\n","    # Time-based calculations\n","    df['policy_age_days'] = (df['policy_start'].max() - df['policy_start']).dt.days\n","    df['week_of_month'] = df['day'].apply(lambda x: (x-1)//7 + 1)\n","    df['days_in_month'] = df['policy_start'].dt.days_in_month\n","    df['days_remaining_in_month'] = df['days_in_month'] - df['day']\n","\n","    # Seasonal mapping\n","    season_map = {12:'winter', 1:'winter', 2:'winter',\n","                  3:'spring', 4:'spring', 5:'spring',\n","                  6:'summer', 7:'summer', 8:'summer',\n","                  9:'fall', 10:'fall', 11:'fall'}\n","    df['season'] = df['month'].map(season_map)\n","\n","    return df"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def create_advanced_features(df, is_training=True):\n","    \"\"\"\n","    Create advanced features for insurance premium prediction with proper scaling\n","    \"\"\"\n","    df = df.copy()\n","\n","    # Store scaling factors during training\n","    if is_training:\n","        global scale_params\n","        scale_params = {\n","            'health_score_mean': df['health_score'].mean(),\n","            'health_score_std': df['health_score'].std(),\n","            'credit_score_mean': df['credit_score'].mean(),\n","            'credit_score_std': df['credit_score'].std(),\n","            'customer_feedback_map': {\n","                'Poor': 0.0,    # Higher risk\n","                'Average': 0.5, # Medium risk\n","                'Good': 1.0     # Lower risk\n","            },\n","            'exercise_frequency_map': {\n","                'Rarely': 0.0,   # Highest risk\n","                'Monthly': 0.33, # High risk\n","                'Weekly': 0.66,  # Low risk\n","                'Daily': 1.0     # Lowest risk\n","            },\n","            'smoking_map': {\n","                'Yes': 1.0,  # Higher risk\n","                'No': 0.0    # Lower risk\n","            },\n","            'marital_risk_map': {\n","                'Single': 1.0,    # Base risk\n","                'Married': 0.8,   # Lower risk (shared responsibility)\n","                'Divorced': 1.2   # Higher risk (potentially more financial stress)\n","            },\n","            'property_risk_map': {\n","                'Apartment': 1.0,  # Base risk\n","                'House': 1.5,     # Higher risk (more value/larger space)\n","                'Condo': 1.2      # Medium risk\n","            }\n","        }\n","\n","    # 1. Date-based features\n","    df = create_date_features(df)\n","\n","    # 2. Income-based features with proper scaling\n","    df['income_per_dependent'] = df['annual_income'] / (df['number_of_dependents'] + 1)\n","    df['income_bracket'] = pd.qcut(df['annual_income'], q=5,\n","                                 labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n","\n","    # 3. Risk Score Combinations with standardization\n","    # Standardize health and credit scores\n","    df['health_score_std'] = (df['health_score'] - scale_params['health_score_mean']) / scale_params['health_score_std']\n","    df['credit_score_std'] = (df['credit_score'] - scale_params['credit_score_mean']) / scale_params['credit_score_std']\n","\n","    # Combined risk score (now both features are on same scale)\n","    df['total_risk_score'] = df['health_score_std'] + df['credit_score_std']\n","\n","    # Claims ratio with insurance duration\n","    df['claims_to_duration_ratio'] = df['previous_claims'] / (df['insurance_duration'] + 1)\n","\n","    # 4. Age-related interactions\n","    df['vehicle_to_driver_age_ratio'] = df['vehicle_age'] / df['age']\n","    df['is_young_driver'] = (df['age'] < 25).astype(int)\n","    df['is_senior_driver'] = (df['age'] > 65).astype(int)\n","\n","    # 5. Lifestyle Score (normalized to 0-1 range)\n","    df['exercise_score'] = df['exercise_frequency'].map(scale_params['exercise_frequency_map'])\n","    df['smoking_risk'] = df['smoking_status'].map(scale_params['smoking_map'])\n","    df['lifestyle_score'] = (\n","        df['exercise_score'] * 0.4 +    # Exercise has significant impact\n","        (1 - df['smoking_risk']) * 0.4 + # Non-smoking is positive\n","        (df['health_score_std'] > 0) * 0.2  # Above average health is positive\n","    )\n","\n","    # 6. Location-based features\n","    if is_training:\n","        scale_params['location_risk_map'] = df.groupby('location')['previous_claims'].mean()\n","        scale_params['location_credit_map'] = df.groupby('location')['credit_score'].mean()\n","\n","    df['location_risk'] = df['location'].map(scale_params['location_risk_map'])\n","    df['location_avg_credit'] = df['location'].map(scale_params['location_credit_map'])\n","\n","    # 7. Complex Interaction Features\n","    df['customer_feedback_score'] = df['customer_feedback'].map(scale_params['customer_feedback_map'])\n","\n","    # Weighted responsibility score (all components now 0-1 scaled)\n","    df['responsibility_score'] = (\n","        df['credit_score_std'].clip(-3, 3) * 0.4 +  # Limit outlier effect\n","        df['customer_feedback_score'] * 0.3 +\n","        (1 - df['claims_to_duration_ratio'].clip(0, 1)) * 0.3  # Lower claims is better\n","    )\n","\n","    # 8. Family and Property Risk\n","    df['marital_risk'] = df['marital_status'].map(scale_params['marital_risk_map'])\n","    df['property_risk'] = df['property_type'].map(scale_params['property_risk_map'])\n","\n","    # Combined risk factors\n","    df['family_risk_factor'] = df['marital_risk'] * (df['number_of_dependents'] + 1)\n","    df['asset_risk'] = (\n","        df['property_risk'] * 0.6 +\n","        (df['vehicle_age'] / df['vehicle_age'].max()) * 0.4  # Normalized vehicle age\n","    )\n","\n","    # 9. Customer Segment Features\n","    df['premium_segment'] = 'Standard'\n","    mask_premium = (\n","        (df['credit_score_std'] > 1) &  # Above 1 std in credit\n","        (df['previous_claims'] == 0) &   # No claims\n","        (df['health_score_std'] > 1)     # Above 1 std in health\n","    )\n","    mask_high_risk = (\n","        (df['credit_score_std'] < -1) |  # Below 1 std in credit\n","        (df['previous_claims'] > 3)       # Multiple claims\n","    )\n","\n","    df.loc[mask_premium, 'premium_segment'] = 'Premium'\n","    df.loc[mask_high_risk, 'premium_segment'] = 'High Risk'\n","\n","    # 10. Additional Ratio Features\n","    df['claims_per_year'] = df['previous_claims'] / (df['insurance_duration'] + 1)\n","    df['dependent_income_ratio'] = df['number_of_dependents'] / df['annual_income']\n","\n","    # Drop intermediate columns\n","    intermediate_cols = ['health_score_std', 'credit_score_std', 'exercise_score',\n","                        'smoking_risk', 'customer_feedback_score', 'marital_risk',\n","                        'property_risk']\n","    df = df.drop(columns=[col for col in intermediate_cols if col in df.columns])\n","\n","    return df"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training data transformation successful!\n","Test data transformation successful!\n"]}],"source":["try:\n","    # Transform training data\n","    train_transformed = create_advanced_features(train_fused, is_training=True)\n","    print(\"Training data transformation successful!\")\n","\n","    # Transform test data\n","    test_transformed = create_advanced_features(test, is_training=False)\n","    print(\"Test data transformation successful!\")\n","\n","except Exception as e:\n","    print(f\"An error occurred: {str(e)}\")\n","    print(\"Please check your data types and column names.\")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["train_transformed.to_parquet(os.path.join(base_path, \"train_transformed_original.parquet\"))\n","test_transformed.to_parquet(os.path.join(base_path, \"test_transformed_original.parquet\"))"]},{"cell_type":"markdown","metadata":{"id":"XaRagHNL6IcG"},"source":["# **Autogluon Train 8 hours**"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1735449634604,"user":{"displayName":"Fabián Alberto Reyes Madrid","userId":"11772448517145602220"},"user_tz":180},"id":"PGz-o44GrhYM"},"outputs":[],"source":["# # Create the AutoGluon scorer using sklearn's implementation\n","# rmsle_scorer = make_scorer(\n","#     name='rmsle',\n","#     score_func=sklearn.metrics.root_mean_squared_log_error,\n","#     optimum=0,\n","#     greater_is_better=False,\n","#     needs_pred=True\n","# )"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["train_transformed['premium_amount_log'] = np.log1p(train_transformed['premium_amount'])"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"HW8tAB-b6Tcu"},"outputs":[{"name":"stderr","output_type":"stream","text":["Warning: path already exists! This predictor may overwrite an existing predictor! path=\"Autogluon/202412_ps4s12_8hr_training_original\"\n","Verbosity: 2 (Standard Logging)\n","=================== System Info ===================\n","AutoGluon Version:  1.1.1\n","Python Version:     3.10.14\n","Operating System:   Darwin\n","Platform Machine:   arm64\n","Platform Version:   Darwin Kernel Version 23.0.0: Fri Sep 15 14:43:05 PDT 2023; root:xnu-10002.1.13~1/RELEASE_ARM64_T6020\n","CPU Count:          10\n","Memory Avail:       5.54 GB / 16.00 GB (34.6%)\n","Disk Space Avail:   79.90 GB / 460.43 GB (17.4%)\n","===================================================\n","Presets specified: ['best_quality']\n","Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n","Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=5, num_bag_sets=2\n","DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n","\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n","\tRunning DyStack for up to 7200s of the 28800s of remaining time (25%).\n","\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n","2024-12-30 02:23:09,932\tINFO worker.py:1743 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n","\t\tContext path: \"Autogluon/202412_ps4s12_8hr_training_original/ds_sub_fit/sub_fit_ho\"\n","\u001b[36m(_dystack pid=15693)\u001b[0m Running DyStack sub-fit ...\n","\u001b[36m(_dystack pid=15693)\u001b[0m Beginning AutoGluon training ... Time limit = 7197s\n","\u001b[36m(_dystack pid=15693)\u001b[0m AutoGluon will save models to \"Autogluon/202412_ps4s12_8hr_training_original/ds_sub_fit/sub_fit_ho\"\n","\u001b[36m(_dystack pid=15693)\u001b[0m Train Data Rows:    888888\n","\u001b[36m(_dystack pid=15693)\u001b[0m Train Data Columns: 59\n","\u001b[36m(_dystack pid=15693)\u001b[0m Label Column:       premium_amount_log\n","\u001b[36m(_dystack pid=15693)\u001b[0m Problem Type:       regression\n","\u001b[36m(_dystack pid=15693)\u001b[0m Preprocessing data ...\n","\u001b[36m(_dystack pid=15693)\u001b[0m Using Feature Generators to preprocess the data ...\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tAvailable Memory:                    6003.68 MB\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tTrain Data (Original)  Memory Usage: 1085.46 MB (18.1% of available memory)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tWarning: Data size prior to feature transformation consumes 18.1% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tStage 1 Generators:\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\t\tNote: Converting 8 features to boolean dtype as they only contain 2 unique values.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tStage 2 Generators:\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tStage 3 Generators:\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\tFitting DatetimeFeatureGenerator...\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tStage 4 Generators:\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tStage 5 Generators:\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tUseless Original Features (Count: 2): ['year_cos', 'is_senior_driver']\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\tThese features carry no predictive signal and should be manually investigated.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\tThis is typically a feature which has the same value for all rows.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\tThese features do not need to be present at inference time.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tUnused Original Features (Count: 1): ['claims_per_year']\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\tThese features do not need to be present at inference time.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\t('float', []) : 1 | ['claims_per_year']\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\t('category', [])                   :  1 | ['income_bracket']\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\t('datetime', [])                   :  1 | ['policy_start']\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\t('float', [])                      : 24 | ['age', 'annual_income', 'number_of_dependents', 'health_score', 'previous_claims', ...]\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\t('int', [])                        : 15 | ['year', 'month', 'day', 'week_of_year', 'quarter', ...]\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\t('object', [])                     : 14 | ['gender', 'marital_status', 'education_level', 'occupation', 'location', ...]\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\t('object', ['datetime_as_object']) :  1 | ['policy_start_date']\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\t('category', [])             : 13 | ['marital_status', 'education_level', 'occupation', 'location', 'policy_type', ...]\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\t('float', [])                : 24 | ['age', 'annual_income', 'number_of_dependents', 'health_score', 'previous_claims', ...]\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\t('int', [])                  :  9 | ['year', 'month', 'day', 'week_of_year', 'quarter', ...]\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\t('int', ['bool'])            :  8 | ['gender', 'smoking_status', 'is_weekend', 'is_month_end', 'is_month_start', ...]\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\t('int', ['datetime_as_int']) :  2 | ['policy_start_date', 'policy_start_date.dayofweek']\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t5.5s = Fit runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t56 features in original data used to generate 56 features in processed data.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tTrain Data (Processed) Memory Usage: 234.82 MB (3.8% of available memory)\n","\u001b[36m(_dystack pid=15693)\u001b[0m Data preprocessing and feature engineering runtime = 6.24s ...\n","\u001b[36m(_dystack pid=15693)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n","\u001b[36m(_dystack pid=15693)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n","\u001b[36m(_dystack pid=15693)\u001b[0m User-specified model hyperparameters to be fit:\n","\u001b[36m(_dystack pid=15693)\u001b[0m {\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","\u001b[36m(_dystack pid=15693)\u001b[0m }\n","\u001b[36m(_dystack pid=15693)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n","\u001b[36m(_dystack pid=15693)\u001b[0m Excluded models: ['RF', 'NN_TORCH', 'KNN', 'FASTAI'] (Specified by `excluded_model_types`)\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting 54 L1 models ...\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4792.92s of the 7191.18s of remaining time.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 31.31% memory usage per fold, 62.61%/80.00% total).\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=31.31%)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_ray_fit pid=15745)\u001b[0m [1000]\tvalid_set's rmse: 1.14205\n","\u001b[36m(_ray_fit pid=15745)\u001b[0m [2000]\tvalid_set's rmse: 1.13729\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n","\u001b[36m(_ray_fit pid=15745)\u001b[0m [3000]\tvalid_set's rmse: 1.13311\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=15745)\u001b[0m [4000]\tvalid_set's rmse: 1.12927\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=15745)\u001b[0m [5000]\tvalid_set's rmse: 1.12565\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=15745)\u001b[0m [6000]\tvalid_set's rmse: 1.12228\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=15744)\u001b[0m [7000]\tvalid_set's rmse: 1.12225\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=15744)\u001b[0m [8000]\tvalid_set's rmse: 1.11923\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=15744)\u001b[0m [9000]\tvalid_set's rmse: 1.11623\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=15744)\u001b[0m [10000]\tvalid_set's rmse: 1.11338\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=16448)\u001b[0m [1000]\tvalid_set's rmse: 1.14506\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=16448)\u001b[0m [2000]\tvalid_set's rmse: 1.13998\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=16448)\u001b[0m [3000]\tvalid_set's rmse: 1.1356\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=16448)\u001b[0m [4000]\tvalid_set's rmse: 1.13168\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=16448)\u001b[0m [5000]\tvalid_set's rmse: 1.12808\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=16457)\u001b[0m [5000]\tvalid_set's rmse: 1.13143\n","\u001b[36m(_ray_fit pid=16448)\u001b[0m [6000]\tvalid_set's rmse: 1.12452\n","\u001b[36m(_ray_fit pid=16457)\u001b[0m [6000]\tvalid_set's rmse: 1.12804\n","\u001b[36m(_ray_fit pid=16448)\u001b[0m [7000]\tvalid_set's rmse: 1.12117\n","\u001b[36m(_ray_fit pid=16457)\u001b[0m [7000]\tvalid_set's rmse: 1.12476\n","\u001b[36m(_ray_fit pid=16448)\u001b[0m [8000]\tvalid_set's rmse: 1.11809\n","\u001b[36m(_ray_fit pid=16457)\u001b[0m [8000]\tvalid_set's rmse: 1.12161\n","\u001b[36m(_ray_fit pid=16448)\u001b[0m [9000]\tvalid_set's rmse: 1.11508\n","\u001b[36m(_ray_fit pid=16457)\u001b[0m [9000]\tvalid_set's rmse: 1.11858\n","\u001b[36m(_ray_fit pid=16448)\u001b[0m [10000]\tvalid_set's rmse: 1.1123\n","\u001b[36m(_ray_fit pid=16457)\u001b[0m [10000]\tvalid_set's rmse: 1.11572\n","\u001b[36m(_ray_fit pid=17071)\u001b[0m [1000]\tvalid_set's rmse: 1.14762\n","\u001b[36m(_ray_fit pid=17071)\u001b[0m [2000]\tvalid_set's rmse: 1.14253\n","\u001b[36m(_ray_fit pid=17071)\u001b[0m [3000]\tvalid_set's rmse: 1.13812\n","\u001b[36m(_ray_fit pid=17071)\u001b[0m [4000]\tvalid_set's rmse: 1.13407\n","\u001b[36m(_ray_fit pid=17071)\u001b[0m [5000]\tvalid_set's rmse: 1.13022\n","\u001b[36m(_ray_fit pid=17071)\u001b[0m [6000]\tvalid_set's rmse: 1.1267\n","\u001b[36m(_ray_fit pid=17071)\u001b[0m [7000]\tvalid_set's rmse: 1.12339\n","\u001b[36m(_ray_fit pid=17071)\u001b[0m [8000]\tvalid_set's rmse: 1.1203\n","\u001b[36m(_ray_fit pid=17071)\u001b[0m [9000]\tvalid_set's rmse: 1.11739\n","\u001b[36m(_ray_fit pid=17071)\u001b[0m [10000]\tvalid_set's rmse: 1.11456\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_dystack pid=15693)\u001b[0m \t-1.1132\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t492.55s\t = Training   runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t132.15s\t = Validation runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 4279.73s of the 6677.99s of remaining time.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 32.72% memory usage per fold, 65.44%/80.00% total).\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=32.72%)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_ray_fit pid=17632)\u001b[0m [1000]\tvalid_set's rmse: 1.14359\n","\u001b[36m(_ray_fit pid=17632)\u001b[0m [2000]\tvalid_set's rmse: 1.1371\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=17632)\u001b[0m [3000]\tvalid_set's rmse: 1.13135\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=17632)\u001b[0m [4000]\tvalid_set's rmse: 1.1262\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=17632)\u001b[0m [5000]\tvalid_set's rmse: 1.12159\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=17633)\u001b[0m [6000]\tvalid_set's rmse: 1.11353\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=17633)\u001b[0m [7000]\tvalid_set's rmse: 1.10952\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=17632)\u001b[0m [8000]\tvalid_set's rmse: 1.10914\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=17632)\u001b[0m [9000]\tvalid_set's rmse: 1.10536\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=17632)\u001b[0m [10000]\tvalid_set's rmse: 1.10198\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=18325)\u001b[0m [1000]\tvalid_set's rmse: 1.14152\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=18325)\u001b[0m [2000]\tvalid_set's rmse: 1.13509\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=18325)\u001b[0m [3000]\tvalid_set's rmse: 1.1296\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=18325)\u001b[0m [4000]\tvalid_set's rmse: 1.12446\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=18325)\u001b[0m [5000]\tvalid_set's rmse: 1.11973\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=18327)\u001b[0m [5000]\tvalid_set's rmse: 1.12302\n","\u001b[36m(_ray_fit pid=18327)\u001b[0m \n","\u001b[36m(_ray_fit pid=18325)\u001b[0m [6000]\tvalid_set's rmse: 1.11542\n","\u001b[36m(_ray_fit pid=18327)\u001b[0m [6000]\tvalid_set's rmse: 1.1189\n","\u001b[36m(_ray_fit pid=18325)\u001b[0m [7000]\tvalid_set's rmse: 1.1113\n","\u001b[36m(_ray_fit pid=18327)\u001b[0m [7000]\tvalid_set's rmse: 1.11502\n","\u001b[36m(_ray_fit pid=18325)\u001b[0m [8000]\tvalid_set's rmse: 1.10737\n","\u001b[36m(_ray_fit pid=18327)\u001b[0m [8000]\tvalid_set's rmse: 1.11143\n","\u001b[36m(_ray_fit pid=18325)\u001b[0m [9000]\tvalid_set's rmse: 1.10378\n","\u001b[36m(_ray_fit pid=18327)\u001b[0m [9000]\tvalid_set's rmse: 1.10815\n","\u001b[36m(_ray_fit pid=18325)\u001b[0m [10000]\tvalid_set's rmse: 1.10032\n","\u001b[36m(_ray_fit pid=18327)\u001b[0m [10000]\tvalid_set's rmse: 1.10461\n","\u001b[36m(_ray_fit pid=18930)\u001b[0m [1000]\tvalid_set's rmse: 1.14452\n","\u001b[36m(_ray_fit pid=18930)\u001b[0m [2000]\tvalid_set's rmse: 1.13815\n","\u001b[36m(_ray_fit pid=18930)\u001b[0m [3000]\tvalid_set's rmse: 1.13236\n","\u001b[36m(_ray_fit pid=18930)\u001b[0m [4000]\tvalid_set's rmse: 1.12711\n","\u001b[36m(_ray_fit pid=18930)\u001b[0m [5000]\tvalid_set's rmse: 1.12239\n","\u001b[36m(_ray_fit pid=18930)\u001b[0m [6000]\tvalid_set's rmse: 1.11788\n","\u001b[36m(_ray_fit pid=18930)\u001b[0m [7000]\tvalid_set's rmse: 1.11363\n","\u001b[36m(_ray_fit pid=18930)\u001b[0m [8000]\tvalid_set's rmse: 1.10962\n","\u001b[36m(_ray_fit pid=18930)\u001b[0m [9000]\tvalid_set's rmse: 1.10604\n","\u001b[36m(_ray_fit pid=18930)\u001b[0m [10000]\tvalid_set's rmse: 1.10264\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_dystack pid=15693)\u001b[0m \t-1.1017\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t470.98s\t = Training   runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t130.55s\t = Validation runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 3789.37s of the 6187.63s of remaining time.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 32.94% memory usage per fold, 65.87%/80.00% total).\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=32.94%)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t-1.1473\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t3034.24s\t = Training   runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t1.32s\t = Validation runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 753.28s of the 3151.53s of remaining time.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tWarning: Exception caused ExtraTreesMSE_BAG_L1 to fail during training... Skipping this model.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\tInput X contains infinity or a value too large for dtype('float32').\n","\u001b[36m(_dystack pid=15693)\u001b[0m Detailed Traceback:\n","\u001b[36m(_dystack pid=15693)\u001b[0m Traceback (most recent call last):\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","\u001b[36m(_dystack pid=15693)\u001b[0m     model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","\u001b[36m(_dystack pid=15693)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     out = self._fit(**kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 263, in _fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     self._fit_single(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 496, in _fit_single\n","\u001b[36m(_dystack pid=15693)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     out = self._fit(**kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/rf/rf_model.py\", line 195, in _fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     model = model.fit(X, y, sample_weight=sample_weight)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return fit_method(estimator, *args, **kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 377, in fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     estimator._compute_missing_values_in_feature_mask(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 214, in _compute_missing_values_in_feature_mask\n","\u001b[36m(_dystack pid=15693)\u001b[0m     assert_all_finite(X, **common_kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 204, in assert_all_finite\n","\u001b[36m(_dystack pid=15693)\u001b[0m     _assert_all_finite(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 126, in _assert_all_finite\n","\u001b[36m(_dystack pid=15693)\u001b[0m     _assert_all_finite_element_wise(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 175, in _assert_all_finite_element_wise\n","\u001b[36m(_dystack pid=15693)\u001b[0m     raise ValueError(msg_err)\n","\u001b[36m(_dystack pid=15693)\u001b[0m ValueError: Input X contains infinity or a value too large for dtype('float32').\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 752.41s of the 3150.66s of remaining time.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 40.14% memory usage per fold, 40.14%/80.00% total).\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=40.14%)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=30541, ip=127.0.0.1)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     out = self._fit(**kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 153, in _fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return func(**kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1081, in fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     train_dmatrix, evals = _wrap_evaluation_matrices(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 596, in _wrap_evaluation_matrices\n","\u001b[36m(_dystack pid=15693)\u001b[0m     train_dmatrix = create_dmatrix(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1003, in _create_dmatrix\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return QuantileDMatrix(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return func(**kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1573, in __init__\n","\u001b[36m(_dystack pid=15693)\u001b[0m     self._init(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1634, in _init\n","\u001b[36m(_dystack pid=15693)\u001b[0m     _check_call(ret)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 284, in _check_call\n","\u001b[36m(_dystack pid=15693)\u001b[0m     raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n","\u001b[36m(_dystack pid=15693)\u001b[0m xgboost.core.XGBoostError: [03:30:46] /Users/runner/work/xgboost/xgboost/src/data/../common/../data/gradient_index.h:94: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\n","\u001b[36m(_dystack pid=15693)\u001b[0m Stack trace:\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (0) 1   libxgboost.dylib                    0x0000000323f04428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (1) 2   libxgboost.dylib                    0x000000032409c4b8 void xgboost::GHistIndexMatrix::SetIndexData<xgboost::data::CSRArrayAdapterBatch, unsigned int, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&), xgboost::data::IsValidFunctor&>(xgboost::common::Span<xgboost::data::IsValidFunctor&, 18446744073709551615ul>, unsigned long, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, xgboost::data::IsValidFunctor&, unsigned long, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&)&&) + 320\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (2) 3   libxgboost.dylib                    0x000000032409b1c4 void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>) + 400\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (3) 4   libxgboost.dylib                    0x000000032409ac24 void xgboost::GHistIndexMatrix::PushAdapterBatch<xgboost::data::CSRArrayAdapterBatch>(xgboost::Context const*, unsigned long, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, float, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, double, unsigned long) + 300\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (4) 5   libxgboost.dylib                    0x00000003240969f0 xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 10364\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (5) 6   libxgboost.dylib                    0x0000000324093d88 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 840\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (6) 7   libxgboost.dylib                    0x00000003240502b0 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 140\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (7) 8   libxgboost.dylib                    0x0000000323f0d5fc XGQuantileDMatrixCreateFromCallback + 496\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (8) 9   libffi.dylib                        0x0000000194723050 ffi_call_SYSV + 80\n","\u001b[36m(_dystack pid=15693)\u001b[0m Detailed Traceback:\n","\u001b[36m(_dystack pid=15693)\u001b[0m Traceback (most recent call last):\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","\u001b[36m(_dystack pid=15693)\u001b[0m     model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","\u001b[36m(_dystack pid=15693)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     out = self._fit(**kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     self._fit_folds(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","\u001b[36m(_dystack pid=15693)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 666, in after_all_folds_scheduled\n","\u001b[36m(_dystack pid=15693)\u001b[0m     self._run_pseudo_sequential(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 644, in _run_pseudo_sequential\n","\u001b[36m(_dystack pid=15693)\u001b[0m     self._process_fold_results(finished[0], unfinished, fold_ctx)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","\u001b[36m(_dystack pid=15693)\u001b[0m     raise processed_exception\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","\u001b[36m(_dystack pid=15693)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return fn(*args, **kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return func(*args, **kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 2667, in get\n","\u001b[36m(_dystack pid=15693)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 864, in get_objects\n","\u001b[36m(_dystack pid=15693)\u001b[0m     raise value.as_instanceof_cause()\n","\u001b[36m(_dystack pid=15693)\u001b[0m ray.exceptions.RayTaskError(XGBoostError): \u001b[36mray::_ray_fit()\u001b[39m (pid=30541, ip=127.0.0.1)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     out = self._fit(**kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 153, in _fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return func(**kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1081, in fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     train_dmatrix, evals = _wrap_evaluation_matrices(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 596, in _wrap_evaluation_matrices\n","\u001b[36m(_dystack pid=15693)\u001b[0m     train_dmatrix = create_dmatrix(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1003, in _create_dmatrix\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return QuantileDMatrix(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return func(**kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1573, in __init__\n","\u001b[36m(_dystack pid=15693)\u001b[0m     self._init(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1634, in _init\n","\u001b[36m(_dystack pid=15693)\u001b[0m     _check_call(ret)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 284, in _check_call\n","\u001b[36m(_dystack pid=15693)\u001b[0m     raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n","\u001b[36m(_dystack pid=15693)\u001b[0m xgboost.core.XGBoostError: [03:30:46] /Users/runner/work/xgboost/xgboost/src/data/../common/../data/gradient_index.h:94: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\n","\u001b[36m(_dystack pid=15693)\u001b[0m Stack trace:\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (0) 1   libxgboost.dylib                    0x0000000323f04428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (1) 2   libxgboost.dylib                    0x000000032409c4b8 void xgboost::GHistIndexMatrix::SetIndexData<xgboost::data::CSRArrayAdapterBatch, unsigned int, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&), xgboost::data::IsValidFunctor&>(xgboost::common::Span<xgboost::data::IsValidFunctor&, 18446744073709551615ul>, unsigned long, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, xgboost::data::IsValidFunctor&, unsigned long, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&)&&) + 320\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (2) 3   libxgboost.dylib                    0x000000032409b1c4 void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>) + 400\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (3) 4   libxgboost.dylib                    0x000000032409ac24 void xgboost::GHistIndexMatrix::PushAdapterBatch<xgboost::data::CSRArrayAdapterBatch>(xgboost::Context const*, unsigned long, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, float, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, double, unsigned long) + 300\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (4) 5   libxgboost.dylib                    0x00000003240969f0 xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 10364\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (5) 6   libxgboost.dylib                    0x0000000324093d88 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 840\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (6) 7   libxgboost.dylib                    0x00000003240502b0 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 140\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (7) 8   libxgboost.dylib                    0x0000000323f0d5fc XGQuantileDMatrixCreateFromCallback + 496\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (8) 9   libffi.dylib                        0x0000000194723050 ffi_call_SYSV + 80\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 746.51s of the 3144.76s of remaining time.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 32.73% memory usage per fold, 65.46%/80.00% total).\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=32.73%)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_ray_fit pid=30571)\u001b[0m [1000]\tvalid_set's rmse: 1.13134\n","\u001b[36m(_ray_fit pid=30571)\u001b[0m [2000]\tvalid_set's rmse: 1.11664\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=30571)\u001b[0m [3000]\tvalid_set's rmse: 1.10464\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=30571)\u001b[0m [4000]\tvalid_set's rmse: 1.09477\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=30571)\u001b[0m [5000]\tvalid_set's rmse: 1.08602\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=30571)\u001b[0m [6000]\tvalid_set's rmse: 1.07837\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=31488)\u001b[0m [1000]\tvalid_set's rmse: 1.12986\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=31488)\u001b[0m [2000]\tvalid_set's rmse: 1.1151\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=31488)\u001b[0m [3000]\tvalid_set's rmse: 1.10282\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=31488)\u001b[0m [4000]\tvalid_set's rmse: 1.09258\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=31488)\u001b[0m [5000]\tvalid_set's rmse: 1.08418\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=31488)\u001b[0m [6000]\tvalid_set's rmse: 1.07681\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=32356)\u001b[0m [1000]\tvalid_set's rmse: 1.13229\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=32356)\u001b[0m [2000]\tvalid_set's rmse: 1.11772\n","\u001b[36m(_ray_fit pid=32356)\u001b[0m [3000]\tvalid_set's rmse: 1.10527\n","\u001b[36m(_ray_fit pid=32356)\u001b[0m [4000]\tvalid_set's rmse: 1.09566\n","\u001b[36m(_ray_fit pid=32356)\u001b[0m [5000]\tvalid_set's rmse: 1.08732\n","\u001b[36m(_ray_fit pid=32356)\u001b[0m [6000]\tvalid_set's rmse: 1.08011\n","\u001b[36m(_ray_fit pid=32356)\u001b[0m [7000]\tvalid_set's rmse: 1.07368\n","\u001b[36m(_ray_fit pid=32356)\u001b[0m [8000]\tvalid_set's rmse: 1.06804\n","\u001b[36m(_ray_fit pid=32356)\u001b[0m [9000]\tvalid_set's rmse: 1.06295\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_dystack pid=15693)\u001b[0m \t-1.0708\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t659.94s\t = Training   runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t131.9s\t = Validation runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 63.28s of the 2461.53s of remaining time.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 32.53% memory usage per fold, 65.07%/80.00% total).\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=32.53%)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t-1.1514\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t53.25s\t = Training   runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t0.64s\t = Validation runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 8.2s of the 2406.46s of remaining time.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 28.14% memory usage per fold, 56.29%/80.00% total).\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=28.14%)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t-1.157\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t10.02s\t = Training   runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t0.68s\t = Validation runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m Completed 1/2 k-fold bagging repeats ...\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 479.29s of the 2394.69s of remaining time.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tEnsemble Weights: {'LightGBMLarge_BAG_L1': 1.0}\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t-1.0708\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t0.42s\t = Training   runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t0.01s\t = Validation runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m Excluded models: ['RF', 'NN_TORCH', 'KNN', 'FASTAI'] (Specified by `excluded_model_types`)\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting 54 L2 models ...\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2394.25s of the 2394.19s of remaining time.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 33.80% memory usage per fold, 67.61%/80.00% total).\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=33.80%)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_ray_fit pid=33552)\u001b[0m [1000]\tvalid_set's rmse: 1.03489\n","\u001b[36m(_ray_fit pid=33553)\u001b[0m [1000]\tvalid_set's rmse: 1.03961\n","\u001b[36m(_ray_fit pid=33552)\u001b[0m [2000]\tvalid_set's rmse: 1.0332\n","\u001b[36m(_ray_fit pid=33553)\u001b[0m [2000]\tvalid_set's rmse: 1.03764\n","\u001b[36m(_ray_fit pid=33552)\u001b[0m [3000]\tvalid_set's rmse: 1.03228\n","\u001b[36m(_ray_fit pid=33553)\u001b[0m [3000]\tvalid_set's rmse: 1.03647\n","\u001b[36m(_ray_fit pid=33552)\u001b[0m [4000]\tvalid_set's rmse: 1.03159\n","\u001b[36m(_ray_fit pid=33553)\u001b[0m [4000]\tvalid_set's rmse: 1.03574\n","\u001b[36m(_ray_fit pid=33552)\u001b[0m [5000]\tvalid_set's rmse: 1.03104\n","\u001b[36m(_ray_fit pid=33553)\u001b[0m [5000]\tvalid_set's rmse: 1.03516\n","\u001b[36m(_ray_fit pid=33552)\u001b[0m [6000]\tvalid_set's rmse: 1.03057\n","\u001b[36m(_ray_fit pid=33553)\u001b[0m [6000]\tvalid_set's rmse: 1.03468\n","\u001b[36m(_ray_fit pid=33552)\u001b[0m [7000]\tvalid_set's rmse: 1.03004\n","\u001b[36m(_ray_fit pid=33553)\u001b[0m [7000]\tvalid_set's rmse: 1.03424\n","\u001b[36m(_ray_fit pid=33552)\u001b[0m [8000]\tvalid_set's rmse: 1.02976\n","\u001b[36m(_ray_fit pid=33553)\u001b[0m [8000]\tvalid_set's rmse: 1.03378\n","\u001b[36m(_ray_fit pid=33552)\u001b[0m [9000]\tvalid_set's rmse: 1.02942\n","\u001b[36m(_ray_fit pid=33553)\u001b[0m [9000]\tvalid_set's rmse: 1.03339\n","\u001b[36m(_ray_fit pid=33552)\u001b[0m [10000]\tvalid_set's rmse: 1.02914\n","\u001b[36m(_ray_fit pid=33553)\u001b[0m [10000]\tvalid_set's rmse: 1.03306\n","\u001b[36m(_ray_fit pid=34333)\u001b[0m [1000]\tvalid_set's rmse: 1.03612\n","\u001b[36m(_ray_fit pid=34369)\u001b[0m [1000]\tvalid_set's rmse: 1.03567\n","\u001b[36m(_ray_fit pid=34333)\u001b[0m [2000]\tvalid_set's rmse: 1.03451\n","\u001b[36m(_ray_fit pid=34369)\u001b[0m [2000]\tvalid_set's rmse: 1.03406\n","\u001b[36m(_ray_fit pid=34333)\u001b[0m [3000]\tvalid_set's rmse: 1.03345\n","\u001b[36m(_ray_fit pid=34369)\u001b[0m [3000]\tvalid_set's rmse: 1.03293\n","\u001b[36m(_ray_fit pid=34333)\u001b[0m [4000]\tvalid_set's rmse: 1.03283\n","\u001b[36m(_ray_fit pid=34369)\u001b[0m [4000]\tvalid_set's rmse: 1.0322\n","\u001b[36m(_ray_fit pid=34333)\u001b[0m [5000]\tvalid_set's rmse: 1.03217\n","\u001b[36m(_ray_fit pid=34369)\u001b[0m [5000]\tvalid_set's rmse: 1.03158\n","\u001b[36m(_ray_fit pid=34333)\u001b[0m [6000]\tvalid_set's rmse: 1.03181\n","\u001b[36m(_ray_fit pid=34369)\u001b[0m [6000]\tvalid_set's rmse: 1.0312\n","\u001b[36m(_ray_fit pid=34333)\u001b[0m [7000]\tvalid_set's rmse: 1.03138\n","\u001b[36m(_ray_fit pid=34369)\u001b[0m [7000]\tvalid_set's rmse: 1.03078\n","\u001b[36m(_ray_fit pid=34333)\u001b[0m [8000]\tvalid_set's rmse: 1.03102\n","\u001b[36m(_ray_fit pid=34369)\u001b[0m [8000]\tvalid_set's rmse: 1.03038\n","\u001b[36m(_ray_fit pid=34333)\u001b[0m [9000]\tvalid_set's rmse: 1.03073\n","\u001b[36m(_ray_fit pid=34369)\u001b[0m [9000]\tvalid_set's rmse: 1.03002\n","\u001b[36m(_ray_fit pid=34333)\u001b[0m [10000]\tvalid_set's rmse: 1.03033\n","\u001b[36m(_ray_fit pid=34369)\u001b[0m [10000]\tvalid_set's rmse: 1.0296\n","\u001b[36m(_ray_fit pid=35135)\u001b[0m [1000]\tvalid_set's rmse: 1.03574\n","\u001b[36m(_ray_fit pid=35135)\u001b[0m [2000]\tvalid_set's rmse: 1.03394\n","\u001b[36m(_ray_fit pid=35135)\u001b[0m [3000]\tvalid_set's rmse: 1.03289\n","\u001b[36m(_ray_fit pid=35135)\u001b[0m [4000]\tvalid_set's rmse: 1.03218\n","\u001b[36m(_ray_fit pid=35135)\u001b[0m [5000]\tvalid_set's rmse: 1.03167\n","\u001b[36m(_ray_fit pid=35135)\u001b[0m [6000]\tvalid_set's rmse: 1.03116\n","\u001b[36m(_ray_fit pid=35135)\u001b[0m [7000]\tvalid_set's rmse: 1.03068\n","\u001b[36m(_ray_fit pid=35135)\u001b[0m [8000]\tvalid_set's rmse: 1.03027\n","\u001b[36m(_ray_fit pid=35135)\u001b[0m [9000]\tvalid_set's rmse: 1.02991\n","\u001b[36m(_ray_fit pid=35135)\u001b[0m [10000]\tvalid_set's rmse: 1.02965\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_dystack pid=15693)\u001b[0m \t-1.0304\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t531.63s\t = Training   runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t143.18s\t = Validation runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 1843.25s of the 1843.2s of remaining time.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 34.42% memory usage per fold, 68.84%/80.00% total).\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=34.42%)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_ray_fit pid=35697)\u001b[0m [1000]\tvalid_set's rmse: 1.03302\n","\u001b[36m(_ray_fit pid=35698)\u001b[0m [1000]\tvalid_set's rmse: 1.03771\n","\u001b[36m(_ray_fit pid=35697)\u001b[0m [2000]\tvalid_set's rmse: 1.03148\n","\u001b[36m(_ray_fit pid=35698)\u001b[0m [2000]\tvalid_set's rmse: 1.03611\n","\u001b[36m(_ray_fit pid=35697)\u001b[0m [3000]\tvalid_set's rmse: 1.03087\n","\u001b[36m(_ray_fit pid=35698)\u001b[0m [3000]\tvalid_set's rmse: 1.03518\n","\u001b[36m(_ray_fit pid=35697)\u001b[0m [4000]\tvalid_set's rmse: 1.03019\n","\u001b[36m(_ray_fit pid=35698)\u001b[0m [4000]\tvalid_set's rmse: 1.03448\n","\u001b[36m(_ray_fit pid=35697)\u001b[0m [5000]\tvalid_set's rmse: 1.02969\n","\u001b[36m(_ray_fit pid=35698)\u001b[0m [5000]\tvalid_set's rmse: 1.03378\n","\u001b[36m(_ray_fit pid=35697)\u001b[0m [6000]\tvalid_set's rmse: 1.0294\n","\u001b[36m(_ray_fit pid=35698)\u001b[0m [6000]\tvalid_set's rmse: 1.03325\n","\u001b[36m(_ray_fit pid=35697)\u001b[0m [7000]\tvalid_set's rmse: 1.02911\n","\u001b[36m(_ray_fit pid=35698)\u001b[0m [7000]\tvalid_set's rmse: 1.03277\n","\u001b[36m(_ray_fit pid=35697)\u001b[0m [8000]\tvalid_set's rmse: 1.02878\n","\u001b[36m(_ray_fit pid=35698)\u001b[0m [8000]\tvalid_set's rmse: 1.03231\n","\u001b[36m(_ray_fit pid=35697)\u001b[0m [9000]\tvalid_set's rmse: 1.0285\n","\u001b[36m(_ray_fit pid=35698)\u001b[0m [9000]\tvalid_set's rmse: 1.03189\n","\u001b[36m(_ray_fit pid=35697)\u001b[0m [10000]\tvalid_set's rmse: 1.02812\n","\u001b[36m(_ray_fit pid=35698)\u001b[0m [10000]\tvalid_set's rmse: 1.03161\n","\u001b[36m(_ray_fit pid=36459)\u001b[0m [1000]\tvalid_set's rmse: 1.03469\n","\u001b[36m(_ray_fit pid=36474)\u001b[0m [1000]\tvalid_set's rmse: 1.03346\n","\u001b[36m(_ray_fit pid=36459)\u001b[0m [2000]\tvalid_set's rmse: 1.03302\n","\u001b[36m(_ray_fit pid=36474)\u001b[0m [2000]\tvalid_set's rmse: 1.03213\n","\u001b[36m(_ray_fit pid=36459)\u001b[0m [3000]\tvalid_set's rmse: 1.03212\n","\u001b[36m(_ray_fit pid=36474)\u001b[0m [3000]\tvalid_set's rmse: 1.03128\n","\u001b[36m(_ray_fit pid=36459)\u001b[0m [4000]\tvalid_set's rmse: 1.03164\n","\u001b[36m(_ray_fit pid=36474)\u001b[0m [4000]\tvalid_set's rmse: 1.03052\n","\u001b[36m(_ray_fit pid=36459)\u001b[0m [5000]\tvalid_set's rmse: 1.03115\n","\u001b[36m(_ray_fit pid=36474)\u001b[0m [5000]\tvalid_set's rmse: 1.02991\n","\u001b[36m(_ray_fit pid=36459)\u001b[0m [6000]\tvalid_set's rmse: 1.03063\n","\u001b[36m(_ray_fit pid=36474)\u001b[0m [6000]\tvalid_set's rmse: 1.02928\n","\u001b[36m(_ray_fit pid=36459)\u001b[0m [7000]\tvalid_set's rmse: 1.0302\n","\u001b[36m(_ray_fit pid=36474)\u001b[0m [7000]\tvalid_set's rmse: 1.02882\n","\u001b[36m(_ray_fit pid=36459)\u001b[0m [8000]\tvalid_set's rmse: 1.0296\n","\u001b[36m(_ray_fit pid=36474)\u001b[0m [8000]\tvalid_set's rmse: 1.02854\n","\u001b[36m(_ray_fit pid=36459)\u001b[0m [9000]\tvalid_set's rmse: 1.02927\n","\u001b[36m(_ray_fit pid=36474)\u001b[0m [9000]\tvalid_set's rmse: 1.02825\n","\u001b[36m(_ray_fit pid=36459)\u001b[0m [10000]\tvalid_set's rmse: 1.02885\n","\u001b[36m(_ray_fit pid=36474)\u001b[0m [10000]\tvalid_set's rmse: 1.02789\n","\u001b[36m(_ray_fit pid=37227)\u001b[0m [1000]\tvalid_set's rmse: 1.03387\n","\u001b[36m(_ray_fit pid=37227)\u001b[0m [2000]\tvalid_set's rmse: 1.0326\n","\u001b[36m(_ray_fit pid=37227)\u001b[0m [3000]\tvalid_set's rmse: 1.03179\n","\u001b[36m(_ray_fit pid=37227)\u001b[0m [4000]\tvalid_set's rmse: 1.03096\n","\u001b[36m(_ray_fit pid=37227)\u001b[0m [5000]\tvalid_set's rmse: 1.03036\n","\u001b[36m(_ray_fit pid=37227)\u001b[0m [6000]\tvalid_set's rmse: 1.02971\n","\u001b[36m(_ray_fit pid=37227)\u001b[0m [7000]\tvalid_set's rmse: 1.02924\n","\u001b[36m(_ray_fit pid=37227)\u001b[0m [8000]\tvalid_set's rmse: 1.0288\n","\u001b[36m(_ray_fit pid=37227)\u001b[0m [9000]\tvalid_set's rmse: 1.02828\n","\u001b[36m(_ray_fit pid=37227)\u001b[0m [10000]\tvalid_set's rmse: 1.02773\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_dystack pid=15693)\u001b[0m \t-1.0288\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t527.98s\t = Training   runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t169.18s\t = Validation runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 1296.02s of the 1295.96s of remaining time.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 36.34% memory usage per fold, 72.67%/80.00% total).\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=36.34%)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t-1.0373\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t1038.8s\t = Training   runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t0.82s\t = Validation runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 255.09s of the 255.03s of remaining time.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tWarning: Exception caused ExtraTreesMSE_BAG_L2 to fail during training... Skipping this model.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\tInput X contains infinity or a value too large for dtype('float32').\n","\u001b[36m(_dystack pid=15693)\u001b[0m Detailed Traceback:\n","\u001b[36m(_dystack pid=15693)\u001b[0m Traceback (most recent call last):\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","\u001b[36m(_dystack pid=15693)\u001b[0m     model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","\u001b[36m(_dystack pid=15693)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     out = self._fit(**kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 263, in _fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     self._fit_single(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 496, in _fit_single\n","\u001b[36m(_dystack pid=15693)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     out = self._fit(**kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/rf/rf_model.py\", line 195, in _fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     model = model.fit(X, y, sample_weight=sample_weight)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return fit_method(estimator, *args, **kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 377, in fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     estimator._compute_missing_values_in_feature_mask(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 214, in _compute_missing_values_in_feature_mask\n","\u001b[36m(_dystack pid=15693)\u001b[0m     assert_all_finite(X, **common_kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 204, in assert_all_finite\n","\u001b[36m(_dystack pid=15693)\u001b[0m     _assert_all_finite(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 126, in _assert_all_finite\n","\u001b[36m(_dystack pid=15693)\u001b[0m     _assert_all_finite_element_wise(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 175, in _assert_all_finite_element_wise\n","\u001b[36m(_dystack pid=15693)\u001b[0m     raise ValueError(msg_err)\n","\u001b[36m(_dystack pid=15693)\u001b[0m ValueError: Input X contains infinity or a value too large for dtype('float32').\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 254.14s of the 254.08s of remaining time.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 44.96% memory usage per fold, 44.96%/80.00% total).\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=44.96%)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=41577, ip=127.0.0.1)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     out = self._fit(**kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 153, in _fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return func(**kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1081, in fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     train_dmatrix, evals = _wrap_evaluation_matrices(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 596, in _wrap_evaluation_matrices\n","\u001b[36m(_dystack pid=15693)\u001b[0m     train_dmatrix = create_dmatrix(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1003, in _create_dmatrix\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return QuantileDMatrix(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return func(**kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1573, in __init__\n","\u001b[36m(_dystack pid=15693)\u001b[0m     self._init(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1634, in _init\n","\u001b[36m(_dystack pid=15693)\u001b[0m     _check_call(ret)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 284, in _check_call\n","\u001b[36m(_dystack pid=15693)\u001b[0m     raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n","\u001b[36m(_dystack pid=15693)\u001b[0m xgboost.core.XGBoostError: [04:19:03] /Users/runner/work/xgboost/xgboost/src/data/../common/../data/gradient_index.h:94: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\n","\u001b[36m(_dystack pid=15693)\u001b[0m Stack trace:\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (0) 1   libxgboost.dylib                    0x0000000329e8c428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (1) 2   libxgboost.dylib                    0x000000032a0244b8 void xgboost::GHistIndexMatrix::SetIndexData<xgboost::data::CSRArrayAdapterBatch, unsigned int, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&), xgboost::data::IsValidFunctor&>(xgboost::common::Span<xgboost::data::IsValidFunctor&, 18446744073709551615ul>, unsigned long, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, xgboost::data::IsValidFunctor&, unsigned long, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&)&&) + 320\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (2) 3   libxgboost.dylib                    0x000000032a0231c4 void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>) + 400\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (3) 4   libxgboost.dylib                    0x000000032a022c24 void xgboost::GHistIndexMatrix::PushAdapterBatch<xgboost::data::CSRArrayAdapterBatch>(xgboost::Context const*, unsigned long, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, float, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, double, unsigned long) + 300\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (4) 5   libxgboost.dylib                    0x000000032a01e9f0 xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 10364\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (5) 6   libxgboost.dylib                    0x000000032a01bd88 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 840\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (6) 7   libxgboost.dylib                    0x0000000329fd82b0 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 140\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (7) 8   libxgboost.dylib                    0x0000000329e955fc XGQuantileDMatrixCreateFromCallback + 496\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (8) 9   libffi.dylib                        0x0000000194723050 ffi_call_SYSV + 80\n","\u001b[36m(_dystack pid=15693)\u001b[0m Detailed Traceback:\n","\u001b[36m(_dystack pid=15693)\u001b[0m Traceback (most recent call last):\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","\u001b[36m(_dystack pid=15693)\u001b[0m     model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","\u001b[36m(_dystack pid=15693)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     out = self._fit(**kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     self._fit_folds(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","\u001b[36m(_dystack pid=15693)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 666, in after_all_folds_scheduled\n","\u001b[36m(_dystack pid=15693)\u001b[0m     self._run_pseudo_sequential(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 644, in _run_pseudo_sequential\n","\u001b[36m(_dystack pid=15693)\u001b[0m     self._process_fold_results(finished[0], unfinished, fold_ctx)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","\u001b[36m(_dystack pid=15693)\u001b[0m     raise processed_exception\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","\u001b[36m(_dystack pid=15693)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return fn(*args, **kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return func(*args, **kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 2667, in get\n","\u001b[36m(_dystack pid=15693)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 864, in get_objects\n","\u001b[36m(_dystack pid=15693)\u001b[0m     raise value.as_instanceof_cause()\n","\u001b[36m(_dystack pid=15693)\u001b[0m ray.exceptions.RayTaskError(XGBoostError): \u001b[36mray::_ray_fit()\u001b[39m (pid=41577, ip=127.0.0.1)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     out = self._fit(**kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 153, in _fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return func(**kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1081, in fit\n","\u001b[36m(_dystack pid=15693)\u001b[0m     train_dmatrix, evals = _wrap_evaluation_matrices(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 596, in _wrap_evaluation_matrices\n","\u001b[36m(_dystack pid=15693)\u001b[0m     train_dmatrix = create_dmatrix(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1003, in _create_dmatrix\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return QuantileDMatrix(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","\u001b[36m(_dystack pid=15693)\u001b[0m     return func(**kwargs)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1573, in __init__\n","\u001b[36m(_dystack pid=15693)\u001b[0m     self._init(\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1634, in _init\n","\u001b[36m(_dystack pid=15693)\u001b[0m     _check_call(ret)\n","\u001b[36m(_dystack pid=15693)\u001b[0m   File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 284, in _check_call\n","\u001b[36m(_dystack pid=15693)\u001b[0m     raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n","\u001b[36m(_dystack pid=15693)\u001b[0m xgboost.core.XGBoostError: [04:19:03] /Users/runner/work/xgboost/xgboost/src/data/../common/../data/gradient_index.h:94: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\n","\u001b[36m(_dystack pid=15693)\u001b[0m Stack trace:\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (0) 1   libxgboost.dylib                    0x0000000329e8c428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (1) 2   libxgboost.dylib                    0x000000032a0244b8 void xgboost::GHistIndexMatrix::SetIndexData<xgboost::data::CSRArrayAdapterBatch, unsigned int, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&), xgboost::data::IsValidFunctor&>(xgboost::common::Span<xgboost::data::IsValidFunctor&, 18446744073709551615ul>, unsigned long, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, xgboost::data::IsValidFunctor&, unsigned long, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&)&&) + 320\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (2) 3   libxgboost.dylib                    0x000000032a0231c4 void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>) + 400\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (3) 4   libxgboost.dylib                    0x000000032a022c24 void xgboost::GHistIndexMatrix::PushAdapterBatch<xgboost::data::CSRArrayAdapterBatch>(xgboost::Context const*, unsigned long, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, float, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, double, unsigned long) + 300\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (4) 5   libxgboost.dylib                    0x000000032a01e9f0 xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 10364\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (5) 6   libxgboost.dylib                    0x000000032a01bd88 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 840\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (6) 7   libxgboost.dylib                    0x0000000329fd82b0 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 140\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (7) 8   libxgboost.dylib                    0x0000000329e955fc XGQuantileDMatrixCreateFromCallback + 496\n","\u001b[36m(_dystack pid=15693)\u001b[0m   [bt] (8) 9   libffi.dylib                        0x0000000194723050 ffi_call_SYSV + 80\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 247.89s of the 247.83s of remaining time.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 33.52% memory usage per fold, 67.05%/80.00% total).\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=33.52%)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(_ray_fit pid=41613)\u001b[0m [1000]\tvalid_set's rmse: 1.03019\n","\u001b[36m(_ray_fit pid=41614)\u001b[0m [1000]\tvalid_set's rmse: 1.03498\n","\u001b[36m(_ray_fit pid=41884)\u001b[0m [1000]\tvalid_set's rmse: 1.03073\n","\u001b[36m(_ray_fit pid=41883)\u001b[0m [1000]\tvalid_set's rmse: 1.03174\n","\u001b[36m(_ray_fit pid=42176)\u001b[0m [1000]\tvalid_set's rmse: 1.03105\n","\u001b[36m(_ray_fit pid=42176)\u001b[0m [2000]\tvalid_set's rmse: 1.02792\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(_dystack pid=15693)\u001b[0m \t-1.0284\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t210.4s\t = Training   runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t18.26s\t = Validation runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 32.41s of the 32.35s of remaining time.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 35.51% memory usage per fold, 71.02%/80.00% total).\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=35.51%)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t-1.0441\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t28.34s\t = Training   runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t0.58s\t = Validation runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m Completed 1/2 k-fold bagging repeats ...\n","\u001b[36m(_dystack pid=15693)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 2.02s of remaining time.\n","\u001b[36m(_dystack pid=15693)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L2': 0.389, 'LightGBMLarge_BAG_L2': 0.389, 'LightGBMXT_BAG_L2': 0.222}\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t-1.0261\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t0.42s\t = Training   runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m \t0.0s\t = Validation runtime\n","\u001b[36m(_dystack pid=15693)\u001b[0m AutoGluon training complete, total runtime = 7195.97s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 244.3 rows/s (177778 batch size)\n","\u001b[36m(_dystack pid=15693)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"Autogluon/202412_ps4s12_8hr_training_original/ds_sub_fit/sub_fit_ho\")\n","\u001b[36m(_dystack pid=15693)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n","Leaderboard on holdout data (DyStack):\n","                   model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n","0        LightGBM_BAG_L2      -0.991882  -1.028838  root_mean_squared_error      158.467781     566.408844  5248.955537                40.923145              169.183501         527.978125            2       True          9\n","1    WeightedEnsemble_L3      -0.992928  -1.026099  root_mean_squared_error      207.197176     727.846809  5991.415169                 0.002173                0.004715           0.424352            3       True         13\n","2      LightGBMXT_BAG_L2      -0.993370  -1.030350  root_mean_squared_error      159.625607     540.400352  5252.611488                42.080971              143.175009         531.634076            2       True          8\n","3   LightGBMLarge_BAG_L2      -0.995961  -1.028408  root_mean_squared_error      124.190887     415.483584  4931.378616                 6.646252               18.258241         210.401204            2       True         11\n","4        CatBoost_BAG_L2      -1.003397  -1.037330  root_mean_squared_error      118.032068     398.048177  5759.778816                 0.487432                0.822834        1038.801405            2       True         10\n","5   CatBoost_r177_BAG_L2      -1.013192  -1.044066  root_mean_squared_error      117.890025     397.805748  4749.315039                 0.345389                0.580405          28.337627            2       True         12\n","6   LightGBMLarge_BAG_L1      -1.056651  -1.070787  root_mean_squared_error       29.833313     131.896348   659.939820                29.833313              131.896348         659.939820            1       True          4\n","7    WeightedEnsemble_L2      -1.056651  -1.070787  root_mean_squared_error       29.835101     131.902461   660.360905                 0.001788                0.006113           0.421085            2       True          7\n","8        LightGBM_BAG_L1      -1.089149  -1.101708  root_mean_squared_error       41.626938     130.549905   470.978185                41.626938              130.549905         470.978185            1       True          2\n","9      LightGBMXT_BAG_L1      -1.103866  -1.113238  root_mean_squared_error       44.429934     132.145648   492.551058                44.429934              132.145648         492.551058            1       True          1\n","10       CatBoost_BAG_L1      -1.142315  -1.147334  root_mean_squared_error        0.969079       1.315695  3034.238408                 0.969079                1.315695        3034.238408            1       True          3\n","11  CatBoost_r177_BAG_L1      -1.146961  -1.151392  root_mean_squared_error        0.369429       0.640510    53.250414                 0.369429                0.640510          53.250414            1       True          5\n","12  LightGBM_r131_BAG_L1      -1.153185  -1.156951  root_mean_squared_error        0.315943       0.677238    10.019526                 0.315943                0.677238          10.019526            1       True          6\n","\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n","\t7410s\t = DyStack   runtime |\t21390s\t = Remaining runtime\n","Starting main fit with num_stack_levels=1.\n","\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n","Beginning AutoGluon training ... Time limit = 21390s\n","AutoGluon will save models to \"Autogluon/202412_ps4s12_8hr_training_original\"\n","Train Data Rows:    1000000\n","Train Data Columns: 59\n","Label Column:       premium_amount_log\n","Problem Type:       regression\n","Preprocessing data ...\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    6524.75 MB\n","\tTrain Data (Original)  Memory Usage: 1221.17 MB (18.7% of available memory)\n","\tWarning: Data size prior to feature transformation consumes 18.7% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 8 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\t\tFitting CategoryFeatureGenerator...\n","\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n","\t\tFitting DatetimeFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tUseless Original Features (Count: 2): ['year_cos', 'is_senior_driver']\n","\t\tThese features carry no predictive signal and should be manually investigated.\n","\t\tThis is typically a feature which has the same value for all rows.\n","\t\tThese features do not need to be present at inference time.\n","\tUnused Original Features (Count: 1): ['claims_per_year']\n","\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n","\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n","\t\tThese features do not need to be present at inference time.\n","\t\t('float', []) : 1 | ['claims_per_year']\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('category', [])                   :  1 | ['income_bracket']\n","\t\t('datetime', [])                   :  1 | ['policy_start']\n","\t\t('float', [])                      : 24 | ['age', 'annual_income', 'number_of_dependents', 'health_score', 'previous_claims', ...]\n","\t\t('int', [])                        : 15 | ['year', 'month', 'day', 'week_of_year', 'quarter', ...]\n","\t\t('object', [])                     : 14 | ['gender', 'marital_status', 'education_level', 'occupation', 'location', ...]\n","\t\t('object', ['datetime_as_object']) :  1 | ['policy_start_date']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('category', [])             : 13 | ['marital_status', 'education_level', 'occupation', 'location', 'policy_type', ...]\n","\t\t('float', [])                : 24 | ['age', 'annual_income', 'number_of_dependents', 'health_score', 'previous_claims', ...]\n","\t\t('int', [])                  :  9 | ['year', 'month', 'day', 'week_of_year', 'quarter', ...]\n","\t\t('int', ['bool'])            :  8 | ['gender', 'smoking_status', 'is_weekend', 'is_month_end', 'is_month_start', ...]\n","\t\t('int', ['datetime_as_int']) :  2 | ['policy_start_date', 'policy_start_date.dayofweek']\n","\t5.6s = Fit runtime\n","\t56 features in original data used to generate 56 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 264.17 MB (4.2% of available memory)\n","Data preprocessing and feature engineering runtime = 6.28s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n","\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n","\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","AutoGluon will fit 2 stack levels (L1 to L2) ...\n","Excluded models: ['NN_TORCH', 'FASTAI', 'KNN', 'RF'] (Specified by `excluded_model_types`)\n","Fitting 54 L1 models ...\n","Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 14251.98s of the 21383.31s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 34.13% memory usage per fold, 68.26%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=34.13%)\n","\t-1.1109\t = Validation score   (-root_mean_squared_error)\n","\t570.92s\t = Training   runtime\n","\t150.39s\t = Validation runtime\n","Fitting model: LightGBM_BAG_L1 ... Training model for up to 13657.5s of the 20788.83s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 31.75% memory usage per fold, 63.50%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=31.75%)\n","\t-1.0984\t = Validation score   (-root_mean_squared_error)\n","\t524.44s\t = Training   runtime\n","\t189.36s\t = Validation runtime\n","Fitting model: CatBoost_BAG_L1 ... Training model for up to 13111.62s of the 20242.95s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 34.84% memory usage per fold, 69.68%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=34.84%)\n","\t-1.1405\t = Validation score   (-root_mean_squared_error)\n","\t10496.25s\t = Training   runtime\n","\t3.76s\t = Validation runtime\n","Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 2612.9s of the 9744.23s of remaining time.\n","\tWarning: Exception caused ExtraTreesMSE_BAG_L1 to fail during training... Skipping this model.\n","\t\tInput X contains infinity or a value too large for dtype('float32').\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 263, in _fit\n","    self._fit_single(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 496, in _fit_single\n","    model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/rf/rf_model.py\", line 195, in _fit\n","    model = model.fit(X, y, sample_weight=sample_weight)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n","    return fit_method(estimator, *args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 377, in fit\n","    estimator._compute_missing_values_in_feature_mask(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 214, in _compute_missing_values_in_feature_mask\n","    assert_all_finite(X, **common_kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 204, in assert_all_finite\n","    _assert_all_finite(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 126, in _assert_all_finite\n","    _assert_all_finite_element_wise(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 175, in _assert_all_finite_element_wise\n","    raise ValueError(msg_err)\n","ValueError: Input X contains infinity or a value too large for dtype('float32').\n","Fitting model: XGBoost_BAG_L1 ... Training model for up to 2611.92s of the 9743.25s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 40.24% memory usage per fold, 40.24%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=40.24%)\n","\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n","\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=85696, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 153, in _fit\n","    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1081, in fit\n","    train_dmatrix, evals = _wrap_evaluation_matrices(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 596, in _wrap_evaluation_matrices\n","    train_dmatrix = create_dmatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1003, in _create_dmatrix\n","    return QuantileDMatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1573, in __init__\n","    self._init(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1634, in _init\n","    _check_call(ret)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 284, in _check_call\n","    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n","xgboost.core.XGBoostError: [07:40:51] /Users/runner/work/xgboost/xgboost/src/data/../common/../data/gradient_index.h:94: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\n","Stack trace:\n","  [bt] (0) 1   libxgboost.dylib                    0x00000003222e0428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n","  [bt] (1) 2   libxgboost.dylib                    0x00000003224784b8 void xgboost::GHistIndexMatrix::SetIndexData<xgboost::data::CSRArrayAdapterBatch, unsigned int, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&), xgboost::data::IsValidFunctor&>(xgboost::common::Span<xgboost::data::IsValidFunctor&, 18446744073709551615ul>, unsigned long, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, xgboost::data::IsValidFunctor&, unsigned long, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&)&&) + 320\n","  [bt] (2) 3   libxgboost.dylib                    0x00000003224771c4 void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>) + 400\n","  [bt] (3) 4   libxgboost.dylib                    0x0000000322476c24 void xgboost::GHistIndexMatrix::PushAdapterBatch<xgboost::data::CSRArrayAdapterBatch>(xgboost::Context const*, unsigned long, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, float, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, double, unsigned long) + 300\n","  [bt] (4) 5   libxgboost.dylib                    0x00000003224729f0 xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 10364\n","  [bt] (5) 6   libxgboost.dylib                    0x000000032246fd88 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 840\n","  [bt] (6) 7   libxgboost.dylib                    0x000000032242c2b0 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 140\n","  [bt] (7) 8   libxgboost.dylib                    0x00000003222e95fc XGQuantileDMatrixCreateFromCallback + 496\n","  [bt] (8) 9   libffi.dylib                        0x0000000194723050 ffi_call_SYSV + 80\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","    self._fit_folds(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","    fold_fitting_strategy.after_all_folds_scheduled()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 666, in after_all_folds_scheduled\n","    self._run_pseudo_sequential(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 644, in _run_pseudo_sequential\n","    self._process_fold_results(finished[0], unfinished, fold_ctx)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","    raise processed_exception\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 2667, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 864, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(XGBoostError): \u001b[36mray::_ray_fit()\u001b[39m (pid=85696, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 153, in _fit\n","    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1081, in fit\n","    train_dmatrix, evals = _wrap_evaluation_matrices(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 596, in _wrap_evaluation_matrices\n","    train_dmatrix = create_dmatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1003, in _create_dmatrix\n","    return QuantileDMatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1573, in __init__\n","    self._init(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1634, in _init\n","    _check_call(ret)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 284, in _check_call\n","    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n","xgboost.core.XGBoostError: [07:40:51] /Users/runner/work/xgboost/xgboost/src/data/../common/../data/gradient_index.h:94: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\n","Stack trace:\n","  [bt] (0) 1   libxgboost.dylib                    0x00000003222e0428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n","  [bt] (1) 2   libxgboost.dylib                    0x00000003224784b8 void xgboost::GHistIndexMatrix::SetIndexData<xgboost::data::CSRArrayAdapterBatch, unsigned int, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&), xgboost::data::IsValidFunctor&>(xgboost::common::Span<xgboost::data::IsValidFunctor&, 18446744073709551615ul>, unsigned long, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, xgboost::data::IsValidFunctor&, unsigned long, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&)&&) + 320\n","  [bt] (2) 3   libxgboost.dylib                    0x00000003224771c4 void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>) + 400\n","  [bt] (3) 4   libxgboost.dylib                    0x0000000322476c24 void xgboost::GHistIndexMatrix::PushAdapterBatch<xgboost::data::CSRArrayAdapterBatch>(xgboost::Context const*, unsigned long, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, float, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, double, unsigned long) + 300\n","  [bt] (4) 5   libxgboost.dylib                    0x00000003224729f0 xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 10364\n","  [bt] (5) 6   libxgboost.dylib                    0x000000032246fd88 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 840\n","  [bt] (6) 7   libxgboost.dylib                    0x000000032242c2b0 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 140\n","  [bt] (7) 8   libxgboost.dylib                    0x00000003222e95fc XGQuantileDMatrixCreateFromCallback + 496\n","  [bt] (8) 9   libffi.dylib                        0x0000000194723050 ffi_call_SYSV + 80\n","Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2605.48s of the 9736.81s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 31.48% memory usage per fold, 62.95%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=31.48%)\n","\t-1.0503\t = Validation score   (-root_mean_squared_error)\n","\t953.42s\t = Training   runtime\n","\t258.22s\t = Validation runtime\n","Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 1625.38s of the 8756.72s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 32.19% memory usage per fold, 64.38%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=32.19%)\n","\t-1.1388\t = Validation score   (-root_mean_squared_error)\n","\t1304.03s\t = Training   runtime\n","\t1.93s\t = Validation runtime\n","Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 319.27s of the 7450.6s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 26.76% memory usage per fold, 53.52%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=26.76%)\n","\t-1.1356\t = Validation score   (-root_mean_squared_error)\n","\t286.39s\t = Training   runtime\n","\t61.59s\t = Validation runtime\n","Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 22.47s of the 7153.81s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 33.37% memory usage per fold, 66.74%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=33.37%)\n","\t-1.1568\t = Validation score   (-root_mean_squared_error)\n","\t19.59s\t = Training   runtime\n","\t0.75s\t = Validation runtime\n","Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 1.27s of the 7132.6s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 28.93% memory usage per fold, 57.86%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=28.93%)\n","\t-1.1641\t = Validation score   (-root_mean_squared_error)\n","\t5.92s\t = Training   runtime\n","\t0.2s\t = Validation runtime\n","Completed 1/2 k-fold bagging repeats ...\n","Fitting model: WeightedEnsemble_L2 ... Training model for up to 1425.2s of the 7125.07s of remaining time.\n","\tEnsemble Weights: {'LightGBMLarge_BAG_L1': 1.0}\n","\t-1.0503\t = Validation score   (-root_mean_squared_error)\n","\t0.46s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Excluded models: ['NN_TORCH', 'FASTAI', 'KNN', 'RF'] (Specified by `excluded_model_types`)\n","Fitting 54 L2 models ...\n","Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 7124.59s of the 7124.51s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 34.97% memory usage per fold, 69.94%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=34.97%)\n","\t-1.0066\t = Validation score   (-root_mean_squared_error)\n","\t621.69s\t = Training   runtime\n","\t199.62s\t = Validation runtime\n","Fitting model: LightGBM_BAG_L2 ... Training model for up to 6481.06s of the 6480.98s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 37.28% memory usage per fold, 74.56%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=37.28%)\n","\t-1.0047\t = Validation score   (-root_mean_squared_error)\n","\t609.96s\t = Training   runtime\n","\t200.31s\t = Validation runtime\n","Fitting model: CatBoost_BAG_L2 ... Training model for up to 5849.21s of the 5849.13s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 37.49% memory usage per fold, 74.97%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=37.49%)\n","\t-1.0109\t = Validation score   (-root_mean_squared_error)\n","\t4682.56s\t = Training   runtime\n","\t2.08s\t = Validation runtime\n","Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 1164.45s of the 1164.36s of remaining time.\n","\tWarning: Exception caused ExtraTreesMSE_BAG_L2 to fail during training... Skipping this model.\n","\t\tInput X contains infinity or a value too large for dtype('float32').\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 263, in _fit\n","    self._fit_single(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 496, in _fit_single\n","    model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/rf/rf_model.py\", line 195, in _fit\n","    model = model.fit(X, y, sample_weight=sample_weight)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n","    return fit_method(estimator, *args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 377, in fit\n","    estimator._compute_missing_values_in_feature_mask(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 214, in _compute_missing_values_in_feature_mask\n","    assert_all_finite(X, **common_kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 204, in assert_all_finite\n","    _assert_all_finite(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 126, in _assert_all_finite\n","    _assert_all_finite_element_wise(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 175, in _assert_all_finite_element_wise\n","    raise ValueError(msg_err)\n","ValueError: Input X contains infinity or a value too large for dtype('float32').\n","Fitting model: XGBoost_BAG_L2 ... Training model for up to 1163.28s of the 1163.2s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 40.68% memory usage per fold, 40.68%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=40.68%)\n","\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n","\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=11454, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 153, in _fit\n","    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1081, in fit\n","    train_dmatrix, evals = _wrap_evaluation_matrices(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 596, in _wrap_evaluation_matrices\n","    train_dmatrix = create_dmatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1003, in _create_dmatrix\n","    return QuantileDMatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1573, in __init__\n","    self._init(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1634, in _init\n","    _check_call(ret)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 284, in _check_call\n","    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n","xgboost.core.XGBoostError: [10:03:51] /Users/runner/work/xgboost/xgboost/src/data/../common/../data/gradient_index.h:94: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\n","Stack trace:\n","  [bt] (0) 1   libxgboost.dylib                    0x000000032cf8c428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n","  [bt] (1) 2   libxgboost.dylib                    0x000000032d1244b8 void xgboost::GHistIndexMatrix::SetIndexData<xgboost::data::CSRArrayAdapterBatch, unsigned int, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&), xgboost::data::IsValidFunctor&>(xgboost::common::Span<xgboost::data::IsValidFunctor&, 18446744073709551615ul>, unsigned long, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, xgboost::data::IsValidFunctor&, unsigned long, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&)&&) + 320\n","  [bt] (2) 3   libxgboost.dylib                    0x000000032d1231c4 void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>) + 400\n","  [bt] (3) 4   libxgboost.dylib                    0x000000032d122c24 void xgboost::GHistIndexMatrix::PushAdapterBatch<xgboost::data::CSRArrayAdapterBatch>(xgboost::Context const*, unsigned long, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, float, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, double, unsigned long) + 300\n","  [bt] (4) 5   libxgboost.dylib                    0x000000032d11e9f0 xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 10364\n","  [bt] (5) 6   libxgboost.dylib                    0x000000032d11bd88 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 840\n","  [bt] (6) 7   libxgboost.dylib                    0x000000032d0d82b0 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 140\n","  [bt] (7) 8   libxgboost.dylib                    0x000000032cf955fc XGQuantileDMatrixCreateFromCallback + 496\n","  [bt] (8) 9   libffi.dylib                        0x0000000194723050 ffi_call_SYSV + 80\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","    self._fit_folds(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","    fold_fitting_strategy.after_all_folds_scheduled()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 666, in after_all_folds_scheduled\n","    self._run_pseudo_sequential(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 644, in _run_pseudo_sequential\n","    self._process_fold_results(finished[0], unfinished, fold_ctx)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","    raise processed_exception\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 2667, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 864, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(XGBoostError): \u001b[36mray::_ray_fit()\u001b[39m (pid=11454, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 153, in _fit\n","    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1081, in fit\n","    train_dmatrix, evals = _wrap_evaluation_matrices(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 596, in _wrap_evaluation_matrices\n","    train_dmatrix = create_dmatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1003, in _create_dmatrix\n","    return QuantileDMatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1573, in __init__\n","    self._init(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1634, in _init\n","    _check_call(ret)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 284, in _check_call\n","    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n","xgboost.core.XGBoostError: [10:03:51] /Users/runner/work/xgboost/xgboost/src/data/../common/../data/gradient_index.h:94: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\n","Stack trace:\n","  [bt] (0) 1   libxgboost.dylib                    0x000000032cf8c428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n","  [bt] (1) 2   libxgboost.dylib                    0x000000032d1244b8 void xgboost::GHistIndexMatrix::SetIndexData<xgboost::data::CSRArrayAdapterBatch, unsigned int, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&), xgboost::data::IsValidFunctor&>(xgboost::common::Span<xgboost::data::IsValidFunctor&, 18446744073709551615ul>, unsigned long, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, xgboost::data::IsValidFunctor&, unsigned long, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&)&&) + 320\n","  [bt] (2) 3   libxgboost.dylib                    0x000000032d1231c4 void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>) + 400\n","  [bt] (3) 4   libxgboost.dylib                    0x000000032d122c24 void xgboost::GHistIndexMatrix::PushAdapterBatch<xgboost::data::CSRArrayAdapterBatch>(xgboost::Context const*, unsigned long, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, float, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, double, unsigned long) + 300\n","  [bt] (4) 5   libxgboost.dylib                    0x000000032d11e9f0 xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 10364\n","  [bt] (5) 6   libxgboost.dylib                    0x000000032d11bd88 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 840\n","  [bt] (6) 7   libxgboost.dylib                    0x000000032d0d82b0 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 140\n","  [bt] (7) 8   libxgboost.dylib                    0x000000032cf955fc XGQuantileDMatrixCreateFromCallback + 496\n","  [bt] (8) 9   libffi.dylib                        0x0000000194723050 ffi_call_SYSV + 80\n","Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 1156.2s of the 1156.12s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 32.16% memory usage per fold, 64.31%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=32.16%)\n","\t-0.9955\t = Validation score   (-root_mean_squared_error)\n","\t1009.34s\t = Training   runtime\n","\t234.08s\t = Validation runtime\n","Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 120.1s of the 120.02s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 36.36% memory usage per fold, 72.72%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=36.36%)\n","\tWarning: Exception caused CatBoost_r177_BAG_L2 to fail during training... Skipping this model.\n","\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=12287, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 106, in _fit\n","    try_import_catboost()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/common/utils/try_import.py\", line 75, in try_import_catboost\n","    assert catboost_version >= parse_version(\n","AssertionError: Currently, we support \"catboost>=1.2\". Installed version: \"catboost==1.1.1\".\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    get_models_func = self.construct_model_templates\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    self.models[model.name] = model\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    ----------\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    elif num_rows < 45000:\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","    raise AssertionError(f\"n_repeat_start and k_fold_start must be 0 with refit_folds=True, values: ({n_repeat_start}, {k_fold_start})\")\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","    20,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n","    self._update_bagged_ensemble_times()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n","    model_base_ref=model_base_ref,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","    self.terminate_all_unfinished_tasks(unfinished)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 2667, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 864, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=12287, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 106, in _fit\n","    try_import_catboost()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/common/utils/try_import.py\", line 75, in try_import_catboost\n","    assert catboost_version >= parse_version(\n","AssertionError: Currently, we support \"catboost>=1.2\". Installed version: \"catboost==1.1.1\".\n","Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 116.9s of the 116.82s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 34.96% memory usage per fold, 69.92%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=34.96%)\n","2024-12-30 10:21:16,283\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=12286, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 106, in _fit\n","    try_import_catboost()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/common/utils/try_import.py\", line 75, in try_import_catboost\n","    assert catboost_version >= parse_version(\n","AssertionError: Currently, we support \"catboost>=1.2\". Installed version: \"catboost==1.1.1\".\n","\tWarning: Exception caused LightGBM_r131_BAG_L2 to fail during training... Skipping this model.\n","\t\ttoo many values to unpack (expected 7)\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    get_models_func = self.construct_model_templates\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    self.models[model.name] = model\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    ----------\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    elif num_rows < 45000:\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","    raise AssertionError(f\"n_repeat_start and k_fold_start must be 0 with refit_folds=True, values: ({n_repeat_start}, {k_fold_start})\")\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","    20,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n","    self._update_bagged_ensemble_times()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n","    model_base_ref=model_base_ref,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","    self.terminate_all_unfinished_tasks(unfinished)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","ValueError: too many values to unpack (expected 7)\n","Fitting model: CatBoost_r9_BAG_L2 ... Training model for up to 80.38s of the 80.3s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 34.46% memory usage per fold, 68.91%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=34.46%)\n","\tWarning: Exception caused CatBoost_r9_BAG_L2 to fail during training... Skipping this model.\n","\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=12299, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 106, in _fit\n","    try_import_catboost()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/common/utils/try_import.py\", line 75, in try_import_catboost\n","    assert catboost_version >= parse_version(\n","AssertionError: Currently, we support \"catboost>=1.2\". Installed version: \"catboost==1.1.1\".\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    get_models_func = self.construct_model_templates\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    self.models[model.name] = model\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    ----------\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    elif num_rows < 45000:\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","    raise AssertionError(f\"n_repeat_start and k_fold_start must be 0 with refit_folds=True, values: ({n_repeat_start}, {k_fold_start})\")\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","    20,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n","    self._update_bagged_ensemble_times()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n","    model_base_ref=model_base_ref,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","    self.terminate_all_unfinished_tasks(unfinished)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 2667, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 864, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=12299, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 106, in _fit\n","    try_import_catboost()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/common/utils/try_import.py\", line 75, in try_import_catboost\n","    assert catboost_version >= parse_version(\n","AssertionError: Currently, we support \"catboost>=1.2\". Installed version: \"catboost==1.1.1\".\n","2024-12-30 10:21:50,126\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n","Fitting model: LightGBM_r96_BAG_L2 ... Training model for up to 77.94s of the 77.86s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 35.91% memory usage per fold, 71.82%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=35.91%)\n","2024-12-30 10:21:55,327\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n","\tWarning: Exception caused LightGBM_r96_BAG_L2 to fail during training... Skipping this model.\n","\t\ttoo many values to unpack (expected 7)\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    get_models_func = self.construct_model_templates\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    self.models[model.name] = model\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    ----------\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    elif num_rows < 45000:\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","    raise AssertionError(f\"n_repeat_start and k_fold_start must be 0 with refit_folds=True, values: ({n_repeat_start}, {k_fold_start})\")\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","    20,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n","    self._update_bagged_ensemble_times()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n","    model_base_ref=model_base_ref,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","    self.terminate_all_unfinished_tasks(unfinished)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","ValueError: too many values to unpack (expected 7)\n","Fitting model: XGBoost_r33_BAG_L2 ... Training model for up to 53.51s of the 53.43s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 46.00% memory usage per fold, 46.00%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=46.00%)\n","\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","2024-12-30 10:22:20,355\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n","\tWarning: Exception caused XGBoost_r33_BAG_L2 to fail during training... Skipping this model.\n","\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=12312, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 194, in _fit\n","    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1081, in fit\n","    train_dmatrix, evals = _wrap_evaluation_matrices(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 596, in _wrap_evaluation_matrices\n","    train_dmatrix = create_dmatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1003, in _create_dmatrix\n","    return QuantileDMatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1573, in __init__\n","    self._init(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1634, in _init\n","    _check_call(ret)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 284, in _check_call\n","    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n","xgboost.core.XGBoostError: [10:22:20] /Users/runner/work/xgboost/xgboost/src/data/../common/../data/gradient_index.h:94: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\n","Stack trace:\n","  [bt] (0) 1   libxgboost.dylib                    0x000000016e2bc428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n","  [bt] (1) 2   libxgboost.dylib                    0x000000016e4544b8 void xgboost::GHistIndexMatrix::SetIndexData<xgboost::data::CSRArrayAdapterBatch, unsigned int, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&), xgboost::data::IsValidFunctor&>(xgboost::common::Span<xgboost::data::IsValidFunctor&, 18446744073709551615ul>, unsigned long, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, xgboost::data::IsValidFunctor&, unsigned long, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&)&&) + 320\n","  [bt] (2) 3   libxgboost.dylib                    0x000000016e4531c4 void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>) + 400\n","  [bt] (3) 4   libxgboost.dylib                    0x000000016e452c24 void xgboost::GHistIndexMatrix::PushAdapterBatch<xgboost::data::CSRArrayAdapterBatch>(xgboost::Context const*, unsigned long, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, float, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, double, unsigned long) + 300\n","  [bt] (4) 5   libxgboost.dylib                    0x000000016e44e9f0 xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 10364\n","  [bt] (5) 6   libxgboost.dylib                    0x000000016e44bd88 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 840\n","  [bt] (6) 7   libxgboost.dylib                    0x000000016e4082b0 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 140\n","  [bt] (7) 8   libxgboost.dylib                    0x000000016e2c55fc XGQuantileDMatrixCreateFromCallback + 496\n","  [bt] (8) 9   libffi.dylib                        0x0000000194723050 ffi_call_SYSV + 80\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    get_models_func = self.construct_model_templates\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    self.models[model.name] = model\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    ----------\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    elif num_rows < 45000:\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","    raise AssertionError(f\"n_repeat_start and k_fold_start must be 0 with refit_folds=True, values: ({n_repeat_start}, {k_fold_start})\")\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","    20,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 666, in after_all_folds_scheduled\n","    self._process_fold_results(finished[0], unfinished, fold_ctx)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 644, in _run_pseudo_sequential\n","    overhead. Here, at most, we have the overhead of one fold. In the case of `_run_parallel` the asynchronous\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","    self.terminate_all_unfinished_tasks(unfinished)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 2667, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 864, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(XGBoostError): \u001b[36mray::_ray_fit()\u001b[39m (pid=12312, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 194, in _fit\n","    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1081, in fit\n","    train_dmatrix, evals = _wrap_evaluation_matrices(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 596, in _wrap_evaluation_matrices\n","    train_dmatrix = create_dmatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1003, in _create_dmatrix\n","    return QuantileDMatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1573, in __init__\n","    self._init(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1634, in _init\n","    _check_call(ret)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 284, in _check_call\n","    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n","xgboost.core.XGBoostError: [10:22:20] /Users/runner/work/xgboost/xgboost/src/data/../common/../data/gradient_index.h:94: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\n","Stack trace:\n","  [bt] (0) 1   libxgboost.dylib                    0x000000016e2bc428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n","  [bt] (1) 2   libxgboost.dylib                    0x000000016e4544b8 void xgboost::GHistIndexMatrix::SetIndexData<xgboost::data::CSRArrayAdapterBatch, unsigned int, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&), xgboost::data::IsValidFunctor&>(xgboost::common::Span<xgboost::data::IsValidFunctor&, 18446744073709551615ul>, unsigned long, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, xgboost::data::IsValidFunctor&, unsigned long, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&)&&) + 320\n","  [bt] (2) 3   libxgboost.dylib                    0x000000016e4531c4 void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>) + 400\n","  [bt] (3) 4   libxgboost.dylib                    0x000000016e452c24 void xgboost::GHistIndexMatrix::PushAdapterBatch<xgboost::data::CSRArrayAdapterBatch>(xgboost::Context const*, unsigned long, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, float, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, double, unsigned long) + 300\n","  [bt] (4) 5   libxgboost.dylib                    0x000000016e44e9f0 xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 10364\n","  [bt] (5) 6   libxgboost.dylib                    0x000000016e44bd88 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 840\n","  [bt] (6) 7   libxgboost.dylib                    0x000000016e4082b0 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 140\n","  [bt] (7) 8   libxgboost.dylib                    0x000000016e2c55fc XGQuantileDMatrixCreateFromCallback + 496\n","  [bt] (8) 9   libffi.dylib                        0x0000000194723050 ffi_call_SYSV + 80\n","Fitting model: ExtraTrees_r42_BAG_L2 ... Training model for up to 47.04s of the 46.96s of remaining time.\n","\tWarning: Exception caused ExtraTrees_r42_BAG_L2 to fail during training... Skipping this model.\n","\t\tInput X contains infinity or a value too large for dtype('float32').\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    get_models_func = self.construct_model_templates\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    self.models[model.name] = model\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    ----------\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    elif num_rows < 45000:\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 263, in _fit\n","    #  Therefore we must override save_bag_folds for these unsupported models so that the refit versions have a fold model to copy.\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 496, in _fit_single\n","    #  Consider pseudolabelling to respect the original distribution\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    ----------\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/rf/rf_model.py\", line 195, in _fit\n","    params[\"warm_start\"] = True\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n","    class _UnstableArchMixin:\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 377, in fit\n","    estimator._compute_missing_values_in_feature_mask(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 214, in _compute_missing_values_in_feature_mask\n","    assert_all_finite(X, **common_kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 204, in assert_all_finite\n","    >>> import numpy as np\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 126, in _assert_all_finite\n","    allow_nan=allow_nan,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 175, in _assert_all_finite_element_wise\n","    def assert_all_finite(\n","ValueError: Input X contains infinity or a value too large for dtype('float32').\n","Fitting model: CatBoost_r137_BAG_L2 ... Training model for up to 46.0s of the 45.93s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 34.67% memory usage per fold, 69.35%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=34.67%)\n","\tWarning: Exception caused CatBoost_r137_BAG_L2 to fail during training... Skipping this model.\n","\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=12315, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 106, in _fit\n","    try_import_catboost()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/common/utils/try_import.py\", line 75, in try_import_catboost\n","    assert catboost_version >= parse_version(\n","AssertionError: Currently, we support \"catboost>=1.2\". Installed version: \"catboost==1.1.1\".\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    get_models_func = self.construct_model_templates\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    self.models[model.name] = model\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    ----------\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    elif num_rows < 45000:\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","    raise AssertionError(f\"n_repeat_start and k_fold_start must be 0 with refit_folds=True, values: ({n_repeat_start}, {k_fold_start})\")\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","    20,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n","    self._update_bagged_ensemble_times()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n","    model_base_ref=model_base_ref,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","    self.terminate_all_unfinished_tasks(unfinished)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 2667, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 864, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=12315, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 106, in _fit\n","    try_import_catboost()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/common/utils/try_import.py\", line 75, in try_import_catboost\n","    assert catboost_version >= parse_version(\n","AssertionError: Currently, we support \"catboost>=1.2\". Installed version: \"catboost==1.1.1\".\n","Fitting model: CatBoost_r13_BAG_L2 ... Training model for up to 43.56s of the 43.48s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 37.69% memory usage per fold, 75.39%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=37.69%)\n","\tWarning: Exception caused CatBoost_r13_BAG_L2 to fail during training... Skipping this model.\n","\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=12319, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 106, in _fit\n","    try_import_catboost()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/common/utils/try_import.py\", line 75, in try_import_catboost\n","    assert catboost_version >= parse_version(\n","AssertionError: Currently, we support \"catboost>=1.2\". Installed version: \"catboost==1.1.1\".\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    get_models_func = self.construct_model_templates\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    self.models[model.name] = model\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    ----------\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    elif num_rows < 45000:\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","    raise AssertionError(f\"n_repeat_start and k_fold_start must be 0 with refit_folds=True, values: ({n_repeat_start}, {k_fold_start})\")\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","    20,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n","    self._update_bagged_ensemble_times()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n","    model_base_ref=model_base_ref,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","    self.terminate_all_unfinished_tasks(unfinished)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 2667, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 864, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=12319, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 106, in _fit\n","    try_import_catboost()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/common/utils/try_import.py\", line 75, in try_import_catboost\n","    assert catboost_version >= parse_version(\n","AssertionError: Currently, we support \"catboost>=1.2\". Installed version: \"catboost==1.1.1\".\n","2024-12-30 10:22:26,629\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=12316, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 106, in _fit\n","    try_import_catboost()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/common/utils/try_import.py\", line 75, in try_import_catboost\n","    assert catboost_version >= parse_version(\n","AssertionError: Currently, we support \"catboost>=1.2\". Installed version: \"catboost==1.1.1\".\n","Fitting model: LightGBM_r188_BAG_L2 ... Training model for up to 41.43s of the 41.35s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 35.76% memory usage per fold, 71.52%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=35.76%)\n","2024-12-30 10:22:32,370\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n","2024-12-30 10:22:32,371\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n","\tWarning: Exception caused LightGBM_r188_BAG_L2 to fail during training... Skipping this model.\n","\t\ttoo many values to unpack (expected 7)\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    get_models_func = self.construct_model_templates\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    self.models[model.name] = model\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    ----------\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    elif num_rows < 45000:\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","    raise AssertionError(f\"n_repeat_start and k_fold_start must be 0 with refit_folds=True, values: ({n_repeat_start}, {k_fold_start})\")\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","    20,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n","    self._update_bagged_ensemble_times()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n","    model_base_ref=model_base_ref,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","    self.terminate_all_unfinished_tasks(unfinished)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","ValueError: too many values to unpack (expected 7)\n","Fitting model: XGBoost_r89_BAG_L2 ... Training model for up to 27.43s of the 27.35s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 43.38% memory usage per fold, 43.38%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=43.38%)\n","\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","2024-12-30 10:22:46,390\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n","\tWarning: Exception caused XGBoost_r89_BAG_L2 to fail during training... Skipping this model.\n","\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=12329, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 194, in _fit\n","    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1081, in fit\n","    train_dmatrix, evals = _wrap_evaluation_matrices(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 596, in _wrap_evaluation_matrices\n","    train_dmatrix = create_dmatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1003, in _create_dmatrix\n","    return QuantileDMatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1573, in __init__\n","    self._init(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1634, in _init\n","    _check_call(ret)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 284, in _check_call\n","    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n","xgboost.core.XGBoostError: [10:22:46] /Users/runner/work/xgboost/xgboost/src/data/../common/../data/gradient_index.h:94: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\n","Stack trace:\n","  [bt] (0) 1   libxgboost.dylib                    0x0000000322918428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n","  [bt] (1) 2   libxgboost.dylib                    0x0000000322ab04b8 void xgboost::GHistIndexMatrix::SetIndexData<xgboost::data::CSRArrayAdapterBatch, unsigned int, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&), xgboost::data::IsValidFunctor&>(xgboost::common::Span<xgboost::data::IsValidFunctor&, 18446744073709551615ul>, unsigned long, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, xgboost::data::IsValidFunctor&, unsigned long, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&)&&) + 320\n","  [bt] (2) 3   libxgboost.dylib                    0x0000000322aaf1c4 void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>) + 400\n","  [bt] (3) 4   libxgboost.dylib                    0x0000000322aaec24 void xgboost::GHistIndexMatrix::PushAdapterBatch<xgboost::data::CSRArrayAdapterBatch>(xgboost::Context const*, unsigned long, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, float, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, double, unsigned long) + 300\n","  [bt] (4) 5   libxgboost.dylib                    0x0000000322aaa9f0 xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 10364\n","  [bt] (5) 6   libxgboost.dylib                    0x0000000322aa7d88 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 840\n","  [bt] (6) 7   libxgboost.dylib                    0x0000000322a642b0 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 140\n","  [bt] (7) 8   libxgboost.dylib                    0x00000003229215fc XGQuantileDMatrixCreateFromCallback + 496\n","  [bt] (8) 9   libffi.dylib                        0x0000000194723050 ffi_call_SYSV + 80\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    get_models_func = self.construct_model_templates\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    self.models[model.name] = model\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    ----------\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    elif num_rows < 45000:\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","    raise AssertionError(f\"n_repeat_start and k_fold_start must be 0 with refit_folds=True, values: ({n_repeat_start}, {k_fold_start})\")\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","    20,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 666, in after_all_folds_scheduled\n","    self._process_fold_results(finished[0], unfinished, fold_ctx)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 644, in _run_pseudo_sequential\n","    overhead. Here, at most, we have the overhead of one fold. In the case of `_run_parallel` the asynchronous\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","    self.terminate_all_unfinished_tasks(unfinished)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 2667, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 864, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(XGBoostError): \u001b[36mray::_ray_fit()\u001b[39m (pid=12329, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 194, in _fit\n","    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1081, in fit\n","    train_dmatrix, evals = _wrap_evaluation_matrices(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 596, in _wrap_evaluation_matrices\n","    train_dmatrix = create_dmatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1003, in _create_dmatrix\n","    return QuantileDMatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1573, in __init__\n","    self._init(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1634, in _init\n","    _check_call(ret)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 284, in _check_call\n","    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n","xgboost.core.XGBoostError: [10:22:46] /Users/runner/work/xgboost/xgboost/src/data/../common/../data/gradient_index.h:94: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\n","Stack trace:\n","  [bt] (0) 1   libxgboost.dylib                    0x0000000322918428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n","  [bt] (1) 2   libxgboost.dylib                    0x0000000322ab04b8 void xgboost::GHistIndexMatrix::SetIndexData<xgboost::data::CSRArrayAdapterBatch, unsigned int, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&), xgboost::data::IsValidFunctor&>(xgboost::common::Span<xgboost::data::IsValidFunctor&, 18446744073709551615ul>, unsigned long, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, xgboost::data::IsValidFunctor&, unsigned long, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&)&&) + 320\n","  [bt] (2) 3   libxgboost.dylib                    0x0000000322aaf1c4 void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::CSRArrayAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::CSRArrayAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>) + 400\n","  [bt] (3) 4   libxgboost.dylib                    0x0000000322aaec24 void xgboost::GHistIndexMatrix::PushAdapterBatch<xgboost::data::CSRArrayAdapterBatch>(xgboost::Context const*, unsigned long, unsigned long, xgboost::data::CSRArrayAdapterBatch const&, float, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, double, unsigned long) + 300\n","  [bt] (4) 5   libxgboost.dylib                    0x0000000322aaa9f0 xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 10364\n","  [bt] (5) 6   libxgboost.dylib                    0x0000000322aa7d88 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 840\n","  [bt] (6) 7   libxgboost.dylib                    0x0000000322a642b0 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 140\n","  [bt] (7) 8   libxgboost.dylib                    0x00000003229215fc XGQuantileDMatrixCreateFromCallback + 496\n","  [bt] (8) 9   libffi.dylib                        0x0000000194723050 ffi_call_SYSV + 80\n","Fitting model: LightGBM_r130_BAG_L2 ... Training model for up to 21.14s of the 21.07s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 35.81% memory usage per fold, 71.62%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=35.81%)\n","\tWarning: Exception caused LightGBM_r130_BAG_L2 to fail during training... Skipping this model.\n","\t\ttoo many values to unpack (expected 7)\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    get_models_func = self.construct_model_templates\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    self.models[model.name] = model\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    ----------\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    elif num_rows < 45000:\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","    raise AssertionError(f\"n_repeat_start and k_fold_start must be 0 with refit_folds=True, values: ({n_repeat_start}, {k_fold_start})\")\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","    20,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n","    self._update_bagged_ensemble_times()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n","    model_base_ref=model_base_ref,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","    self.terminate_all_unfinished_tasks(unfinished)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","ValueError: too many values to unpack (expected 7)\n","Fitting model: CatBoost_r50_BAG_L2 ... Training model for up to 12.96s of the 12.87s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 32.55% memory usage per fold, 65.10%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=32.55%)\n","\tWarning: Exception caused CatBoost_r50_BAG_L2 to fail during training... Skipping this model.\n","\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=12336, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 106, in _fit\n","    try_import_catboost()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/common/utils/try_import.py\", line 75, in try_import_catboost\n","    assert catboost_version >= parse_version(\n","AssertionError: Currently, we support \"catboost>=1.2\". Installed version: \"catboost==1.1.1\".\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    get_models_func = self.construct_model_templates\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    self.models[model.name] = model\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    ----------\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    elif num_rows < 45000:\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","    raise AssertionError(f\"n_repeat_start and k_fold_start must be 0 with refit_folds=True, values: ({n_repeat_start}, {k_fold_start})\")\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","    20,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n","    self._update_bagged_ensemble_times()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n","    model_base_ref=model_base_ref,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","    self.terminate_all_unfinished_tasks(unfinished)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 2667, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 864, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=12336, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 106, in _fit\n","    try_import_catboost()\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/common/utils/try_import.py\", line 75, in try_import_catboost\n","    assert catboost_version >= parse_version(\n","AssertionError: Currently, we support \"catboost>=1.2\". Installed version: \"catboost==1.1.1\".\n","2024-12-30 10:22:57,335\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n","Fitting model: XGBoost_r194_BAG_L2 ... Training model for up to 10.73s of the 10.65s of remaining time.\n","\tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 44.00% memory usage per fold, 44.00%/80.00% total).\n","\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=10, gpus=0, memory=44.00%)\n","\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n","\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n","\tWarning: Exception caused XGBoost_r194_BAG_L2 to fail during training... Skipping this model.\n","\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=12341, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 194, in _fit\n","    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1081, in fit\n","    train_dmatrix, evals = _wrap_evaluation_matrices(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 596, in _wrap_evaluation_matrices\n","    train_dmatrix = create_dmatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1003, in _create_dmatrix\n","    return QuantileDMatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1573, in __init__\n","    self._init(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1634, in _init\n","    _check_call(ret)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 284, in _check_call\n","    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n","xgboost.core.XGBoostError: [10:23:00] /Users/runner/work/xgboost/xgboost/src/data/../common/../data/gradient_index.h:94: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\n","Stack trace:\n","  [bt] (0) 1   libxgboost.dylib                    0x000000032d138428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n","  [bt] (1) 2   libxgboost.dylib                    0x000000032d2db7b4 void xgboost::GHistIndexMatrix::SetIndexData<xgboost::data::ColumnarAdapterBatch, unsigned int, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::ColumnarAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&), xgboost::data::IsValidFunctor&>(xgboost::common::Span<xgboost::data::IsValidFunctor&, 18446744073709551615ul>, unsigned long, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, unsigned long, xgboost::data::ColumnarAdapterBatch const&, xgboost::data::IsValidFunctor&, unsigned long, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::ColumnarAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&)&&) + 328\n","  [bt] (2) 3   libxgboost.dylib                    0x000000032d2da514 void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::ColumnarAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>) + 404\n","  [bt] (3) 4   libxgboost.dylib                    0x000000032d2d9f60 void xgboost::GHistIndexMatrix::PushAdapterBatch<xgboost::data::ColumnarAdapterBatch>(xgboost::Context const*, unsigned long, unsigned long, xgboost::data::ColumnarAdapterBatch const&, float, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, double, unsigned long) + 308\n","  [bt] (4) 5   libxgboost.dylib                    0x000000032d2cae4c xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 11480\n","  [bt] (5) 6   libxgboost.dylib                    0x000000032d2c7d88 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 840\n","  [bt] (6) 7   libxgboost.dylib                    0x000000032d2842b0 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 140\n","  [bt] (7) 8   libxgboost.dylib                    0x000000032d1415fc XGQuantileDMatrixCreateFromCallback + 496\n","  [bt] (8) 9   libffi.dylib                        0x0000000194723050 ffi_call_SYSV + 80\n","Detailed Traceback:\n","Traceback (most recent call last):\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n","    get_models_func = self.construct_model_templates\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n","    self.models[model.name] = model\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n","    ----------\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n","    elif num_rows < 45000:\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n","    raise AssertionError(f\"n_repeat_start and k_fold_start must be 0 with refit_folds=True, values: ({n_repeat_start}, {k_fold_start})\")\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n","    20,\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 666, in after_all_folds_scheduled\n","    self._process_fold_results(finished[0], unfinished, fold_ctx)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 644, in _run_pseudo_sequential\n","    overhead. Here, at most, we have the overhead of one fold. In the case of `_run_parallel` the asynchronous\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n","    self.terminate_all_unfinished_tasks(unfinished)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 2667, in get\n","    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py\", line 864, in get_objects\n","    raise value.as_instanceof_cause()\n","ray.exceptions.RayTaskError(XGBoostError): \u001b[36mray::_ray_fit()\u001b[39m (pid=12341, ip=127.0.0.1)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n","    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n","    out = self._fit(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 194, in _fit\n","    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1081, in fit\n","    train_dmatrix, evals = _wrap_evaluation_matrices(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 596, in _wrap_evaluation_matrices\n","    train_dmatrix = create_dmatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1003, in _create_dmatrix\n","    return QuantileDMatrix(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1573, in __init__\n","    self._init(\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 1634, in _init\n","    _check_call(ret)\n","  File \"/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py\", line 284, in _check_call\n","    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n","xgboost.core.XGBoostError: [10:23:00] /Users/runner/work/xgboost/xgboost/src/data/../common/../data/gradient_index.h:94: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\n","Stack trace:\n","  [bt] (0) 1   libxgboost.dylib                    0x000000032d138428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n","  [bt] (1) 2   libxgboost.dylib                    0x000000032d2db7b4 void xgboost::GHistIndexMatrix::SetIndexData<xgboost::data::ColumnarAdapterBatch, unsigned int, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::ColumnarAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&), xgboost::data::IsValidFunctor&>(xgboost::common::Span<xgboost::data::IsValidFunctor&, 18446744073709551615ul>, unsigned long, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, unsigned long, xgboost::data::ColumnarAdapterBatch const&, xgboost::data::IsValidFunctor&, unsigned long, void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::ColumnarAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>)::'lambda'(xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&)&&) + 328\n","  [bt] (2) 3   libxgboost.dylib                    0x000000032d2da514 void xgboost::GHistIndexMatrix::PushBatchImpl<xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&>(int, xgboost::data::ColumnarAdapterBatch const&, unsigned long, xgboost::data::IsValidFunctor&, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>) + 404\n","  [bt] (3) 4   libxgboost.dylib                    0x000000032d2d9f60 void xgboost::GHistIndexMatrix::PushAdapterBatch<xgboost::data::ColumnarAdapterBatch>(xgboost::Context const*, unsigned long, unsigned long, xgboost::data::ColumnarAdapterBatch const&, float, xgboost::common::Span<xgboost::FeatureType const, 18446744073709551615ul>, double, unsigned long) + 308\n","  [bt] (4) 5   libxgboost.dylib                    0x000000032d2cae4c xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 11480\n","  [bt] (5) 6   libxgboost.dylib                    0x000000032d2c7d88 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 840\n","  [bt] (6) 7   libxgboost.dylib                    0x000000032d2842b0 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 140\n","  [bt] (7) 8   libxgboost.dylib                    0x000000032d1415fc XGQuantileDMatrixCreateFromCallback + 496\n","  [bt] (8) 9   libffi.dylib                        0x0000000194723050 ffi_call_SYSV + 80\n","2024-12-30 10:23:00,299\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n","Completed 1/2 k-fold bagging repeats ...\n","Fitting model: WeightedEnsemble_L3 ... Training model for up to 712.46s of the 7.4s of remaining time.\n","\tEnsemble Weights: {'LightGBMLarge_BAG_L2': 0.889, 'LightGBMXT_BAG_L2': 0.056, 'LightGBM_BAG_L2': 0.056}\n","\t-0.9954\t = Validation score   (-root_mean_squared_error)\n","\t0.48s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","AutoGluon training complete, total runtime = 21382.8s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 153.8 rows/s (200000 batch size)\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"Autogluon/202412_ps4s12_8hr_training_original\")\n"]},{"data":{"text/plain":["<autogluon.tabular.predictor.predictor.TabularPredictor at 0x2d5ab7370>"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# Setting up\n","label = 'premium_amount_log'\n","problem_type='regression'\n","excluded_model_types = ['KNN', 'RF', 'NN_TORCH', 'FASTAI']\n","hours = 8\n","\n","# Initialize the TabularPredictor\n","predictor = TabularPredictor(label=label,\n","                             problem_type=problem_type,\n","                             eval_metric=\"rmse\",\n","                             path = \"Autogluon/202412_ps4s12_8hr_training_original\")\n","\n","# Fit the model\n","predictor.fit(train_data=train_transformed.drop(columns=['premium_amount']),\n","              time_limit=3600*hours,\n","              presets=\"best_quality\",\n","              excluded_model_types=excluded_model_types,\n","              num_bag_folds=5,\n","              num_bag_sets=2\n",")"]},{"cell_type":"markdown","metadata":{"id":"gJ4L9SP17wQa"},"source":["# **Best Submission**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AELB7AvC7xk4"},"outputs":[],"source":["predictor = TabularPredictor.load(os.path.join(base_path, \"Autogluon/202412_ps4s12_8hr_training_original\"))"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"PsoZUUls72uC"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>score_val</th>\n","      <th>eval_metric</th>\n","      <th>pred_time_val</th>\n","      <th>fit_time</th>\n","      <th>pred_time_val_marginal</th>\n","      <th>fit_time_marginal</th>\n","      <th>stack_level</th>\n","      <th>can_infer</th>\n","      <th>fit_order</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>WeightedEnsemble_L3</td>\n","      <td>-0.995392</td>\n","      <td>root_mean_squared_error</td>\n","      <td>1300.210797</td>\n","      <td>16402.413986</td>\n","      <td>0.006064</td>\n","      <td>0.476131</td>\n","      <td>3</td>\n","      <td>True</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LightGBMLarge_BAG_L2</td>\n","      <td>-0.995494</td>\n","      <td>root_mean_squared_error</td>\n","      <td>900.282029</td>\n","      <td>15170.290432</td>\n","      <td>234.083690</td>\n","      <td>1009.336476</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>LightGBM_BAG_L2</td>\n","      <td>-1.004729</td>\n","      <td>root_mean_squared_error</td>\n","      <td>866.504131</td>\n","      <td>14770.910212</td>\n","      <td>200.305791</td>\n","      <td>609.956256</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>LightGBMXT_BAG_L2</td>\n","      <td>-1.006648</td>\n","      <td>root_mean_squared_error</td>\n","      <td>865.815251</td>\n","      <td>14782.645123</td>\n","      <td>199.616912</td>\n","      <td>621.691167</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>CatBoost_BAG_L2</td>\n","      <td>-1.010893</td>\n","      <td>root_mean_squared_error</td>\n","      <td>668.280543</td>\n","      <td>18843.517187</td>\n","      <td>2.082204</td>\n","      <td>4682.563231</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>LightGBMLarge_BAG_L1</td>\n","      <td>-1.050315</td>\n","      <td>root_mean_squared_error</td>\n","      <td>258.215411</td>\n","      <td>953.421759</td>\n","      <td>258.215411</td>\n","      <td>953.421759</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>WeightedEnsemble_L2</td>\n","      <td>-1.050315</td>\n","      <td>root_mean_squared_error</td>\n","      <td>258.220562</td>\n","      <td>953.879233</td>\n","      <td>0.005151</td>\n","      <td>0.457474</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>LightGBM_BAG_L1</td>\n","      <td>-1.098387</td>\n","      <td>root_mean_squared_error</td>\n","      <td>189.355563</td>\n","      <td>524.436608</td>\n","      <td>189.355563</td>\n","      <td>524.436608</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>LightGBMXT_BAG_L1</td>\n","      <td>-1.110897</td>\n","      <td>root_mean_squared_error</td>\n","      <td>150.392537</td>\n","      <td>570.918260</td>\n","      <td>150.392537</td>\n","      <td>570.918260</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>LightGBM_r131_BAG_L1</td>\n","      <td>-1.135642</td>\n","      <td>root_mean_squared_error</td>\n","      <td>61.594825</td>\n","      <td>286.393722</td>\n","      <td>61.594825</td>\n","      <td>286.393722</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>CatBoost_r177_BAG_L1</td>\n","      <td>-1.138830</td>\n","      <td>root_mean_squared_error</td>\n","      <td>1.928087</td>\n","      <td>1304.027241</td>\n","      <td>1.928087</td>\n","      <td>1304.027241</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>CatBoost_BAG_L1</td>\n","      <td>-1.140515</td>\n","      <td>root_mean_squared_error</td>\n","      <td>3.760793</td>\n","      <td>10496.247094</td>\n","      <td>3.760793</td>\n","      <td>10496.247094</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>CatBoost_r9_BAG_L1</td>\n","      <td>-1.156763</td>\n","      <td>root_mean_squared_error</td>\n","      <td>0.748355</td>\n","      <td>19.591328</td>\n","      <td>0.748355</td>\n","      <td>19.591328</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>LightGBM_r96_BAG_L1</td>\n","      <td>-1.164101</td>\n","      <td>root_mean_squared_error</td>\n","      <td>0.202768</td>\n","      <td>5.917944</td>\n","      <td>0.202768</td>\n","      <td>5.917944</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   model  score_val              eval_metric  pred_time_val  \\\n","0    WeightedEnsemble_L3  -0.995392  root_mean_squared_error    1300.210797   \n","1   LightGBMLarge_BAG_L2  -0.995494  root_mean_squared_error     900.282029   \n","2        LightGBM_BAG_L2  -1.004729  root_mean_squared_error     866.504131   \n","3      LightGBMXT_BAG_L2  -1.006648  root_mean_squared_error     865.815251   \n","4        CatBoost_BAG_L2  -1.010893  root_mean_squared_error     668.280543   \n","5   LightGBMLarge_BAG_L1  -1.050315  root_mean_squared_error     258.215411   \n","6    WeightedEnsemble_L2  -1.050315  root_mean_squared_error     258.220562   \n","7        LightGBM_BAG_L1  -1.098387  root_mean_squared_error     189.355563   \n","8      LightGBMXT_BAG_L1  -1.110897  root_mean_squared_error     150.392537   \n","9   LightGBM_r131_BAG_L1  -1.135642  root_mean_squared_error      61.594825   \n","10  CatBoost_r177_BAG_L1  -1.138830  root_mean_squared_error       1.928087   \n","11       CatBoost_BAG_L1  -1.140515  root_mean_squared_error       3.760793   \n","12    CatBoost_r9_BAG_L1  -1.156763  root_mean_squared_error       0.748355   \n","13   LightGBM_r96_BAG_L1  -1.164101  root_mean_squared_error       0.202768   \n","\n","        fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  \\\n","0   16402.413986                0.006064           0.476131            3   \n","1   15170.290432              234.083690        1009.336476            2   \n","2   14770.910212              200.305791         609.956256            2   \n","3   14782.645123              199.616912         621.691167            2   \n","4   18843.517187                2.082204        4682.563231            2   \n","5     953.421759              258.215411         953.421759            1   \n","6     953.879233                0.005151           0.457474            2   \n","7     524.436608              189.355563         524.436608            1   \n","8     570.918260              150.392537         570.918260            1   \n","9     286.393722               61.594825         286.393722            1   \n","10   1304.027241                1.928087        1304.027241            1   \n","11  10496.247094                3.760793       10496.247094            1   \n","12     19.591328                0.748355          19.591328            1   \n","13      5.917944                0.202768           5.917944            1   \n","\n","    can_infer  fit_order  \n","0        True         14  \n","1        True         13  \n","2        True         11  \n","3        True         10  \n","4        True         12  \n","5        True          4  \n","6        True          9  \n","7        True          2  \n","8        True          1  \n","9        True          6  \n","10       True          5  \n","11       True          3  \n","12       True          7  \n","13       True          8  "]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["leaderboard_test = predictor.leaderboard(silent=True)\n","leaderboard_test"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Best model by autogluon is WeightedEnsemble_L3\n","With a score of RMSLE 0.9953924636329955\n"]}],"source":["models = leaderboard_test.head(5)['model'].to_list()\n","best_model = models[0]\n","print(\"Best model by autogluon is\", models[0])\n","print(\"With a score of RMSLE\", np.abs(leaderboard_test[leaderboard_test['model']==best_model]['score_val'][0]))\n","\n","training = \"8hr_log_original_local\"\n","sub_autogluon = submission.copy()\n","sub_autogluon['premium_amount_log'] =  predictor.predict(test_transformed, as_pandas=False, model=best_model)\n","sub_autogluon['Premium Amount'] = np.expm1(sub_autogluon['premium_amount_log'])\n","sub_autogluon.drop(columns=['premium_amount_log'], inplace=True)\n","sub_autogluon.to_csv(os.path.join(base_path, f\"submissions/submission_{best_model}_{training}.csv\"), index=False)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Premium Amount</th>\n","      <th>premium_amount</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1200000</td>\n","      <td>1102.545</td>\n","      <td>802.825012</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1200001</td>\n","      <td>1102.545</td>\n","      <td>835.359253</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1200002</td>\n","      <td>1102.545</td>\n","      <td>784.259827</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1200003</td>\n","      <td>1102.545</td>\n","      <td>752.573425</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1200004</td>\n","      <td>1102.545</td>\n","      <td>606.276184</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>799995</th>\n","      <td>1999995</td>\n","      <td>1102.545</td>\n","      <td>670.903809</td>\n","    </tr>\n","    <tr>\n","      <th>799996</th>\n","      <td>1999996</td>\n","      <td>1102.545</td>\n","      <td>489.644684</td>\n","    </tr>\n","    <tr>\n","      <th>799997</th>\n","      <td>1999997</td>\n","      <td>1102.545</td>\n","      <td>746.510132</td>\n","    </tr>\n","    <tr>\n","      <th>799998</th>\n","      <td>1999998</td>\n","      <td>1102.545</td>\n","      <td>694.593201</td>\n","    </tr>\n","    <tr>\n","      <th>799999</th>\n","      <td>1999999</td>\n","      <td>1102.545</td>\n","      <td>731.632629</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>800000 rows × 3 columns</p>\n","</div>"],"text/plain":["             id  Premium Amount  premium_amount\n","0       1200000        1102.545      802.825012\n","1       1200001        1102.545      835.359253\n","2       1200002        1102.545      784.259827\n","3       1200003        1102.545      752.573425\n","4       1200004        1102.545      606.276184\n","...         ...             ...             ...\n","799995  1999995        1102.545      670.903809\n","799996  1999996        1102.545      489.644684\n","799997  1999997        1102.545      746.510132\n","799998  1999998        1102.545      694.593201\n","799999  1999999        1102.545      731.632629\n","\n","[800000 rows x 3 columns]"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["sub_autogluon"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["sub_autogluon.drop(columns=\"Premium Amount\").rename(columns={'premium_amount': 'Premium Amount'}).to_csv(os.path.join(base_path, f\"submissions/submission_{best_model}_{training}.csv\"), index=False)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Generating OOF predictions for WeightedEnsemble_L3 - 0/14\n","Generating OOF predictions for LightGBMLarge_BAG_L2 - 1/14\n","Generating OOF predictions for LightGBM_BAG_L2 - 2/14\n","Generating OOF predictions for LightGBMXT_BAG_L2 - 3/14\n","Generating OOF predictions for CatBoost_BAG_L2 - 4/14\n","Generating OOF predictions for LightGBMLarge_BAG_L1 - 5/14\n","Generating OOF predictions for WeightedEnsemble_L2 - 6/14\n","Generating OOF predictions for LightGBM_BAG_L1 - 7/14\n","Generating OOF predictions for LightGBMXT_BAG_L1 - 8/14\n","Generating OOF predictions for LightGBM_r131_BAG_L1 - 9/14\n","Generating OOF predictions for CatBoost_r177_BAG_L1 - 10/14\n","Generating OOF predictions for CatBoost_BAG_L1 - 11/14\n","Generating OOF predictions for CatBoost_r9_BAG_L1 - 12/14\n","Generating OOF predictions for LightGBM_r96_BAG_L1 - 13/14\n","Saved 14 model predictions for experiment 8log_original\n"]}],"source":["def save_experiment_oofs(predictor, models, experiment_name, path, islog=True):\n","    \"\"\"\n","    Save OOF predictions as a single DataFrame with experiment identifier in column names\n","    \"\"\"\n","    # Create DataFrame with index from training data\n","    oof_df = pd.DataFrame(index=predictor.predict_oof().index)\n","\n","    # Add OOF predictions for each model with experiment identifier\n","    for i, model in enumerate(models):\n","        print(f\"Generating OOF predictions for {model} - {i}/{len(models)}\")\n","        oof_preds = predictor.predict_oof(model=model)\n","        if islog:\n","            oof_preds = np.power(10, oof_preds)\n","        col_name = f\"{experiment_name}_{model}\"\n","        oof_df[col_name] = oof_preds\n","\n","    # Save DataFrame\n","    filename = f\"oof_preds_{experiment_name}.csv\"\n","    filepath = os.path.join(path, filename)\n","\n","    oof_df.to_csv(filepath)\n","    print(f\"Saved {len(models)} model predictions for experiment {experiment_name}\")\n","    return oof_df\n","\n","# Saving OOFs for later use\n","oofs_path = \"Data/oofs/\"\n","models = leaderboard_test['model'].to_list()\n","experiment_name = \"8log_original\"\n","oof_df = save_experiment_oofs(predictor, models, experiment_name, oofs_path, islog=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyNVq/bC8GPgxpsBIy3shjYY","gpuType":"V28","machine_shape":"hm","name":"","provenance":[{"file_id":"1EitC7l0z_iX7HTBqvZJyYgN8NpiIH7HH","timestamp":1735442226403}],"toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
