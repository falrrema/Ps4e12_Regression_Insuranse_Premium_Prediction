{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Gradient Boosting Experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Set-up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q perpetual rgf_python starboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "from sklearn.metrics import root_mean_squared_log_error, root_mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import ML\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "from xgboost import XGBRegressor\n",
    "from perpetual import PerpetualBooster\n",
    "from lightgbm import LGBMRegressor, early_stopping\n",
    "from rgf.sklearn import RGFRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import starboost as sb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from autogluon.features.generators import AutoMLPipelineFeatureGenerator\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing functions\n",
    "from Ensembler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getenv('DATA_FOLDER_PATH', 'Data/')\n",
    "#base_path = os.getenv('DATA_FOLDER_PATH', '/content/drive/MyDrive/DS_Projects/Playground_Series/Ps4e12_Regression_Insuranse_Premium_Prediction/Data/')\n",
    "\n",
    "train = pd.read_parquet(os.path.join(base_path, 'train_transformed_fe.parquet'))\n",
    "test = pd.read_parquet(os.path.join(base_path, 'test_transformed_fe.parquet'))\n",
    "submission = pd.read_csv(os.path.join(base_path, 'sample_submission.csv'))\n",
    "original = pd.read_parquet(os.path.join(base_path, 'original_transformed.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bagged_model(\n",
    "    X, \n",
    "    y, \n",
    "    model_name,\n",
    "    model_params, \n",
    "    n_folds=5, \n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a bagged model using K-fold cross-validation approach.\n",
    "    \n",
    "    Args:\n",
    "        X: Training features\n",
    "        y: Target variable\n",
    "        model_name: Boosting algorithm class (XGBRegressor, LGBMRegressor, etc)\n",
    "        model_params: Parameters for the model\n",
    "        n_folds: Number of folds for cross-validation\n",
    "        random_state: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        oof_predictions: Out-of-fold predictions for training data\n",
    "        models: List of trained models\n",
    "    \"\"\"\n",
    "    # Initialize arrays for predictions\n",
    "    oof_predictions = np.zeros(len(X))\n",
    "    models = []\n",
    "    \n",
    "    # Initialize lists to store CV scores for each fold\n",
    "    fold_rmse_scores = []\n",
    "    \n",
    "    # Create K-fold splits\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Check if model supports early stopping\n",
    "    has_early_stopping = hasattr(model_name, 'early_stopping_rounds') or \\\n",
    "                        any(param in model_params for param in ['early_stopping_rounds', 'early_stopping'])\n",
    "    if has_early_stopping:\n",
    "        print(\"Early stopping enabled\")\n",
    "    \n",
    "    # Train K models\n",
    "    for fold, (train_idx, val_idx) in tqdm(enumerate(kf.split(X, y))):\n",
    "        # Initialize and train model\n",
    "        model = model_name(\n",
    "            **model_params,\n",
    "        )\n",
    "        \n",
    "        if 'LGBMRegressor' not in model_name.__name__ and has_early_stopping:\n",
    "            model.fit(\n",
    "                X.iloc[train_idx],\n",
    "                y.iloc[train_idx],\n",
    "                eval_set=[(X.iloc[val_idx], y.iloc[val_idx])],\n",
    "                verbose=0\n",
    "            )\n",
    "        elif 'LGBMRegressor' in model_name.__name__:\n",
    "            model.fit(\n",
    "                X.iloc[train_idx],\n",
    "                y.iloc[train_idx],\n",
    "                eval_set=[(X.iloc[val_idx], y.iloc[val_idx])]\n",
    "            )\n",
    "        else:\n",
    "            model.fit(\n",
    "                X.iloc[train_idx],\n",
    "                y.iloc[train_idx]\n",
    "            )\n",
    "        \n",
    "        # Generate OOF predictions for this fold\n",
    "        fold_preds = model.predict(X.iloc[val_idx])\n",
    "        oof_predictions[val_idx] = fold_preds\n",
    "        \n",
    "        # Calculate and store RMSE for this fold\n",
    "        fold_rmse = root_mean_squared_error(y.iloc[val_idx], fold_preds)\n",
    "        fold_rmse_scores.append(fold_rmse)\n",
    "        \n",
    "        # Save model\n",
    "        models.append(model)\n",
    "\n",
    "    # Calculate mean and std of fold scores\n",
    "    mean_rmse = np.mean(fold_rmse_scores)\n",
    "    std_rmse = np.std(fold_rmse_scores)\n",
    "\n",
    "    print(f\"Cross-validation Scores (Mean ± Std):\")\n",
    "    print(f\"RMSE Score: {mean_rmse:.4f} ± {std_rmse:.4f}\")\n",
    "\n",
    "    return oof_predictions, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training and test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tWarning: Data size prior to feature transformation consumes 20.2% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  Index(['age', 'gender', 'annual_income', 'number_of_dependents',\n",
      "       'health_score', 'previous_claims', 'vehicle_age', 'credit_score',\n",
      "       'insurance_duration', 'smoking_status', 'year', 'month', 'day',\n",
      "       'week_of_year', 'quarter', 'year_sin', 'month_sin', 'month_cos',\n",
      "       'day_sin', 'day_cos', 'is_weekend', 'is_month_end', 'is_month_start',\n",
      "       'is_quarter_end', 'is_quarter_start', 'policy_age_days',\n",
      "       'week_of_month', 'days_in_month', 'days_remaining_in_month',\n",
      "       'income_per_dependent', 'total_risk_score', 'claims_to_duration_ratio',\n",
      "       'vehicle_to_driver_age_ratio', 'is_young_driver', 'lifestyle_score',\n",
      "       'location_risk', 'location_avg_credit', 'responsibility_score',\n",
      "       'family_risk_factor', 'asset_risk', 'dependent_income_ratio',\n",
      "       '4log_WeightedEnsemble_L4', '12nonlog_WeightedEnsemble_L3',\n",
      "       'xgb_complicated_probs', 'lgb_complicated_probs',\n",
      "       'xgb_complicated_probs_rank', 'lgb_complicated_probs_rank',\n",
      "       'marital_status', 'education_level', 'occupation', 'location',\n",
      "       'policy_type', 'customer_feedback', 'exercise_frequency',\n",
      "       'property_type', 'day_of_week', 'month_name', 'season',\n",
      "       'income_bracket', 'premium_segment', 'policy_start',\n",
      "       'policy_start.dayofweek'],\n",
      "      dtype='object')\n",
      "Target:  [7.96206731 7.30249642 6.34212142 6.64118217 7.61233684]\n"
     ]
    }
   ],
   "source": [
    "target_log = train['premium_amount_log']\n",
    "feature_generator = AutoMLPipelineFeatureGenerator()\n",
    "train.drop(columns = ['policy_start_date', 'premium_amount_log'], inplace=True)\n",
    "train_transformed = feature_generator.fit_transform(train, target_log)\n",
    "test_transformed = feature_generator.transform(test)\n",
    "features = train_transformed.columns\n",
    "\n",
    "print(\"Features: \", features)\n",
    "print(\"Target: \", target_log.head().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Original**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tWarning: Data size prior to feature transformation consumes 5.1% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  Index(['age', 'gender', 'annual_income', 'number_of_dependents',\n",
      "       'health_score', 'previous_claims', 'vehicle_age', 'credit_score',\n",
      "       'insurance_duration', 'smoking_status', 'year', 'month', 'day',\n",
      "       'week_of_year', 'quarter', 'year_sin', 'month_sin', 'month_cos',\n",
      "       'day_sin', 'day_cos', 'is_weekend', 'is_month_end', 'is_month_start',\n",
      "       'is_quarter_end', 'is_quarter_start', 'policy_age_days',\n",
      "       'week_of_month', 'days_in_month', 'days_remaining_in_month',\n",
      "       'income_per_dependent', 'total_risk_score', 'claims_to_duration_ratio',\n",
      "       'vehicle_to_driver_age_ratio', 'is_young_driver', 'lifestyle_score',\n",
      "       'location_risk', 'location_avg_credit', 'responsibility_score',\n",
      "       'family_risk_factor', 'asset_risk', 'dependent_income_ratio',\n",
      "       '4log_WeightedEnsemble_L4', '12nonlog_WeightedEnsemble_L3',\n",
      "       'xgb_complicated_probs', 'lgb_complicated_probs',\n",
      "       'xgb_complicated_probs_rank', 'lgb_complicated_probs_rank',\n",
      "       'marital_status', 'education_level', 'occupation', 'location',\n",
      "       'policy_type', 'customer_feedback', 'exercise_frequency',\n",
      "       'property_type', 'day_of_week', 'month_name', 'season',\n",
      "       'income_bracket', 'premium_segment', 'policy_start',\n",
      "       'policy_start.dayofweek'],\n",
      "      dtype='object')\n",
      "Target:  [5.73334128 6.24997524 6.74523635 6.83303173 5.7170277 ]\n"
     ]
    }
   ],
   "source": [
    "target_original = np.log1p(original['premium_amount'])\n",
    "original.drop(columns = ['policy_start_date', 'premium_amount'], inplace=True)\n",
    "feature_generator = AutoMLPipelineFeatureGenerator()\n",
    "original_transformed = feature_generator.fit_transform(original, target_original)\n",
    "features_original = original_transformed.columns\n",
    "\n",
    "print(\"Features: \", features)\n",
    "print(\"Target: \", target_original.head().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'Missing' category and fill NaN values for both train and test\n",
    "categorical_features = [col for col in train_transformed.columns if train_transformed[col].dtype == 'category']\n",
    "for col in categorical_features:\n",
    "    # Add 'Missing' to categories\n",
    "    train_transformed[col] = train_transformed[col].cat.add_categories('Missing')\n",
    "    test_transformed[col] = test_transformed[col].cat.add_categories('Missing')\n",
    "    original_transformed[col] = original_transformed[col].cat.add_categories('Missing')\n",
    "    \n",
    "    # Now fill NaN values\n",
    "    train_transformed[col] = train_transformed[col].fillna('Missing')\n",
    "    test_transformed[col] = test_transformed[col].fillna('Missing')\n",
    "    original_transformed[col] = original_transformed[col].fillna('Missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Optuna Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-31 13:35:47,500] Using an existing study with name 'lgbm_tunning' instead of creating a new one.\n",
      "[I 2024-12-31 13:36:00,397] Trial 2 finished with value: 1.0446194751755853 and parameters: {'n_estimators': 916, 'learning_rate': 0.06265863073509052, 'num_leaves': 546, 'max_depth': 5, 'min_child_samples': 367, 'subsample': 0.9002722723060412, 'colsample_bytree': 0.8383653344073876, 'reg_alpha': 3.2877744185521007, 'reg_lambda': 8.31373515608923}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:36:13,069] Trial 3 finished with value: 1.0446661099120897 and parameters: {'n_estimators': 959, 'learning_rate': 0.15217068150918125, 'num_leaves': 382, 'max_depth': 6, 'min_child_samples': 363, 'subsample': 0.8887117734071955, 'colsample_bytree': 0.7912968310301876, 'reg_alpha': 9.410271550390382, 'reg_lambda': 8.182802298486214}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:36:50,889] Trial 4 finished with value: 1.0452072253158355 and parameters: {'n_estimators': 486, 'learning_rate': 0.042772143093783445, 'num_leaves': 231, 'max_depth': 10, 'min_child_samples': 69, 'subsample': 0.6209936669113211, 'colsample_bytree': 0.9374859808414674, 'reg_alpha': 8.89807597205193, 'reg_lambda': 1.5673044772517173}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:37:16,582] Trial 5 finished with value: 1.0447350687859387 and parameters: {'n_estimators': 1625, 'learning_rate': 0.02612613435010859, 'num_leaves': 62, 'max_depth': 12, 'min_child_samples': 246, 'subsample': 0.5145256288709963, 'colsample_bytree': 0.8659260481178295, 'reg_alpha': 8.51337153286141, 'reg_lambda': 8.48838289764493}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:37:26,527] Trial 6 finished with value: 1.044717951947886 and parameters: {'n_estimators': 2190, 'learning_rate': 0.15035248378930086, 'num_leaves': 156, 'max_depth': 4, 'min_child_samples': 143, 'subsample': 0.5750256237899699, 'colsample_bytree': 0.6149833154330594, 'reg_alpha': 8.936244799674418, 'reg_lambda': 9.36272572926986}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:37:36,618] Trial 7 finished with value: 1.0446905953338268 and parameters: {'n_estimators': 1677, 'learning_rate': 0.1423837037777787, 'num_leaves': 811, 'max_depth': 3, 'min_child_samples': 226, 'subsample': 0.5540776611899754, 'colsample_bytree': 0.607729981330196, 'reg_alpha': 3.4269481954092966, 'reg_lambda': 5.839647384507494}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:37:55,195] Trial 8 finished with value: 1.0446815568922267 and parameters: {'n_estimators': 832, 'learning_rate': 0.03683165398946806, 'num_leaves': 609, 'max_depth': 3, 'min_child_samples': 129, 'subsample': 0.6271100712734536, 'colsample_bytree': 0.5374168360536569, 'reg_alpha': 7.276021875900362, 'reg_lambda': 2.275142308606929}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:38:04,874] Trial 9 finished with value: 1.0446838673515488 and parameters: {'n_estimators': 2667, 'learning_rate': 0.14300827625043472, 'num_leaves': 166, 'max_depth': 3, 'min_child_samples': 24, 'subsample': 0.9059718643998742, 'colsample_bytree': 0.6107453122494113, 'reg_alpha': 8.507029568784166, 'reg_lambda': 1.0136535784102276}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:38:28,536] Trial 10 finished with value: 1.0454303890187262 and parameters: {'n_estimators': 1040, 'learning_rate': 0.1868473963505479, 'num_leaves': 935, 'max_depth': 9, 'min_child_samples': 122, 'subsample': 0.5534407967385154, 'colsample_bytree': 0.8414849002678588, 'reg_alpha': 4.659605983391796, 'reg_lambda': 6.6026844017901345}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:38:37,105] Trial 11 finished with value: 1.0447122842121974 and parameters: {'n_estimators': 1861, 'learning_rate': 0.1801354933813508, 'num_leaves': 104, 'max_depth': 3, 'min_child_samples': 349, 'subsample': 0.8530940875176213, 'colsample_bytree': 0.6679012458089758, 'reg_alpha': 7.144525328660447, 'reg_lambda': 8.054647444409866}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:38:50,926] Trial 12 finished with value: 1.0446972149326734 and parameters: {'n_estimators': 454, 'learning_rate': 0.07838304418487746, 'num_leaves': 576, 'max_depth': 6, 'min_child_samples': 482, 'subsample': 0.9876830599208128, 'colsample_bytree': 0.9871755172546797, 'reg_alpha': 0.12697316540936043, 'reg_lambda': 3.9640014392471468}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:39:04,889] Trial 13 finished with value: 1.044748679900109 and parameters: {'n_estimators': 1195, 'learning_rate': 0.08685366568288276, 'num_leaves': 385, 'max_depth': 6, 'min_child_samples': 381, 'subsample': 0.778763002842457, 'colsample_bytree': 0.7732968025775804, 'reg_alpha': 2.656439717959147, 'reg_lambda': 7.299263892092779}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:39:19,210] Trial 14 finished with value: 1.044723991858513 and parameters: {'n_estimators': 1274, 'learning_rate': 0.11227915578915738, 'num_leaves': 423, 'max_depth': 6, 'min_child_samples': 379, 'subsample': 0.9053683483299746, 'colsample_bytree': 0.7547435406135256, 'reg_alpha': 1.5959174023450917, 'reg_lambda': 9.91249833510681}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:39:41,531] Trial 15 finished with value: 1.0448259361210954 and parameters: {'n_estimators': 736, 'learning_rate': 0.06479361265387751, 'num_leaves': 707, 'max_depth': 8, 'min_child_samples': 484, 'subsample': 0.7471171171631388, 'colsample_bytree': 0.8393749205150528, 'reg_alpha': 5.401967706290151, 'reg_lambda': 4.537134906172212}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:39:52,962] Trial 16 finished with value: 1.0446744598029059 and parameters: {'n_estimators': 309, 'learning_rate': 0.11106065628085146, 'num_leaves': 372, 'max_depth': 5, 'min_child_samples': 315, 'subsample': 0.9881752973874596, 'colsample_bytree': 0.7036855631775868, 'reg_alpha': 5.505975219052173, 'reg_lambda': 5.9177248063378025}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:40:07,869] Trial 17 finished with value: 1.0448538988552822 and parameters: {'n_estimators': 1358, 'learning_rate': 0.1624898904122568, 'num_leaves': 509, 'max_depth': 7, 'min_child_samples': 429, 'subsample': 0.7825096904901793, 'colsample_bytree': 0.9011530599010392, 'reg_alpha': 3.4920070373606493, 'reg_lambda': 8.560069244856157}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:40:42,056] Trial 18 finished with value: 1.0446771399048105 and parameters: {'n_estimators': 841, 'learning_rate': 0.010565794497953487, 'num_leaves': 722, 'max_depth': 5, 'min_child_samples': 294, 'subsample': 0.8943759397392171, 'colsample_bytree': 0.7869602092449358, 'reg_alpha': 9.84115095425926, 'reg_lambda': 3.2383322486679815}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:41:00,907] Trial 19 finished with value: 1.0450062218839937 and parameters: {'n_estimators': 2965, 'learning_rate': 0.1285290664852628, 'num_leaves': 296, 'max_depth': 8, 'min_child_samples': 416, 'subsample': 0.8417383004236881, 'colsample_bytree': 0.8121011153006299, 'reg_alpha': 6.631045255742079, 'reg_lambda': 7.430967688344305}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:41:14,426] Trial 20 finished with value: 1.044660975071919 and parameters: {'n_estimators': 2116, 'learning_rate': 0.05937501315762316, 'num_leaves': 480, 'max_depth': 5, 'min_child_samples': 196, 'subsample': 0.7158177481746156, 'colsample_bytree': 0.703825937951518, 'reg_alpha': 1.1290852842993928, 'reg_lambda': 0.004190439896884612}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:41:27,815] Trial 21 finished with value: 1.04466706073825 and parameters: {'n_estimators': 2305, 'learning_rate': 0.05762573984538145, 'num_leaves': 527, 'max_depth': 5, 'min_child_samples': 202, 'subsample': 0.7093800260641968, 'colsample_bytree': 0.7080664886702479, 'reg_alpha': 0.21002169300577234, 'reg_lambda': 0.3565042041935138}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:41:39,610] Trial 22 finished with value: 1.0447508353325181 and parameters: {'n_estimators': 2159, 'learning_rate': 0.089219696214974, 'num_leaves': 952, 'max_depth': 4, 'min_child_samples': 190, 'subsample': 0.6912885789383227, 'colsample_bytree': 0.50665077873257, 'reg_alpha': 1.5476303697368845, 'reg_lambda': 2.964944861396269}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:41:59,283] Trial 23 finished with value: 1.0448246981037177 and parameters: {'n_estimators': 1492, 'learning_rate': 0.05154963794541321, 'num_leaves': 460, 'max_depth': 7, 'min_child_samples': 279, 'subsample': 0.8308795115188012, 'colsample_bytree': 0.7213582287073318, 'reg_alpha': 1.647911539671493, 'reg_lambda': 5.1251903467801245}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:42:11,624] Trial 24 finished with value: 1.0446597754309401 and parameters: {'n_estimators': 1908, 'learning_rate': 0.07180287430493376, 'num_leaves': 287, 'max_depth': 5, 'min_child_samples': 339, 'subsample': 0.9468856147909344, 'colsample_bytree': 0.6619330859250918, 'reg_alpha': 4.114382089526041, 'reg_lambda': 9.064646479531662}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:42:22,669] Trial 25 finished with value: 1.0446332166567462 and parameters: {'n_estimators': 1943, 'learning_rate': 0.07123258446301639, 'num_leaves': 272, 'max_depth': 4, 'min_child_samples': 310, 'subsample': 0.9479455662721394, 'colsample_bytree': 0.6651634861486385, 'reg_alpha': 4.2510690242226445, 'reg_lambda': 9.569142932379409}. Best is trial 2 with value: 1.0446194751755853.\n",
      "[I 2024-12-31 13:42:34,151] Trial 26 finished with value: 1.0446119477009475 and parameters: {'n_estimators': 1841, 'learning_rate': 0.07715145129307084, 'num_leaves': 298, 'max_depth': 4, 'min_child_samples': 329, 'subsample': 0.9410836635994333, 'colsample_bytree': 0.6510573585742427, 'reg_alpha': 3.865785602747518, 'reg_lambda': 9.3514368681053}. Best is trial 26 with value: 1.0446119477009475.\n",
      "[I 2024-12-31 13:42:44,673] Trial 27 finished with value: 1.044698033223311 and parameters: {'n_estimators': 2443, 'learning_rate': 0.09502260494112963, 'num_leaves': 269, 'max_depth': 4, 'min_child_samples': 416, 'subsample': 0.9408213959529527, 'colsample_bytree': 0.5819208880958808, 'reg_alpha': 2.7324569448683915, 'reg_lambda': 9.887888739298402}. Best is trial 26 with value: 1.0446119477009475.\n",
      "[I 2024-12-31 13:42:54,831] Trial 28 finished with value: 1.0446949670011925 and parameters: {'n_estimators': 1916, 'learning_rate': 0.10401097027847629, 'num_leaves': 30, 'max_depth': 4, 'min_child_samples': 287, 'subsample': 0.9561082021749743, 'colsample_bytree': 0.6477919288586903, 'reg_alpha': 4.58808975480212, 'reg_lambda': 7.226748848284339}. Best is trial 26 with value: 1.0446119477009475.\n",
      "[I 2024-12-31 13:43:08,374] Trial 29 finished with value: 1.0446019222867724 and parameters: {'n_estimators': 1759, 'learning_rate': 0.03467241943500608, 'num_leaves': 641, 'max_depth': 4, 'min_child_samples': 322, 'subsample': 0.9998739011414454, 'colsample_bytree': 0.7477440012532092, 'reg_alpha': 5.801783492189665, 'reg_lambda': 9.005691943025274}. Best is trial 29 with value: 1.0446019222867724.\n",
      "[I 2024-12-31 13:44:24,497] Trial 30 finished with value: 1.0453045008430146 and parameters: {'n_estimators': 1644, 'learning_rate': 0.024735084390675746, 'num_leaves': 680, 'max_depth': 12, 'min_child_samples': 258, 'subsample': 0.9886955175910975, 'colsample_bytree': 0.7364508717075438, 'reg_alpha': 6.142439627524864, 'reg_lambda': 9.042409704163969}. Best is trial 29 with value: 1.0446019222867724.\n",
      "[I 2024-12-31 13:45:13,677] Trial 31 finished with value: 1.0446935117510179 and parameters: {'n_estimators': 1072, 'learning_rate': 0.010677602360683935, 'num_leaves': 622, 'max_depth': 7, 'min_child_samples': 457, 'subsample': 0.8683000223043432, 'colsample_bytree': 0.8106964389688517, 'reg_alpha': 3.045136432836625, 'reg_lambda': 8.106558536253438}. Best is trial 29 with value: 1.0446019222867724.\n",
      "[I 2024-12-31 13:45:30,166] Trial 32 finished with value: 1.0447296710470804 and parameters: {'n_estimators': 1501, 'learning_rate': 0.044511311338409354, 'num_leaves': 847, 'max_depth': 6, 'min_child_samples': 374, 'subsample': 0.8077872101979374, 'colsample_bytree': 0.9142015054812953, 'reg_alpha': 5.931934176971807, 'reg_lambda': 7.7293956251149645}. Best is trial 29 with value: 1.0446019222867724.\n",
      "[I 2024-12-31 13:45:41,174] Trial 33 finished with value: 1.044636061363941 and parameters: {'n_estimators': 1802, 'learning_rate': 0.07558314806958542, 'num_leaves': 327, 'max_depth': 4, 'min_child_samples': 324, 'subsample': 0.9287691212070914, 'colsample_bytree': 0.6694081594031207, 'reg_alpha': 3.9265108268387205, 'reg_lambda': 9.364217611732672}. Best is trial 29 with value: 1.0446019222867724.\n",
      "[I 2024-12-31 13:45:55,814] Trial 34 finished with value: 1.0445858360398745 and parameters: {'n_estimators': 2043, 'learning_rate': 0.02978975570190149, 'num_leaves': 234, 'max_depth': 4, 'min_child_samples': 315, 'subsample': 0.9672702911782369, 'colsample_bytree': 0.7591514248810258, 'reg_alpha': 2.265160578979418, 'reg_lambda': 8.6520952162232}. Best is trial 34 with value: 1.0445858360398745.\n",
      "[I 2024-12-31 13:46:08,552] Trial 35 finished with value: 1.0445897134181594 and parameters: {'n_estimators': 2516, 'learning_rate': 0.03219926682802796, 'num_leaves': 186, 'max_depth': 3, 'min_child_samples': 344, 'subsample': 0.9704028078210343, 'colsample_bytree': 0.8721970077635264, 'reg_alpha': 2.52688747033477, 'reg_lambda': 8.68709498442042}. Best is trial 34 with value: 1.0445858360398745.\n",
      "[I 2024-12-31 13:46:20,980] Trial 36 finished with value: 1.0445617160702148 and parameters: {'n_estimators': 2467, 'learning_rate': 0.03421925264885447, 'num_leaves': 205, 'max_depth': 3, 'min_child_samples': 262, 'subsample': 0.9709691415274193, 'colsample_bytree': 0.8688928624064323, 'reg_alpha': 2.0853334907754233, 'reg_lambda': 8.709289605045848}. Best is trial 36 with value: 1.0445617160702148.\n",
      "[I 2024-12-31 13:46:33,966] Trial 37 finished with value: 1.0445884836638994 and parameters: {'n_estimators': 2591, 'learning_rate': 0.02925790106430691, 'num_leaves': 192, 'max_depth': 3, 'min_child_samples': 272, 'subsample': 0.972449112248494, 'colsample_bytree': 0.8756003494513412, 'reg_alpha': 0.874032877292894, 'reg_lambda': 8.558388496241268}. Best is trial 36 with value: 1.0445617160702148.\n",
      "[I 2024-12-31 13:46:50,931] Trial 38 finished with value: 1.044573259904056 and parameters: {'n_estimators': 2608, 'learning_rate': 0.019543969765585367, 'num_leaves': 210, 'max_depth': 3, 'min_child_samples': 243, 'subsample': 0.9651464470671548, 'colsample_bytree': 0.8765604457519266, 'reg_alpha': 0.7818170220775471, 'reg_lambda': 6.485321994618637}. Best is trial 36 with value: 1.0445617160702148.\n",
      "[I 2024-12-31 13:47:06,849] Trial 39 finished with value: 1.0445265210531955 and parameters: {'n_estimators': 2816, 'learning_rate': 0.021471826254699997, 'num_leaves': 105, 'max_depth': 3, 'min_child_samples': 233, 'subsample': 0.8737928593252876, 'colsample_bytree': 0.9365249766889353, 'reg_alpha': 0.9650452151673026, 'reg_lambda': 6.661640637785808}. Best is trial 39 with value: 1.0445265210531955.\n",
      "[I 2024-12-31 13:47:41,536] Trial 40 finished with value: 1.044932477458065 and parameters: {'n_estimators': 2893, 'learning_rate': 0.019609331006819563, 'num_leaves': 97, 'max_depth': 11, 'min_child_samples': 241, 'subsample': 0.8815834675027144, 'colsample_bytree': 0.971568472248582, 'reg_alpha': 2.1430121302102263, 'reg_lambda': 6.6462923373424365}. Best is trial 39 with value: 1.0445265210531955.\n",
      "[I 2024-12-31 13:47:52,547] Trial 41 finished with value: 1.0445644352430457 and parameters: {'n_estimators': 2776, 'learning_rate': 0.045998405569486825, 'num_leaves': 128, 'max_depth': 3, 'min_child_samples': 158, 'subsample': 0.9175207265764362, 'colsample_bytree': 0.9417036495141163, 'reg_alpha': 0.8491035443368307, 'reg_lambda': 6.3093337234504}. Best is trial 39 with value: 1.0445265210531955.\n",
      "[I 2024-12-31 13:48:02,709] Trial 42 finished with value: 1.0445600219722977 and parameters: {'n_estimators': 2811, 'learning_rate': 0.04744247499673836, 'num_leaves': 119, 'max_depth': 3, 'min_child_samples': 154, 'subsample': 0.9131667671369034, 'colsample_bytree': 0.9464288514332024, 'reg_alpha': 0.7510249052038598, 'reg_lambda': 5.807300066804326}. Best is trial 39 with value: 1.0445265210531955.\n",
      "[I 2024-12-31 13:48:14,000] Trial 43 finished with value: 1.0445939259386257 and parameters: {'n_estimators': 2705, 'learning_rate': 0.045348889457103886, 'num_leaves': 112, 'max_depth': 3, 'min_child_samples': 163, 'subsample': 0.9197478099204122, 'colsample_bytree': 0.94954449206225, 'reg_alpha': 0.7729552684002083, 'reg_lambda': 5.687692220703111}. Best is trial 39 with value: 1.0445265210531955.\n",
      "[I 2024-12-31 13:48:31,290] Trial 44 finished with value: 1.0445292469296745 and parameters: {'n_estimators': 2796, 'learning_rate': 0.016343326391203717, 'num_leaves': 21, 'max_depth': 3, 'min_child_samples': 98, 'subsample': 0.8675887321772011, 'colsample_bytree': 0.9268863194820478, 'reg_alpha': 0.624843095202408, 'reg_lambda': 6.474152647472669}. Best is trial 39 with value: 1.0445265210531955.\n",
      "[I 2024-12-31 13:48:42,847] Trial 45 finished with value: 1.0445535313479517 and parameters: {'n_estimators': 2821, 'learning_rate': 0.04115610428928427, 'num_leaves': 71, 'max_depth': 3, 'min_child_samples': 101, 'subsample': 0.8208675446827942, 'colsample_bytree': 0.937699873705718, 'reg_alpha': 0.017795960697190805, 'reg_lambda': 5.283227874153648}. Best is trial 39 with value: 1.0445265210531955.\n",
      "[I 2024-12-31 13:48:58,839] Trial 46 finished with value: 1.0445571777755263 and parameters: {'n_estimators': 2823, 'learning_rate': 0.019086502888441936, 'num_leaves': 26, 'max_depth': 3, 'min_child_samples': 91, 'subsample': 0.8639085571187566, 'colsample_bytree': 0.995447261506843, 'reg_alpha': 0.0028322797017970025, 'reg_lambda': 5.0776547444600375}. Best is trial 39 with value: 1.0445265210531955.\n",
      "[I 2024-12-31 13:49:16,135] Trial 47 finished with value: 1.0445560295050782 and parameters: {'n_estimators': 2815, 'learning_rate': 0.017592205525464824, 'num_leaves': 27, 'max_depth': 3, 'min_child_samples': 97, 'subsample': 0.8243903116068194, 'colsample_bytree': 0.9971104477180125, 'reg_alpha': 0.12526535128975058, 'reg_lambda': 5.067599885805067}. Best is trial 39 with value: 1.0445265210531955.\n",
      "[I 2024-12-31 13:49:40,760] Trial 48 finished with value: 1.0446370941355727 and parameters: {'n_estimators': 2329, 'learning_rate': 0.018209366778563166, 'num_leaves': 31, 'max_depth': 10, 'min_child_samples': 84, 'subsample': 0.8171576412072066, 'colsample_bytree': 0.9981390526819204, 'reg_alpha': 0.12956422563317022, 'reg_lambda': 4.90441813104207}. Best is trial 39 with value: 1.0445265210531955.\n",
      "[I 2024-12-31 13:50:06,285] Trial 49 finished with value: 1.0445464041415795 and parameters: {'n_estimators': 2889, 'learning_rate': 0.010756086700192744, 'num_leaves': 71, 'max_depth': 3, 'min_child_samples': 12, 'subsample': 0.8592575359840797, 'colsample_bytree': 0.9679530718391969, 'reg_alpha': 0.10289935172739953, 'reg_lambda': 4.081444309402099}. Best is trial 39 with value: 1.0445265210531955.\n",
      "[I 2024-12-31 13:50:31,981] Trial 50 finished with value: 1.044548629328215 and parameters: {'n_estimators': 2999, 'learning_rate': 0.010985649330859153, 'num_leaves': 76, 'max_depth': 4, 'min_child_samples': 21, 'subsample': 0.7890271296233279, 'colsample_bytree': 0.9123995032308958, 'reg_alpha': 1.322802120981072, 'reg_lambda': 4.0397474795037756}. Best is trial 39 with value: 1.0445265210531955.\n",
      "[I 2024-12-31 13:50:45,912] Trial 51 finished with value: 1.0445563354265885 and parameters: {'n_estimators': 2989, 'learning_rate': 0.03765939466178474, 'num_leaves': 67, 'max_depth': 4, 'min_child_samples': 5, 'subsample': 0.7860158348004898, 'colsample_bytree': 0.9099948498615671, 'reg_alpha': 1.335092024231179, 'reg_lambda': 4.166240561729679}. Best is trial 39 with value: 1.0445265210531955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LightGBM: {'n_estimators': 2816, 'learning_rate': 0.021471826254699997, 'num_leaves': 105, 'max_depth': 3, 'min_child_samples': 233, 'subsample': 0.8737928593252876, 'colsample_bytree': 0.9365249766889353, 'reg_alpha': 0.9650452151673026, 'reg_lambda': 6.661640637785808}\n"
     ]
    }
   ],
   "source": [
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 300, 3000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 1024),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 500),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0),\n",
    "        'verbose': -1,\n",
    "        'early_stopping_rounds': 100,\n",
    "        'n_jobs':-1\n",
    "    }\n",
    "\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=3, shuffle=True)\n",
    "    scores = []\n",
    "\n",
    "    # Perform k-fold cross validation\n",
    "    for train_idx, valid_idx in kf.split(train_transformed[features]):\n",
    "        X_train_fold = train_transformed[features].iloc[train_idx]\n",
    "        y_train_fold = target_log.iloc[train_idx]\n",
    "        X_valid_fold = train_transformed[features].iloc[valid_idx]\n",
    "        y_valid_fold = target_log.iloc[valid_idx]\n",
    "\n",
    "        model = LGBMRegressor(**params)\n",
    "        model.fit(X_train_fold, y_train_fold,\n",
    "                eval_set=[(X_valid_fold, y_valid_fold)],\n",
    "                eval_metric='rmse')\n",
    "        \n",
    "        preds_fold = model.predict(X_valid_fold)\n",
    "        fold_score = root_mean_squared_error(y_valid_fold, preds_fold)\n",
    "        scores.append(fold_score)\n",
    "\n",
    "    # Return average score across all folds\n",
    "    return np.mean(scores)\n",
    "\n",
    "study_lgbm = optuna.create_study(direction='minimize', \n",
    "                                 study_name=\"lgbm_tunning\",\n",
    "                                 storage=\"sqlite:///\" + os.path.join(base_path, \"optuna_lgb_tuning.db\"),\n",
    "                                 load_if_exists=True,)\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=50)\n",
    "print(\"Best parameters for LightGBM:\", study_lgbm.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-31 13:52:04,392] Using an existing study with name 'xgb_tunning' instead of creating a new one.\n",
      "[I 2024-12-31 13:52:46,188] Trial 3 finished with value: 1.0450581038544389 and parameters: {'n_estimators': 2935, 'learning_rate': 0.08041260849258856, 'max_depth': 8, 'min_child_weight': 363, 'subsample': 0.7085111235451627, 'colsample_bytree': 0.5274600205234209, 'reg_alpha': 8.294206725590694, 'reg_lambda': 4.748755232667342}. Best is trial 3 with value: 1.0450581038544389.\n",
      "[I 2024-12-31 13:53:29,566] Trial 4 finished with value: 1.0449855922375808 and parameters: {'n_estimators': 2715, 'learning_rate': 0.06552044914480966, 'max_depth': 8, 'min_child_weight': 389, 'subsample': 0.5250815173846461, 'colsample_bytree': 0.7439968703794873, 'reg_alpha': 2.0545222720593026, 'reg_lambda': 2.946861715008846}. Best is trial 4 with value: 1.0449855922375808.\n",
      "[I 2024-12-31 13:54:10,344] Trial 5 finished with value: 1.044890852040283 and parameters: {'n_estimators': 2390, 'learning_rate': 0.07951185847192145, 'max_depth': 8, 'min_child_weight': 209, 'subsample': 0.8569593839766769, 'colsample_bytree': 0.8458030139533033, 'reg_alpha': 8.523549395442927, 'reg_lambda': 8.792831513246481}. Best is trial 5 with value: 1.044890852040283.\n",
      "[I 2024-12-31 13:54:50,265] Trial 6 finished with value: 1.044631850301004 and parameters: {'n_estimators': 2144, 'learning_rate': 0.045392687205408525, 'max_depth': 5, 'min_child_weight': 195, 'subsample': 0.5090265392179871, 'colsample_bytree': 0.9416566895256171, 'reg_alpha': 2.2940064890660925, 'reg_lambda': 8.328858680074147}. Best is trial 6 with value: 1.044631850301004.\n",
      "[I 2024-12-31 13:56:32,989] Trial 7 finished with value: 1.0446383553103111 and parameters: {'n_estimators': 1943, 'learning_rate': 0.015187030934364479, 'max_depth': 3, 'min_child_weight': 70, 'subsample': 0.8062773403337853, 'colsample_bytree': 0.5609933471988426, 'reg_alpha': 8.525407357057977, 'reg_lambda': 0.449273522200226}. Best is trial 6 with value: 1.044631850301004.\n",
      "[I 2024-12-31 13:57:26,285] Trial 8 finished with value: 1.0452812270029885 and parameters: {'n_estimators': 1652, 'learning_rate': 0.059376694950170235, 'max_depth': 10, 'min_child_weight': 81, 'subsample': 0.7546987444428721, 'colsample_bytree': 0.5515250046731166, 'reg_alpha': 8.423368388287942, 'reg_lambda': 0.774683239991016}. Best is trial 6 with value: 1.044631850301004.\n",
      "[I 2024-12-31 13:58:31,226] Trial 9 finished with value: 1.0446350205730963 and parameters: {'n_estimators': 2324, 'learning_rate': 0.021514694248560683, 'max_depth': 6, 'min_child_weight': 301, 'subsample': 0.7375194571752379, 'colsample_bytree': 0.9233098716336978, 'reg_alpha': 1.635268200848559, 'reg_lambda': 0.7599208376888726}. Best is trial 6 with value: 1.044631850301004.\n",
      "[I 2024-12-31 14:00:15,075] Trial 10 finished with value: 1.0448429885411834 and parameters: {'n_estimators': 2982, 'learning_rate': 0.017457141885032446, 'max_depth': 10, 'min_child_weight': 412, 'subsample': 0.6107284035532019, 'colsample_bytree': 0.6784549148099002, 'reg_alpha': 6.7395063103303, 'reg_lambda': 6.9695206808378565}. Best is trial 6 with value: 1.044631850301004.\n",
      "[I 2024-12-31 14:01:09,286] Trial 11 finished with value: 1.044569740323818 and parameters: {'n_estimators': 2982, 'learning_rate': 0.026453088770707494, 'max_depth': 4, 'min_child_weight': 437, 'subsample': 0.8638131438112404, 'colsample_bytree': 0.6905630126540658, 'reg_alpha': 7.762441386277238, 'reg_lambda': 2.9992058049736308}. Best is trial 11 with value: 1.044569740323818.\n",
      "[I 2024-12-31 14:04:11,427] Trial 12 finished with value: 1.0446264618589989 and parameters: {'n_estimators': 1882, 'learning_rate': 0.006251560723647804, 'max_depth': 5, 'min_child_weight': 373, 'subsample': 0.8313730309695788, 'colsample_bytree': 0.5820099858748997, 'reg_alpha': 2.085273932828381, 'reg_lambda': 9.675610307330361}. Best is trial 11 with value: 1.044569740323818.\n",
      "[I 2024-12-31 14:04:57,429] Trial 13 finished with value: 1.0445908829718513 and parameters: {'n_estimators': 593, 'learning_rate': 0.04001303401115995, 'max_depth': 3, 'min_child_weight': 486, 'subsample': 0.9893791764902343, 'colsample_bytree': 0.6634368078585517, 'reg_alpha': 5.0445567837698855, 'reg_lambda': 3.6315934942273387}. Best is trial 11 with value: 1.044569740323818.\n",
      "[I 2024-12-31 14:05:42,128] Trial 14 finished with value: 1.0445795660594794 and parameters: {'n_estimators': 716, 'learning_rate': 0.03809600872954823, 'max_depth': 3, 'min_child_weight': 475, 'subsample': 0.9902408409978134, 'colsample_bytree': 0.6744541848586203, 'reg_alpha': 4.695367171402317, 'reg_lambda': 3.5698127521782417}. Best is trial 11 with value: 1.044569740323818.\n",
      "[I 2024-12-31 14:06:28,484] Trial 15 finished with value: 1.0445678629404567 and parameters: {'n_estimators': 1005, 'learning_rate': 0.03426801672839427, 'max_depth': 3, 'min_child_weight': 482, 'subsample': 0.9773485722480273, 'colsample_bytree': 0.787708125925971, 'reg_alpha': 4.4730585837358445, 'reg_lambda': 2.34786708887156}. Best is trial 15 with value: 1.0445678629404567.\n",
      "[I 2024-12-31 14:07:17,410] Trial 16 finished with value: 1.0445926698426666 and parameters: {'n_estimators': 1201, 'learning_rate': 0.030598212993902744, 'max_depth': 5, 'min_child_weight': 478, 'subsample': 0.9136339621492748, 'colsample_bytree': 0.8239903067816838, 'reg_alpha': 4.764438599704439, 'reg_lambda': 2.22773673866504}. Best is trial 15 with value: 1.0445678629404567.\n",
      "[I 2024-12-31 14:08:14,503] Trial 17 finished with value: 1.045247747924885 and parameters: {'n_estimators': 1256, 'learning_rate': 0.05574541227236826, 'max_depth': 12, 'min_child_weight': 302, 'subsample': 0.9091387088935067, 'colsample_bytree': 0.7794203417724981, 'reg_alpha': 6.496282475037775, 'reg_lambda': 5.59009778417595}. Best is trial 15 with value: 1.0445678629404567.\n",
      "[I 2024-12-31 14:08:43,859] Trial 18 finished with value: 1.0445888540592252 and parameters: {'n_estimators': 1082, 'learning_rate': 0.0963554110058318, 'max_depth': 4, 'min_child_weight': 427, 'subsample': 0.9183905314123222, 'colsample_bytree': 0.7433572873484049, 'reg_alpha': 9.9449608077315, 'reg_lambda': 2.00297672365069}. Best is trial 15 with value: 1.0445678629404567.\n",
      "[I 2024-12-31 14:09:37,368] Trial 19 finished with value: 1.044710154189979 and parameters: {'n_estimators': 355, 'learning_rate': 0.026995581343771925, 'max_depth': 6, 'min_child_weight': 321, 'subsample': 0.9608964094518521, 'colsample_bytree': 0.9946534259719335, 'reg_alpha': 0.037017879564960054, 'reg_lambda': 5.17022581986015}. Best is trial 15 with value: 1.0445678629404567.\n",
      "[I 2024-12-31 14:12:31,129] Trial 20 finished with value: 1.0446161228369621 and parameters: {'n_estimators': 1483, 'learning_rate': 0.006980246014488649, 'max_depth': 4, 'min_child_weight': 241, 'subsample': 0.8692149878473907, 'colsample_bytree': 0.6170773013068452, 'reg_alpha': 3.4370542630399425, 'reg_lambda': 1.4787630827862686}. Best is trial 15 with value: 1.0445678629404567.\n",
      "[I 2024-12-31 14:13:19,240] Trial 21 finished with value: 1.0447060807384132 and parameters: {'n_estimators': 856, 'learning_rate': 0.03359859962010305, 'max_depth': 6, 'min_child_weight': 130, 'subsample': 0.7918417430508831, 'colsample_bytree': 0.8503058162494156, 'reg_alpha': 6.28673169653737, 'reg_lambda': 4.14946823276388}. Best is trial 15 with value: 1.0445678629404567.\n",
      "[I 2024-12-31 14:13:56,864] Trial 22 finished with value: 1.0445939050876913 and parameters: {'n_estimators': 1497, 'learning_rate': 0.04938588186055742, 'max_depth': 4, 'min_child_weight': 440, 'subsample': 0.6858754186204998, 'colsample_bytree': 0.7939460872661215, 'reg_alpha': 7.302168002853749, 'reg_lambda': 2.7048450724532995}. Best is trial 15 with value: 1.0445678629404567.\n",
      "[I 2024-12-31 14:14:37,122] Trial 23 finished with value: 1.0447855439335025 and parameters: {'n_estimators': 2669, 'learning_rate': 0.06908024560493845, 'max_depth': 7, 'min_child_weight': 8, 'subsample': 0.9333799649994695, 'colsample_bytree': 0.7183866859739118, 'reg_alpha': 3.9793986301068944, 'reg_lambda': 6.06293947451651}. Best is trial 15 with value: 1.0445678629404567.\n",
      "[I 2024-12-31 14:15:20,081] Trial 24 finished with value: 1.044582608247411 and parameters: {'n_estimators': 788, 'learning_rate': 0.04131868294932933, 'max_depth': 3, 'min_child_weight': 500, 'subsample': 0.9994392505365651, 'colsample_bytree': 0.6650459857935064, 'reg_alpha': 5.060383489832119, 'reg_lambda': 3.64875981581388}. Best is trial 15 with value: 1.0445678629404567.\n",
      "[I 2024-12-31 14:16:09,720] Trial 25 finished with value: 1.0446002283246283 and parameters: {'n_estimators': 344, 'learning_rate': 0.035981958610654376, 'max_depth': 3, 'min_child_weight': 453, 'subsample': 0.8784162671586926, 'colsample_bytree': 0.6229045176264412, 'reg_alpha': 3.564446307388126, 'reg_lambda': 3.235702101284731}. Best is trial 15 with value: 1.0445678629404567.\n",
      "[I 2024-12-31 14:17:04,646] Trial 26 finished with value: 1.0445656225009603 and parameters: {'n_estimators': 888, 'learning_rate': 0.0250907032699808, 'max_depth': 4, 'min_child_weight': 455, 'subsample': 0.9597619238494385, 'colsample_bytree': 0.7060905019090604, 'reg_alpha': 5.710610225282591, 'reg_lambda': 1.823050877506028}. Best is trial 26 with value: 1.0445656225009603.\n",
      "[I 2024-12-31 14:18:05,205] Trial 27 finished with value: 1.0445547839964382 and parameters: {'n_estimators': 1013, 'learning_rate': 0.024735031012492463, 'max_depth': 4, 'min_child_weight': 337, 'subsample': 0.9528201038182489, 'colsample_bytree': 0.7070703378979604, 'reg_alpha': 5.8869061255702295, 'reg_lambda': 1.9492056530064512}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:21:01,901] Trial 28 finished with value: 1.0489417984232707 and parameters: {'n_estimators': 989, 'learning_rate': 0.0012946676079305802, 'max_depth': 5, 'min_child_weight': 333, 'subsample': 0.9467817405317511, 'colsample_bytree': 0.785219385505501, 'reg_alpha': 5.978317022518754, 'reg_lambda': 1.5151849046384231}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:22:35,875] Trial 29 finished with value: 1.0446620410602376 and parameters: {'n_estimators': 597, 'learning_rate': 0.012886817093892243, 'max_depth': 7, 'min_child_weight': 398, 'subsample': 0.9616886679668915, 'colsample_bytree': 0.8818100599258267, 'reg_alpha': 5.585227717811882, 'reg_lambda': 0.2575602962694319}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:23:38,378] Trial 30 finished with value: 1.044565065876262 and parameters: {'n_estimators': 1287, 'learning_rate': 0.02168149498044412, 'max_depth': 4, 'min_child_weight': 348, 'subsample': 0.8952543384277747, 'colsample_bytree': 0.7183101480004329, 'reg_alpha': 4.037158934944129, 'reg_lambda': 1.4917099708629755}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:24:47,629] Trial 31 finished with value: 1.0446022313258847 and parameters: {'n_estimators': 1353, 'learning_rate': 0.022213751116412703, 'max_depth': 4, 'min_child_weight': 268, 'subsample': 0.9015506545899034, 'colsample_bytree': 0.62430276574151, 'reg_alpha': 2.9503341764909607, 'reg_lambda': 1.655443084376658}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:27:10,320] Trial 32 finished with value: 1.0448470622633412 and parameters: {'n_estimators': 1680, 'learning_rate': 0.010165531009004782, 'max_depth': 9, 'min_child_weight': 352, 'subsample': 0.6640339205456035, 'colsample_bytree': 0.7059804315032829, 'reg_alpha': 5.515240957488131, 'reg_lambda': 4.712636026558674}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:28:21,867] Trial 33 finished with value: 1.0447111731847236 and parameters: {'n_estimators': 1135, 'learning_rate': 0.022230179993976074, 'max_depth': 6, 'min_child_weight': 262, 'subsample': 0.8288903884578346, 'colsample_bytree': 0.5069755501918329, 'reg_alpha': 7.037796723104833, 'reg_lambda': 1.03180762974766}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:29:11,176] Trial 34 finished with value: 1.044610671783931 and parameters: {'n_estimators': 968, 'learning_rate': 0.030181890537949507, 'max_depth': 4, 'min_child_weight': 364, 'subsample': 0.96137333051661, 'colsample_bytree': 0.7340251231908184, 'reg_alpha': 4.242580317523252, 'reg_lambda': 2.3636443901141457}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:29:50,014] Trial 35 finished with value: 1.044637329662031 and parameters: {'n_estimators': 1382, 'learning_rate': 0.04481270239515292, 'max_depth': 5, 'min_child_weight': 377, 'subsample': 0.9441164712842638, 'colsample_bytree': 0.7685719710920376, 'reg_alpha': 4.230316309173936, 'reg_lambda': 1.2864951776133644}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:31:07,601] Trial 36 finished with value: 1.0446286344461613 and parameters: {'n_estimators': 576, 'learning_rate': 0.017231419233764478, 'max_depth': 3, 'min_child_weight': 412, 'subsample': 0.8927416697646409, 'colsample_bytree': 0.75474828991758, 'reg_alpha': 5.660171385399985, 'reg_lambda': 0.14798357478457325}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:31:59,698] Trial 37 finished with value: 1.04459003088838 and parameters: {'n_estimators': 996, 'learning_rate': 0.026316578609506786, 'max_depth': 4, 'min_child_weight': 459, 'subsample': 0.5605155676832615, 'colsample_bytree': 0.720571646622065, 'reg_alpha': 2.7070134827160874, 'reg_lambda': 2.4921623780447804}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:32:36,061] Trial 38 finished with value: 1.0446344640795686 and parameters: {'n_estimators': 861, 'learning_rate': 0.052723283916636415, 'max_depth': 5, 'min_child_weight': 346, 'subsample': 0.9591923541493198, 'colsample_bytree': 0.8201995845881339, 'reg_alpha': 0.9038639634886909, 'reg_lambda': 1.9513540848144895}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:33:25,741] Trial 39 finished with value: 1.0445858920370574 and parameters: {'n_estimators': 1256, 'learning_rate': 0.03433597176276682, 'max_depth': 3, 'min_child_weight': 297, 'subsample': 0.9774765426950186, 'colsample_bytree': 0.6448203370358807, 'reg_alpha': 3.2926891643805347, 'reg_lambda': 4.162702227620752}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:34:29,911] Trial 40 finished with value: 1.0447929490560848 and parameters: {'n_estimators': 1545, 'learning_rate': 0.02188162999718384, 'max_depth': 7, 'min_child_weight': 210, 'subsample': 0.8419078746149844, 'colsample_bytree': 0.8063326591630428, 'reg_alpha': 3.951933593577321, 'reg_lambda': 0.8730415270298646}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:35:45,973] Trial 41 finished with value: 1.044610796650069 and parameters: {'n_estimators': 1885, 'learning_rate': 0.016049570386942123, 'max_depth': 5, 'min_child_weight': 404, 'subsample': 0.781438315488271, 'colsample_bytree': 0.8650867983049828, 'reg_alpha': 5.259393855741839, 'reg_lambda': 3.0754822429514355}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:36:25,706] Trial 42 finished with value: 1.0446096539924403 and parameters: {'n_estimators': 684, 'learning_rate': 0.0443350062096494, 'max_depth': 4, 'min_child_weight': 451, 'subsample': 0.9358730078982489, 'colsample_bytree': 0.6991522642000874, 'reg_alpha': 7.7481047823068545, 'reg_lambda': 0.7412337099063605}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:37:10,563] Trial 43 finished with value: 1.0449515593933092 and parameters: {'n_estimators': 1698, 'learning_rate': 0.06706738461176795, 'max_depth': 9, 'min_child_weight': 396, 'subsample': 0.8831155279835727, 'colsample_bytree': 0.758364918414628, 'reg_alpha': 4.519887099039504, 'reg_lambda': 6.706166068375546}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:38:02,200] Trial 44 finished with value: 1.044587131136579 and parameters: {'n_estimators': 2211, 'learning_rate': 0.026760687709531734, 'max_depth': 4, 'min_child_weight': 465, 'subsample': 0.8575876825464934, 'colsample_bytree': 0.7004189690433046, 'reg_alpha': 9.083461913556828, 'reg_lambda': 2.63615141724849}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:38:59,683] Trial 45 finished with value: 1.0446264364113158 and parameters: {'n_estimators': 2604, 'learning_rate': 0.02951436923586306, 'max_depth': 3, 'min_child_weight': 428, 'subsample': 0.9195288945503357, 'colsample_bytree': 0.591248878976726, 'reg_alpha': 7.747837026650995, 'reg_lambda': 1.8270848221522131}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:40:10,519] Trial 46 finished with value: 1.0445769469079342 and parameters: {'n_estimators': 1085, 'learning_rate': 0.020004753048265052, 'max_depth': 4, 'min_child_weight': 498, 'subsample': 0.9768308245218942, 'colsample_bytree': 0.681091410459362, 'reg_alpha': 6.113739247898394, 'reg_lambda': 3.1370837084477765}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:42:07,623] Trial 47 finished with value: 1.0446557221868569 and parameters: {'n_estimators': 2875, 'learning_rate': 0.010832814078873038, 'max_depth': 5, 'min_child_weight': 429, 'subsample': 0.9993338085088285, 'colsample_bytree': 0.6410608800349684, 'reg_alpha': 9.280647527436624, 'reg_lambda': 4.209949470502745}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:43:32,290] Trial 48 finished with value: 1.0451397450752655 and parameters: {'n_estimators': 1378, 'learning_rate': 0.02559971700514545, 'max_depth': 12, 'min_child_weight': 383, 'subsample': 0.8161549410051129, 'colsample_bytree': 0.7414864392186804, 'reg_alpha': 7.0343484977850945, 'reg_lambda': 8.035541871198848}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:44:21,268] Trial 49 finished with value: 1.0446006630345792 and parameters: {'n_estimators': 2032, 'learning_rate': 0.03779592650846574, 'max_depth': 3, 'min_child_weight': 479, 'subsample': 0.7563695457384103, 'colsample_bytree': 0.6840687626435326, 'reg_alpha': 7.595429544174232, 'reg_lambda': 2.213741373239935}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:45:14,689] Trial 50 finished with value: 1.0446715161830546 and parameters: {'n_estimators': 2462, 'learning_rate': 0.03210865272781161, 'max_depth': 6, 'min_child_weight': 325, 'subsample': 0.8592648807668536, 'colsample_bytree': 0.7302603140037638, 'reg_alpha': 8.090423077252733, 'reg_lambda': 1.1781309476834934}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:46:33,689] Trial 51 finished with value: 1.0446276392933938 and parameters: {'n_estimators': 481, 'learning_rate': 0.01586036432406213, 'max_depth': 3, 'min_child_weight': 143, 'subsample': 0.9257500321911436, 'colsample_bytree': 0.6466142294874972, 'reg_alpha': 6.519321656013265, 'reg_lambda': 0.4564385749749551}. Best is trial 27 with value: 1.0445547839964382.\n",
      "[I 2024-12-31 14:49:32,434] Trial 52 finished with value: 1.0446416834340744 and parameters: {'n_estimators': 934, 'learning_rate': 0.004251323572297683, 'max_depth': 5, 'min_child_weight': 290, 'subsample': 0.8997448060211791, 'colsample_bytree': 0.7686221390892405, 'reg_alpha': 4.719072595723949, 'reg_lambda': 3.5463129271813867}. Best is trial 27 with value: 1.0445547839964382.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'n_estimators': 1013, 'learning_rate': 0.024735031012492463, 'max_depth': 4, 'min_child_weight': 337, 'subsample': 0.9528201038182489, 'colsample_bytree': 0.7070703378979604, 'reg_alpha': 5.8869061255702295, 'reg_lambda': 1.9492056530064512}\n"
     ]
    }
   ],
   "source": [
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 300, 3000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 500),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0),\n",
    "        'tree_method': 'hist',\n",
    "        'eval_metric': 'rmse',\n",
    "        'enable_categorical': True,\n",
    "        'early_stopping_rounds': 100,\n",
    "        'n_jobs':-1\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=3, shuffle=True)\n",
    "    scores = []\n",
    "\n",
    "    # Perform k-fold cross validation\n",
    "    for train_idx, valid_idx in kf.split(train_transformed[features]):\n",
    "        X_train_fold = train_transformed[features].iloc[train_idx]\n",
    "        y_train_fold = target_log.iloc[train_idx]\n",
    "        X_valid_fold = train_transformed[features].iloc[valid_idx]\n",
    "        y_valid_fold = target_log.iloc[valid_idx]\n",
    "\n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(X_train_fold, y_train_fold,\n",
    "                eval_set=[(X_valid_fold, y_valid_fold)],\n",
    "                verbose=False)\n",
    "        \n",
    "        preds_fold = model.predict(X_valid_fold)\n",
    "        fold_score = root_mean_squared_error(y_valid_fold, preds_fold)\n",
    "        scores.append(fold_score)\n",
    "\n",
    "    # Return average score across all folds\n",
    "    return np.mean(scores)\n",
    "\n",
    "study_xgb = optuna.create_study(direction='minimize', \n",
    "                                 study_name=\"xgb_tunning\",\n",
    "                                 storage=\"sqlite:///\" + os.path.join(base_path, \"optuna_xgb_tuning.db\"),\n",
    "                                 load_if_exists=True,)\n",
    "study_xgb.optimize(objective_xgb, n_trials=50)\n",
    "print(\"Best parameters for XGBoost:\", study_xgb.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_catboost(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 300, 3000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'depth': trial.suggest_int('depth', 3, 12),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 500),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1.0),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0),\n",
    "        'random_strength': trial.suggest_float('random_strength', 1e-8, 10.0),\n",
    "        'task_type': 'CPU',\n",
    "        'eval_metric': 'RMSE',\n",
    "        'early_stopping_rounds': 100,\n",
    "        'cat_features': categorical_features,\n",
    "        'thread_count': -1\n",
    "    }\n",
    "\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=3, shuffle=True)\n",
    "    scores = []\n",
    "\n",
    "    # Perform k-fold cross validation\n",
    "    for train_idx, valid_idx in kf.split(train_transformed[features]):\n",
    "        X_train_fold = train_transformed[features].iloc[train_idx]\n",
    "        y_train_fold = target_log.iloc[train_idx]\n",
    "        X_valid_fold = train_transformed[features].iloc[valid_idx]\n",
    "        y_valid_fold = target_log.iloc[valid_idx]\n",
    "\n",
    "        model = CatBoostRegressor(**params, verbose=False)\n",
    "        model.fit(X_train_fold, y_train_fold,\n",
    "                eval_set=[(X_valid_fold, y_valid_fold)],\n",
    "                verbose=False)\n",
    "        \n",
    "        preds_fold = model.predict(X_valid_fold)\n",
    "        fold_score = root_mean_squared_error(y_valid_fold, preds_fold)\n",
    "        scores.append(fold_score)\n",
    "\n",
    "    # Return average score across all folds\n",
    "    return np.mean(scores)\n",
    "\n",
    "study_catboost = optuna.create_study(direction='minimize',\n",
    "                                   study_name=\"catboost_tunning\",\n",
    "                                   storage=\"sqlite:///\" + os.path.join(base_path, \"optuna_catboost_tuning.db\"),\n",
    "                                   load_if_exists=True)\n",
    "study_catboost.optimize(objective_catboost, n_trials=50)\n",
    "print(\"Best parameters for CatBoost:\", study_catboost.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rgf_data(df):\n",
    "    df_rgf = df.copy()\n",
    "    \n",
    "    # Handle categorical features and convert to numeric\n",
    "    categorical_features = [col for col in df_rgf.columns if df_rgf[col].dtype == 'category']\n",
    "    for col in categorical_features:\n",
    "        df_rgf[col] = df_rgf[col].cat.codes\n",
    "    \n",
    "    # Fill missing values with -999\n",
    "    numeric_cols = df_rgf.select_dtypes(include=['int64', 'float64']).columns\n",
    "    df_rgf[numeric_cols] = df_rgf[numeric_cols].fillna(-999)\n",
    "    df_rgf = df_rgf.astype(float)\n",
    "    # Remove date column if exists\n",
    "    if 'policy_start_date' in df_rgf.columns:\n",
    "        df_rgf.drop(columns=\"policy_start_date\", inplace=True)\n",
    "    return df_rgf\n",
    "\n",
    "train_transformed_rgf = prepare_rgf_data(train_transformed)\n",
    "test_transformed_rgf = prepare_rgf_data(test_transformed)\n",
    "features_rgf = train_transformed_rgf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training with train and test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LightGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:32,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation Scores (Mean ± Std):\n",
      "RMSE Score: 1.0446 ± 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_lgb_params = study_lgbm.best_params\n",
    "best_lgb_params.update({\n",
    "    'verbose': -1,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'n_jobs':-1\n",
    "})\n",
    "\n",
    "lgb_oofs, lgb_models = train_bagged_model(train_transformed[features], target_log, \n",
    "                   model_name=LGBMRegressor, \n",
    "                   model_params=best_lgb_params, \n",
    "                   n_folds=5, \n",
    "                   random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:32, 18.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation Scores (Mean ± Std):\n",
      "RMSE Score: 1.0446 ± 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_xgb_params = study_xgb.best_params\n",
    "best_xgb_params.update({\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'rmse',\n",
    "    'enable_categorical': True,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'n_jobs':-1\n",
    "})\n",
    "\n",
    "xgb_oofs, xgb_models = train_bagged_model(train_transformed[features], target_log, \n",
    "                   model_name=XGBRegressor, \n",
    "                   model_params=best_xgb_params, \n",
    "                   n_folds=5, \n",
    "                   random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CatBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [07:15, 87.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation Scores (Mean ± Std):\n",
      "RMSE Score: 1.0445 ± 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_cgb_params = study_catboost.best_params\n",
    "best_cgb_params.update({\n",
    "    'task_type': 'CPU',\n",
    "    'eval_metric': 'RMSE',\n",
    "    'early_stopping_rounds': 100,\n",
    "    'cat_features': categorical_features,\n",
    "    'thread_count': -1\n",
    "})\n",
    "\n",
    "cgb_oofs, cgb_models = train_bagged_model(train_transformed[features], target_log, \n",
    "                   model_name=CatBoostRegressor, \n",
    "                   model_params=best_cgb_params, \n",
    "                   n_folds=5, \n",
    "                   random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Regularized greedy forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgf_params = {\n",
    "    'max_leaf': 1000,              # Controls tree size\n",
    "    'algorithm': \"RGF_Sib\",\n",
    "    'loss': \"LS\",                  # Least squares loss\n",
    "    'learning_rate': 0.01,\n",
    "    'reg_depth': 1.0,              # Regularization depth\n",
    "    'l2': 0.1,                     # L2 regularization\n",
    "}\n",
    "\n",
    "rgf_oofs, rgf_models = train_bagged_model(train_transformed_rgf[features_rgf], target_log, \n",
    "                   model_name=RGFRegressor, \n",
    "                   model_params=rgf_params, \n",
    "                   n_folds=3, \n",
    "                   random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training fg2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fg2 = pd.read_csv(os.path.join(base_path, 'train_transformed_fg2.csv'))\n",
    "test_fg2 = pd.read_csv(os.path.join(base_path, 'test_transformed_fg2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fg2 = train_fg2.set_index(\"id\")\n",
    "test_fg2 = test_fg2.set_index(\"id\")\n",
    "target_fg2 = train_fg2['premium_amount']\n",
    "train_fg2.drop(columns = ['premium_amount'], inplace=True)\n",
    "features_fg2 = train_fg2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LightGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [11:42, 234.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation Scores (Mean ± Std):\n",
      "RMSE Score: 1.0456 ± 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\"n_estimators\": 3000,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"max_depth\": 8,\n",
    "    \"num_leaves\": 2**8,\n",
    "    \"colsample_bytree\": 0.5,\n",
    "    \"early_stopping_rounds\": 100,\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"verbose\":-1,\n",
    "    'n_jobs':-1}\n",
    "\n",
    "categorical_columns = train_fg2.select_dtypes(['object', 'category']).columns\n",
    "lgb_train_fg2 = train_fg2.copy()\n",
    "for col in categorical_columns:\n",
    "    lgb_train_fg2[col] = lgb_train_fg2[col].astype('category').cat.codes.astype('int32')\n",
    "\n",
    "lgb_fg2_oofs, lgb_fg2_models = train_bagged_model(lgb_train_fg2[features_fg2], target_fg2, \n",
    "                   model_name=LGBMRegressor, \n",
    "                   model_params=lgb_params, \n",
    "                   n_folds=3, \n",
    "                   random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'n_estimators': 3000,\n",
    "    'max_depth': 8,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'learning_rate': 0.001,\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'rmse',\n",
    "    'enable_categorical': True,\n",
    "    'early_stopping_rounds':100,\n",
    "    'n_jobs':-1,\n",
    "    'verbose':-1\n",
    "}\n",
    "\n",
    "categorical_columns = train_fg2.select_dtypes(['object']).columns\n",
    "xgb_train_fg2 = train_fg2.copy()\n",
    "for col in categorical_columns:\n",
    "    xgb_train_fg2[col] = xgb_train_fg2[col].astype('category')\n",
    "    \n",
    "xgb_fg2_oofs, xgb_fg2_models = train_bagged_model(xgb_train_fg2[features_fg2], target_fg2, \n",
    "                   model_name=XGBRegressor, \n",
    "                   model_params=xgb_params, \n",
    "                   n_folds=3, \n",
    "                   random_state=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [44:21, 887.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation Scores (Mean ± Std):\n",
      "RMSE Score: 1.0331 ± 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = train_fg2.select_dtypes(['object', 'category']).columns\n",
    "cgb_train_fg2 = train_fg2.copy()\n",
    "for col in categorical_columns:\n",
    "    cgb_train_fg2[col] = cgb_train_fg2[col].fillna('Missing')\n",
    "    cgb_train_fg2[col] = cgb_train_fg2[col].astype('category')\n",
    "cat_features = [cgb_train_fg2.columns.get_loc(col) for col in categorical_columns]\n",
    "\n",
    "cat_params = {\n",
    "    'depth': 10,\n",
    "    'learning_rate': 0.01,\n",
    "    'l2_leaf_reg': 3.5,\n",
    "    'random_strength': 4,\n",
    "    'bagging_temperature': 0.25,\n",
    "    'eval_metric': 'RMSE',\n",
    "    'loss_function': 'RMSE',\n",
    "    'cat_features': cat_features,\n",
    "    'iterations': 3000,\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "cgb_fg2_oofs, cgb_fg2_models = train_bagged_model(cgb_train_fg2[features_fg2], target_fg2, \n",
    "                   model_name=CatBoostRegressor, \n",
    "                   model_params=cat_params, \n",
    "                   n_folds=3, \n",
    "                   random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Predicting on main train set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataframe with target first\n",
    "oofs_df = pd.DataFrame()\n",
    "oofs_df['premium_amount_log'] = target_log\n",
    "oofs_df['lgb_oofs'] = lgb_oofs\n",
    "oofs_df['xgb_oofs'] = xgb_oofs\n",
    "oofs_df['cgb_oofs'] = cgb_oofs\n",
    "oofs_df['lgb_fg2_oofs'] = lgb_fg2_oofs\n",
    "# oofs_df['xgb_fg2_oofs'] = xgb_fg2_oofs\n",
    "oofs_df['cgb_fg2_oofs'] = cgb_fg2_oofs\n",
    "\n",
    "# Convert predictions to original scale\n",
    "pred_cols = oofs_df.columns\n",
    "oofs_df[pred_cols] = np.expm1(oofs_df[pred_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, model in enumerate(lgb_fg2_models, 1):\n",
    "#     oofs_df[f'lgb_fg2_{i}'] = model.predict(train_fg2[features])\n",
    "\n",
    "# for i, model in enumerate(xgb_fg2_models, 1):\n",
    "#     oofs_df[f'xgb_fg2_{i}'] = model.predict(train_fg2[features])\n",
    "\n",
    "# for i, model in enumerate(cgb_fg2_models, 1):\n",
    "#     oofs_df[f'cgb_fg2_{i}'] = model.predict(train_fg2[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Greedy Weighted Ensemble**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best single model performance: -1.03312 | Model: cgb_fg2_oofs\n",
      "Initial performance: -1.03312 | Models: ['cgb_fg2_oofs']\n",
      "Iteration 1: Added cgb_fg2_oofs, Bag Score: -1.03308, Mean Bag Score: -1.03308, Full Score: -1.03312\n",
      "Iteration 2: Added cgb_fg2_oofs, Bag Score: -1.03367, Mean Bag Score: -1.03337, Full Score: -1.03312\n",
      "Iteration 3: Added cgb_fg2_oofs, Bag Score: -1.03282, Mean Bag Score: -1.03319, Full Score: -1.03312\n",
      "Iteration 4: Added cgb_fg2_oofs, Bag Score: -1.03413, Mean Bag Score: -1.03343, Full Score: -1.03312\n",
      "Iteration 5: Added cgb_fg2_oofs, Bag Score: -1.03307, Mean Bag Score: -1.03335, Full Score: -1.03312\n",
      "Iteration 6: Added cgb_oofs, Bag Score: -1.03464, Mean Bag Score: -1.03357, Full Score: -1.03308\n",
      "Iteration 7: Added cgb_fg2_oofs, Bag Score: -1.03269, Mean Bag Score: -1.03344, Full Score: -1.03305\n",
      "Iteration 8: Added cgb_fg2_oofs, Bag Score: -1.03082, Mean Bag Score: -1.03311, Full Score: -1.03303\n",
      "Iteration 9: Added cgb_fg2_oofs, Bag Score: -1.03234, Mean Bag Score: -1.03303, Full Score: -1.03302\n",
      "Iteration 10: Added cgb_fg2_oofs, Bag Score: -1.03665, Mean Bag Score: -1.03339, Full Score: -1.03301\n",
      "Iteration 11: Added cgb_fg2_oofs, Bag Score: -1.03240, Mean Bag Score: -1.03330, Full Score: -1.03301\n",
      "Iteration 12: Added cgb_fg2_oofs, Bag Score: -1.03155, Mean Bag Score: -1.03316, Full Score: -1.03301\n",
      "Iteration 13: Added cgb_fg2_oofs, Bag Score: -1.03118, Mean Bag Score: -1.03300, Full Score: -1.03301\n",
      "Iteration 14: Added cgb_fg2_oofs, Bag Score: -1.03279, Mean Bag Score: -1.03299, Full Score: -1.03301\n",
      "Iteration 15: Added cgb_fg2_oofs, Bag Score: -1.03044, Mean Bag Score: -1.03282, Full Score: -1.03302\n",
      "Iteration 16: Added cgb_fg2_oofs, Bag Score: -1.03379, Mean Bag Score: -1.03288, Full Score: -1.03302\n",
      "Iteration 17: Added cgb_fg2_oofs, Bag Score: -1.03457, Mean Bag Score: -1.03298, Full Score: -1.03302\n",
      "Iteration 18: Added cgb_oofs, Bag Score: -1.03363, Mean Bag Score: -1.03301, Full Score: -1.03302\n",
      "Iteration 19: Added cgb_fg2_oofs, Bag Score: -1.03297, Mean Bag Score: -1.03301, Full Score: -1.03302\n",
      "Iteration 20: Added cgb_fg2_oofs, Bag Score: -1.03207, Mean Bag Score: -1.03296, Full Score: -1.03302\n",
      "Iteration 21: Added cgb_fg2_oofs, Bag Score: -1.03150, Mean Bag Score: -1.03290, Full Score: -1.03301\n",
      "Iteration 22: Added cgb_fg2_oofs, Bag Score: -1.03325, Mean Bag Score: -1.03291, Full Score: -1.03301\n",
      "Iteration 23: Added cgb_fg2_oofs, Bag Score: -1.03426, Mean Bag Score: -1.03297, Full Score: -1.03301\n",
      "Iteration 24: Added cgb_fg2_oofs, Bag Score: -1.03084, Mean Bag Score: -1.03288, Full Score: -1.03301\n",
      "Iteration 25: Added cgb_fg2_oofs, Bag Score: -1.03298, Mean Bag Score: -1.03289, Full Score: -1.03301\n",
      "Iteration 26: Added cgb_fg2_oofs, Bag Score: -1.03544, Mean Bag Score: -1.03298, Full Score: -1.03301\n",
      "Iteration 27: Added cgb_fg2_oofs, Bag Score: -1.03274, Mean Bag Score: -1.03297, Full Score: -1.03301\n",
      "Iteration 28: Added cgb_fg2_oofs, Bag Score: -1.03182, Mean Bag Score: -1.03293, Full Score: -1.03301\n",
      "Iteration 29: Added cgb_oofs, Bag Score: -1.03346, Mean Bag Score: -1.03295, Full Score: -1.03302\n",
      "Iteration 30: Added cgb_fg2_oofs, Bag Score: -1.03294, Mean Bag Score: -1.03295, Full Score: -1.03302\n",
      "Iteration 31: Added cgb_fg2_oofs, Bag Score: -1.03308, Mean Bag Score: -1.03295, Full Score: -1.03302\n",
      "Iteration 32: Added cgb_fg2_oofs, Bag Score: -1.03416, Mean Bag Score: -1.03297, Full Score: -1.03301\n",
      "\n",
      "Early stopping triggered at iteration 33\n",
      "\n",
      "Final Ensemble Weights:\n",
      "cgb_fg2_oofs: 0.9062\n",
      "cgb_oofs: 0.0938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Ensembler.BaggedEnsembleSelection at 0x2951160e0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and fit\n",
    "model_cols = [col for col in oofs_df.columns if col != 'premium_amount_log']\n",
    "ensembler = BaggedEnsembleSelection(n_init=3, max_iter=50, corr_threshold=0.7, bag_fraction=0.4, warm_start=30)\n",
    "ensembler.fit(oofs_df[model_cols], oofs_df['premium_amount_log'], performance_func=lambda y, pred: -root_mean_squared_log_error(y, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Predicting on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bagged_probs(models, data):\n",
    "    \"\"\"Get averaged probability predictions from all models.\"\"\"\n",
    "    probs = [model.predict(data) for model in models]\n",
    "    return np.mean(probs, axis=0)\n",
    "\n",
    "features = train_transformed.columns\n",
    "features_rgf = train_transformed_rgf.columns\n",
    "test_transformed['lgb_oofs'] = get_bagged_probs(lgb_models, test_transformed[features])\n",
    "test_transformed['xgb_oofs'] = get_bagged_probs(xgb_models, test_transformed[features])\n",
    "test_transformed['cgb_oofs'] = get_bagged_probs(cgb_models, test_transformed[features])\n",
    "# test_transformed['rgf_oofs'] = get_bagged_probs(rgf_models, test_transformed_rgf[features_rgf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = test_fg2.select_dtypes(['object', 'category']).columns\n",
    "lgb_test_fg2 = test_fg2.copy()\n",
    "for col in categorical_columns:\n",
    "    lgb_test_fg2[col] = lgb_test_fg2[col].astype('category').cat.codes.astype('int32')\n",
    "test_transformed['lgb_fg2_oofs'] = get_bagged_probs(lgb_fg2_models, lgb_test_fg2[features_fg2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_columns = test_fg2.select_dtypes(['object']).columns\n",
    "# xgb_test_fg2 = test_fg2.copy()\n",
    "# for col in categorical_columns:\n",
    "#     xgb_test_fg2[col] = xgb_test_fg2[col].astype('category')\n",
    "# test_transformed['xgb_fg2_oofs'] = get_bagged_probs(xgb_fg2_models, xgb_test_fg2[features_fg2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = test_fg2.select_dtypes(['object', 'category']).columns\n",
    "cgb_test_fg2 = test_fg2.copy()\n",
    "for col in categorical_columns:\n",
    "    cgb_test_fg2[col] = cgb_test_fg2[col].fillna('Missing')\n",
    "    cgb_test_fg2[col] = cgb_test_fg2[col].astype('category')\n",
    "cat_features = [cgb_test_fg2.columns.get_loc(col) for col in categorical_columns]\n",
    "\n",
    "test_transformed['cgb_fg2_oofs'] = get_bagged_probs(cgb_fg2_models, cgb_test_fg2[features_fg2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = original_transformed.columns\n",
    "# features_rgf = original_transformed_rgf.columns\n",
    "# for i, model in enumerate(lgb_original_models, 1):\n",
    "#     test_transformed[f'lgb_ori_{i}'] = model.predict(test_transformed[features])\n",
    "    \n",
    "# for i, model in enumerate(xgb_original_models, 1):\n",
    "#     test_transformed[f'xgb_ori_{i}'] = model.predict(test_transformed[features])\n",
    "\n",
    "# for i, model in enumerate(cgb_original_models, 1):\n",
    "#     test_transformed[f'cgb_ori_{i}'] = model.predict(test_transformed[features])\n",
    "\n",
    "# for i, model in enumerate(rgf_original_models, 1):\n",
    "#     test_transformed[f'rgf_ori_{i}'] = model.predict(test_transformed_rgf[features_rgf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_cols = [col for col in test_transformed.columns if \"_oofs\" in col]\n",
    "#test_transformed[pred_cols] = np.expm1(test_transformed[pred_cols])\n",
    "test_transformed['ensemble_preds'] = ensembler.predict(test_transformed[model_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['Premium Amount'] = test_transformed['ensemble_preds'].values\n",
    "submission.to_csv(os.path.join(base_path, 'submissions/submission_ensemble_fg1_fg2_lgb_xgb_cgb2.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['Premium Amount'] = test_transformed['cgb_fg2_oofs'].values\n",
    "submission.to_csv(os.path.join(base_path, 'submissions/submission_cgb_fg2.csv'), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
